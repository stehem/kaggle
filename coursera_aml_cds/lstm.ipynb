{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this gave 1.04 and for some reason did not ensemble well with the other models but it was still a very interesting experiment, basically for each item_id/shop_id we create time steps which are each a date_block_num and the target is the block after the last time step, big advantage is no need to generate lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#only work with fresh data, after 26 block\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#only ids in test\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "#same as for the lightgbm notebook\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    item_ids = all_item_ids\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as for the lightgbm notebook\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as for the lightgbm notebook\n",
    "\n",
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as for the lightgbm notebook\n",
    "\n",
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "#same as for the lightgbm notebook\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>539180</th>\n",
       "      <td>7956</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2266.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213675</td>\n",
       "      <td>0.399283</td>\n",
       "      <td>0.625882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5602</th>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138748</td>\n",
       "      <td>0.196160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497482</th>\n",
       "      <td>22145</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>4670.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>0.179334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339784</th>\n",
       "      <td>19618</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>2989.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105042</td>\n",
       "      <td>0.176103</td>\n",
       "      <td>0.179534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496800</th>\n",
       "      <td>22137</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078812</td>\n",
       "      <td>0.195342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140210</th>\n",
       "      <td>16153</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.296460</td>\n",
       "      <td>0.203229</td>\n",
       "      <td>0.160670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33985</th>\n",
       "      <td>687</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>18.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.120108</td>\n",
       "      <td>0.316074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482750</th>\n",
       "      <td>21948</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.199301</td>\n",
       "      <td>0.071690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854044</th>\n",
       "      <td>12970</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334764</td>\n",
       "      <td>0.217341</td>\n",
       "      <td>0.193790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045360</th>\n",
       "      <td>15223</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.570411</td>\n",
       "      <td>0.248845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding\n",
       "539180   7956     57       32              1                    6                 9      2015  9.0             2266.0          237.0               11.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.213675               0.399283               0.625882                      \n",
       "5602     83       4        29              0                    40                6      2015  0.0             793.0           9304.0              79.0                     10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.000000               0.138748               0.196160                      \n",
       "1497482  22145    34       27              0                    37                4      2015  0.0             424.0           4670.0              38.0                     12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.043290               0.073360               0.179334                      \n",
       "1339784  19618    5        32              0                    37                9      2015  7.0             1092.0          2989.0              52.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.105042               0.176103               0.179534                      \n",
       "1496800  22137    10       31              0                    40                8      2015  0.0             442.0           8513.0              59.0                     11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        0.000000               0.078812               0.195342                      \n",
       "1140210  16153    18       28              0                    64                5      2015  13.0            1434.0          906.0               10.0                     11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.296460               0.203229               0.160670                      \n",
       "33985    687      39       27              0                    73                4      2015  18.0            754.0           380.0               1.0                      12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.315315               0.120108               0.316074                      \n",
       "1482750  21948    24       30              0                    61                7      2015  0.0             1014.0          516.0               0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.004292               0.199301               0.071690                      \n",
       "854044   12970    56       29              1                    55                6      2015  10.0            1566.0          6017.0              147.0                    10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.334764               0.217341               0.193790                      \n",
       "1045360  15223    42       28              0                    63                5      2015  6.0             4343.0          1370.0              75.0                     11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.086420               0.570411               0.248845                      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "#experimented with various feature combinations in the end i got 1.04 with this very simple basic set of 4 features\n",
    "#overfitting was a real issue\n",
    "\n",
    "features = [\n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "#this is mandatory for NN\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "# 27 to 32 are the time steps, target is 33\n",
    "\n",
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "#building of training data was quite long and parallel processing made it much faster\n",
    "#multiprocessing was annoying to get to work on notebooks, the function has to be in an external file and then\n",
    "#imported\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192780, 6, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstms need 3D arrays samples * time steps * features\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55323, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 3)                 96        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 100\n",
      "Trainable params: 100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 83s 429us/step - loss: 1.0385 - mean_squared_error: 1.0385 - val_loss: 0.9754 - val_mean_squared_error: 0.9754\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 82s 427us/step - loss: 0.9792 - mean_squared_error: 0.9792 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 83s 430us/step - loss: 0.9584 - mean_squared_error: 0.9584 - val_loss: 0.9323 - val_mean_squared_error: 0.9323\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 86s 444us/step - loss: 0.9478 - mean_squared_error: 0.9478 - val_loss: 0.9209 - val_mean_squared_error: 0.9209\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 82s 427us/step - loss: 0.9345 - mean_squared_error: 0.9345 - val_loss: 0.9123 - val_mean_squared_error: 0.9123\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 85s 439us/step - loss: 0.9281 - mean_squared_error: 0.9281 - val_loss: 0.8985 - val_mean_squared_error: 0.8985\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 83s 429us/step - loss: 0.9259 - mean_squared_error: 0.9259 - val_loss: 0.8948 - val_mean_squared_error: 0.8948\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 70s 363us/step - loss: 0.9112 - mean_squared_error: 0.9112 - val_loss: 0.8846 - val_mean_squared_error: 0.8846\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 79s 408us/step - loss: 0.9107 - mean_squared_error: 0.9107 - val_loss: 0.8829 - val_mean_squared_error: 0.8829\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 77s 399us/step - loss: 0.9087 - mean_squared_error: 0.9087 - val_loss: 0.8805 - val_mean_squared_error: 0.8805\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 87s 450us/step - loss: 0.8978 - mean_squared_error: 0.8978 - val_loss: 0.8771 - val_mean_squared_error: 0.8771\n",
      "Epoch 12/100\n",
      "192780/192780 [==============================] - 87s 449us/step - loss: 0.8943 - mean_squared_error: 0.8943 - val_loss: 0.8738 - val_mean_squared_error: 0.8738\n",
      "Epoch 13/100\n",
      "192780/192780 [==============================] - 69s 359us/step - loss: 0.8945 - mean_squared_error: 0.8945 - val_loss: 0.8723 - val_mean_squared_error: 0.8723\n",
      "Epoch 14/100\n",
      "192780/192780 [==============================] - 87s 450us/step - loss: 0.8945 - mean_squared_error: 0.8945 - val_loss: 0.8694 - val_mean_squared_error: 0.8694\n",
      "Epoch 15/100\n",
      "192780/192780 [==============================] - 85s 442us/step - loss: 0.8900 - mean_squared_error: 0.8900 - val_loss: 0.8651 - val_mean_squared_error: 0.8651\n",
      "Epoch 16/100\n",
      "192780/192780 [==============================] - 90s 467us/step - loss: 0.8814 - mean_squared_error: 0.8814 - val_loss: 0.8624 - val_mean_squared_error: 0.8624\n",
      "Epoch 17/100\n",
      "192780/192780 [==============================] - 90s 469us/step - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 0.8632 - val_mean_squared_error: 0.8632\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.929084466013391\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 3,\\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=0, verbose=0)\n",
    "]\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique())) \\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to create intervals to use parallel processing, this step is very slow\n",
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "#this is much faster\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 6, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the code above is needed to get the same shape as training\n",
    "\n",
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68502975],\n",
       "       [0.15780857],\n",
       "       [0.76122224],\n",
       "       ...,\n",
       "       [0.15192589],\n",
       "       [0.137075  ],\n",
       "       [0.12643987]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26820764\n",
      "9.9696865\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
