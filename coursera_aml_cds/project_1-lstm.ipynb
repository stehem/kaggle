{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175775</th>\n",
       "      <td>2863</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.198523</td>\n",
       "      <td>0.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274804</th>\n",
       "      <td>4049</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>23.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.119139</td>\n",
       "      <td>0.410866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360819</th>\n",
       "      <td>19973</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130846</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686924</th>\n",
       "      <td>10334</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>14.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>0.194823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511801</th>\n",
       "      <td>7580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.168571</td>\n",
       "      <td>0.162562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541264</th>\n",
       "      <td>8005</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.116132</td>\n",
       "      <td>0.411594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056743</th>\n",
       "      <td>15298</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>13.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248908</td>\n",
       "      <td>0.197074</td>\n",
       "      <td>0.247976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471001</th>\n",
       "      <td>21732</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.200891</td>\n",
       "      <td>0.194660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930681</th>\n",
       "      <td>13588</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.221706</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948974</th>\n",
       "      <td>13720</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.125223</td>\n",
       "      <td>0.165533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding\n",
       "175775   2863     53       32              0                    25                9      2015  12.0            1229.0          258.0               3.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.317391               0.198523               0.185245                      \n",
       "274804   4049     45       32              0                    23                9      2015  23.0            654.0           3590.0              48.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.493333               0.119139               0.410866                      \n",
       "1360819  19973    41       32              0                    61                9      2015  0.0             686.0           554.0               4.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.000000               0.130846               0.073659                      \n",
       "686924   10334    34       27              0                    40                4      2015  14.0            424.0           10683.0             16.0                     12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.219178               0.071288               0.194823                      \n",
       "511801   7580     50       30              0                    64                7      2015  0.0             1126.0          1076.0              19.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.008368               0.168571               0.162562                      \n",
       "541264   8005     3        30              0                    23                7      2015  1.0             535.0           2759.0              49.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.004292               0.116132               0.411594                      \n",
       "1056743  15298    24       29              0                    63                6      2015  13.0            882.0           1425.0              8.0                      10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.248908               0.197074               0.247976                      \n",
       "1471001  21732    26       27              0                    40                4      2015  10.0            1527.0          10683.0             222.0                    12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.103896               0.200891               0.194660                      \n",
       "930681   13588    38       30              0                    61                7      2015  0.0             1354.0          516.0               13.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.037037               0.221706               0.073659                      \n",
       "948974   13720    49       32              0                    69                9      2015  12.0            567.0           665.0               8.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.416667               0.125223               0.165533                      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    #'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "      \n",
    "    \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3478112  0.18928401 0.10767918 0.08343084 0.07857975]\n",
      "0.8067849771669491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block', 'shop_cnt_block', 'shop_category_cnt_block']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "        \n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632988\n",
      "601517\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192780, 6, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55323, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3)                 84        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 88\n",
      "Trainable params: 88\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 105s 546us/step - loss: 1.0548 - mean_squared_error: 1.0548 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 103s 536us/step - loss: 0.9990 - mean_squared_error: 0.9990 - val_loss: 0.9726 - val_mean_squared_error: 0.9726\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 103s 536us/step - loss: 0.9765 - mean_squared_error: 0.9765 - val_loss: 0.9475 - val_mean_squared_error: 0.9475\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 104s 538us/step - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 0.9378 - val_mean_squared_error: 0.9378\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 103s 535us/step - loss: 0.9468 - mean_squared_error: 0.9468 - val_loss: 0.9319 - val_mean_squared_error: 0.9319\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 104s 537us/step - loss: 0.9467 - mean_squared_error: 0.9467 - val_loss: 0.9262 - val_mean_squared_error: 0.9262\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 103s 537us/step - loss: 0.9381 - mean_squared_error: 0.9381 - val_loss: 0.9226 - val_mean_squared_error: 0.9226\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 103s 534us/step - loss: 0.9428 - mean_squared_error: 0.9428 - val_loss: 0.9182 - val_mean_squared_error: 0.9182\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 103s 534us/step - loss: 0.9293 - mean_squared_error: 0.9293 - val_loss: 0.9126 - val_mean_squared_error: 0.9126\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 103s 535us/step - loss: 0.9252 - mean_squared_error: 0.9252 - val_loss: 0.9096 - val_mean_squared_error: 0.9096\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 104s 537us/step - loss: 0.9238 - mean_squared_error: 0.9238 - val_loss: 0.9096 - val_mean_squared_error: 0.9096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.9537506897206154\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 3,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=0, verbose=0)\n",
    "]\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique())) \\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 6, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.469023  ],\n",
       "       [0.17267114],\n",
       "       [0.5955542 ],\n",
       "       ...,\n",
       "       [0.1465084 ],\n",
       "       [0.11780733],\n",
       "       [0.09114391]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2941262\n",
      "10.263625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3542884257097005\n",
      "16.49019305497366\n"
     ]
    }
   ],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
