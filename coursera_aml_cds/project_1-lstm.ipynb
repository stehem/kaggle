{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175775</th>\n",
       "      <td>2863</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.198523</td>\n",
       "      <td>0.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274804</th>\n",
       "      <td>4049</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>23.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.119139</td>\n",
       "      <td>0.410866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360819</th>\n",
       "      <td>19973</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130846</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686924</th>\n",
       "      <td>10334</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>14.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>0.194823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511801</th>\n",
       "      <td>7580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.168571</td>\n",
       "      <td>0.162562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541264</th>\n",
       "      <td>8005</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.116132</td>\n",
       "      <td>0.411594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056743</th>\n",
       "      <td>15298</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>13.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248908</td>\n",
       "      <td>0.197074</td>\n",
       "      <td>0.247976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471001</th>\n",
       "      <td>21732</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.200891</td>\n",
       "      <td>0.194660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930681</th>\n",
       "      <td>13588</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.221706</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948974</th>\n",
       "      <td>13720</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.125223</td>\n",
       "      <td>0.165533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding\n",
       "175775   2863     53       32              0                    25                9      2015  12.0            1229.0          258.0               3.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.317391               0.198523               0.185245                      \n",
       "274804   4049     45       32              0                    23                9      2015  23.0            654.0           3590.0              48.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.493333               0.119139               0.410866                      \n",
       "1360819  19973    41       32              0                    61                9      2015  0.0             686.0           554.0               4.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.000000               0.130846               0.073659                      \n",
       "686924   10334    34       27              0                    40                4      2015  14.0            424.0           10683.0             16.0                     12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.219178               0.071288               0.194823                      \n",
       "511801   7580     50       30              0                    64                7      2015  0.0             1126.0          1076.0              19.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.008368               0.168571               0.162562                      \n",
       "541264   8005     3        30              0                    23                7      2015  1.0             535.0           2759.0              49.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.004292               0.116132               0.411594                      \n",
       "1056743  15298    24       29              0                    63                6      2015  13.0            882.0           1425.0              8.0                      10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.248908               0.197074               0.247976                      \n",
       "1471001  21732    26       27              0                    40                4      2015  10.0            1527.0          10683.0             222.0                    12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.103896               0.200891               0.194660                      \n",
       "930681   13588    38       30              0                    61                7      2015  0.0             1354.0          516.0               13.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.037037               0.221706               0.073659                      \n",
       "948974   13720    49       32              0                    69                9      2015  12.0            567.0           665.0               8.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.416667               0.125223               0.165533                      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    #'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "      'item_id_mean_encoding', \n",
    "    'shop_id_mean_encoding'\n",
    "    \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3478112  0.18928401 0.10767918 0.08343084 0.07857975]\n",
      "0.8067849771669491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block', 'shop_cnt_block', 'shop_category_cnt_block']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "        \n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632988\n",
      "601517\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192780, 6, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55323, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 3)                 108       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 112\n",
      "Trainable params: 112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 105s 544us/step - loss: 0.9909 - mean_squared_error: 0.9909 - val_loss: 0.9170 - val_mean_squared_error: 0.9170\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from -inf to 0.91698, saving model to lstm_best.hdf5\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 104s 540us/step - loss: 0.9211 - mean_squared_error: 0.9211 - val_loss: 0.8841 - val_mean_squared_error: 0.8841\n",
      "\n",
      "Epoch 00002: val_mean_squared_error did not improve from 0.91698\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 104s 541us/step - loss: 0.8989 - mean_squared_error: 0.8989 - val_loss: 0.8651 - val_mean_squared_error: 0.8651\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 0.91698\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 104s 538us/step - loss: 0.8842 - mean_squared_error: 0.8842 - val_loss: 0.8686 - val_mean_squared_error: 0.8686\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.91698\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.9319905715570118\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 3,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "filepath = \"lstm_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_mean_squared_error',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=0, verbose=0),checkpoint\n",
    "]\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique())) \\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 6, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3094289 ],\n",
       "       [0.06494454],\n",
       "       [1.5547305 ],\n",
       "       ...,\n",
       "       [0.1107872 ],\n",
       "       [0.07014821],\n",
       "       [0.04235309]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31718498\n",
      "6.960547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3542884257097005\n",
      "16.49019305497366\n"
     ]
    }
   ],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
