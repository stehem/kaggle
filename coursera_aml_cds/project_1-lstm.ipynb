{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "#sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "#sales_train = sales_train[sales_train['date_block_num'] > 23]\n",
    "\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'month': 1, 'year': 2013}\n",
      "1 {'month': 2, 'year': 2013}\n",
      "2 {'month': 3, 'year': 2013}\n",
      "3 {'month': 4, 'year': 2013}\n",
      "4 {'month': 5, 'year': 2013}\n",
      "5 {'month': 6, 'year': 2013}\n",
      "6 {'month': 7, 'year': 2013}\n",
      "7 {'month': 8, 'year': 2013}\n",
      "8 {'month': 9, 'year': 2013}\n",
      "9 {'month': 10, 'year': 2013}\n",
      "10 {'month': 11, 'year': 2013}\n",
      "11 {'month': 12, 'year': 2013}\n",
      "12 {'month': 1, 'year': 2014}\n",
      "13 {'month': 2, 'year': 2014}\n",
      "14 {'month': 3, 'year': 2014}\n",
      "15 {'month': 4, 'year': 2014}\n",
      "16 {'month': 5, 'year': 2014}\n",
      "17 {'month': 6, 'year': 2014}\n",
      "18 {'month': 7, 'year': 2014}\n",
      "19 {'month': 8, 'year': 2014}\n",
      "20 {'month': 9, 'year': 2014}\n",
      "21 {'month': 10, 'year': 2014}\n",
      "22 {'month': 11, 'year': 2014}\n",
      "23 {'month': 12, 'year': 2014}\n",
      "24 {'month': 1, 'year': 2015}\n",
      "25 {'month': 2, 'year': 2015}\n",
      "26 {'month': 3, 'year': 2015}\n",
      "27 {'month': 4, 'year': 2015}\n",
      "28 {'month': 5, 'year': 2015}\n",
      "29 {'month': 6, 'year': 2015}\n",
      "30 {'month': 7, 'year': 2015}\n",
      "31 {'month': 8, 'year': 2015}\n",
      "32 {'month': 9, 'year': 2015}\n",
      "33 {'month': 10, 'year': 2015}\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(dates_dict.items()): print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prices = sales_train.groupby(['item_id','date_block_num'])['item_price'].mean().reset_index()\n",
    "training = training.merge(mean_prices, on=['item_id','date_block_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_block 1\n",
      "item_cnt_block 2\n",
      "item_cnt_block 3\n",
      "shop_cnt_block 1\n",
      "shop_cnt_block 2\n",
      "shop_cnt_block 3\n",
      "category_cnt_block 1\n",
      "category_cnt_block 2\n",
      "category_cnt_block 3\n",
      "shop_category_cnt_block 1\n",
      "shop_category_cnt_block 2\n",
      "shop_category_cnt_block 3\n"
     ]
    }
   ],
   "source": [
    "def add_lags(df, cols, name, lags = [1,2,3]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "\n",
    "        df = df.merge(result, on=cols, how='left')\n",
    "        df[lag_name].fillna(0,inplace=True)\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "training = add_lags(training, ['item_id','date_block_num'], 'item_cnt_block')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_cnt_block')\n",
    "training = add_lags(training, ['item_category_id','date_block_num'], 'category_cnt_block')                                        \n",
    "training = add_lags(training, ['shop_id', 'item_category_id','date_block_num'], 'shop_category_cnt_block')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "      <th>item_cnt_block_lag_1</th>\n",
       "      <th>item_cnt_block_lag_2</th>\n",
       "      <th>item_cnt_block_lag_3</th>\n",
       "      <th>shop_cnt_block_lag_1</th>\n",
       "      <th>shop_cnt_block_lag_2</th>\n",
       "      <th>shop_cnt_block_lag_3</th>\n",
       "      <th>category_cnt_block_lag_1</th>\n",
       "      <th>category_cnt_block_lag_2</th>\n",
       "      <th>category_cnt_block_lag_3</th>\n",
       "      <th>shop_category_cnt_block_lag_1</th>\n",
       "      <th>shop_category_cnt_block_lag_2</th>\n",
       "      <th>shop_category_cnt_block_lag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801531</th>\n",
       "      <td>5338</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.801569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7296.0</td>\n",
       "      <td>106.848673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2415.738095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9882.0</td>\n",
       "      <td>2031.852097</td>\n",
       "      <td>2054.5</td>\n",
       "      <td>7485.237451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>8829.063655</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>142.008333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>286.850134</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.122530</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.847572</td>\n",
       "      <td>0</td>\n",
       "      <td>899.000000</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.168214</td>\n",
       "      <td>0.015362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578413</th>\n",
       "      <td>7872</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1459.0</td>\n",
       "      <td>12226.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>24.733137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12185.0</td>\n",
       "      <td>186.717074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3318.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14610.0</td>\n",
       "      <td>2763.487910</td>\n",
       "      <td>2513.5</td>\n",
       "      <td>7494.178627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18876.0</td>\n",
       "      <td>6554.126641</td>\n",
       "      <td>4659.0</td>\n",
       "      <td>148.356443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>272.644752</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.407246</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.654933</td>\n",
       "      <td>0</td>\n",
       "      <td>179.408696</td>\n",
       "      <td>1.065084</td>\n",
       "      <td>0.060809</td>\n",
       "      <td>0.992292</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>14655.0</td>\n",
       "      <td>10130.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281862</th>\n",
       "      <td>3916</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>6353.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>0.074205</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4648.0</td>\n",
       "      <td>4133.0</td>\n",
       "      <td>4063.0</td>\n",
       "      <td>7162.0</td>\n",
       "      <td>8881.0</td>\n",
       "      <td>8036.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601519</th>\n",
       "      <td>13712</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>899.000000</td>\n",
       "      <td>0.029591</td>\n",
       "      <td>0.166081</td>\n",
       "      <td>0.072311</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725938</th>\n",
       "      <td>8365</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2473.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>140.568551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2298.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9865.0</td>\n",
       "      <td>1812.345575</td>\n",
       "      <td>1822.5</td>\n",
       "      <td>5516.103137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14815.0</td>\n",
       "      <td>5303.540630</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>106.721195</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>210.379108</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.255215</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.299405</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>3012.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922836</th>\n",
       "      <td>3002</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.670196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7945.0</td>\n",
       "      <td>162.635180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2556.809524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11204.0</td>\n",
       "      <td>2232.224182</td>\n",
       "      <td>2135.5</td>\n",
       "      <td>7286.702549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23348.0</td>\n",
       "      <td>8658.483072</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>137.576410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2635.0</td>\n",
       "      <td>284.366972</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.134692</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.023150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.166568</td>\n",
       "      <td>0.021762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>2898.0</td>\n",
       "      <td>3075.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486950</th>\n",
       "      <td>4414</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>0.170863</td>\n",
       "      <td>0.015353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900173</th>\n",
       "      <td>8845</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5413.0</td>\n",
       "      <td>8680.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>7.801569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7296.0</td>\n",
       "      <td>106.848673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2415.738095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9882.0</td>\n",
       "      <td>2031.852097</td>\n",
       "      <td>2054.5</td>\n",
       "      <td>7485.237451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>8829.063655</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>142.008333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>286.850134</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.122530</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.847572</td>\n",
       "      <td>0</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.276614</td>\n",
       "      <td>0.341782</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5827.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>5233.0</td>\n",
       "      <td>8623.0</td>\n",
       "      <td>6763.0</td>\n",
       "      <td>5802.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713739</th>\n",
       "      <td>11409</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3915.0</td>\n",
       "      <td>13639.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>13.808431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6547.0</td>\n",
       "      <td>104.918265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1926.547619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8457.0</td>\n",
       "      <td>1587.773679</td>\n",
       "      <td>1464.0</td>\n",
       "      <td>5176.736275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14655.0</td>\n",
       "      <td>4869.002586</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>96.386690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>208.687215</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.213735</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.130777</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016351</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>0.135828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3740.0</td>\n",
       "      <td>3769.0</td>\n",
       "      <td>3663.0</td>\n",
       "      <td>12748.0</td>\n",
       "      <td>15821.0</td>\n",
       "      <td>14452.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024064</th>\n",
       "      <td>6025</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.616078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8537.0</td>\n",
       "      <td>133.563161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2515.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11710.0</td>\n",
       "      <td>2188.593000</td>\n",
       "      <td>2183.0</td>\n",
       "      <td>7283.984314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25433.0</td>\n",
       "      <td>8841.156546</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>138.385205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3018.0</td>\n",
       "      <td>299.165545</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.155840</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.071967</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.173984</td>\n",
       "      <td>0.015008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2441.0</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_price  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding  item_cnt_block_lag_1  item_cnt_block_lag_2  item_cnt_block_lag_3  shop_cnt_block_lag_1  shop_cnt_block_lag_2  shop_cnt_block_lag_3  category_cnt_block_lag_1  category_cnt_block_lag_2  category_cnt_block_lag_3  shop_category_cnt_block_lag_1  shop_category_cnt_block_lag_2  shop_category_cnt_block_lag_3\n",
       "1801531  5338     38       7               0                    58                8      2013  1.0             1895.0          284.0               0.0                      7.801569             0.0                 7296.0              106.848673          0.0                 2415.738095          0.0                 9882.0              2031.852097         2054.5              7485.237451              0.0                     24130.0                 8829.063655             1384.0                  142.008333                    0.0                          2553.0                       286.850134                   16.0                         0.122530                  0                        20                       0.847572                 0                        899.000000  0.012079               0.168214               0.015362                        1.0                   1.0                   1.0                   1453.0                1539.0                1337.0                356.0                     269.0                     345.0                     0.0                            0.0                            0.0                          \n",
       "2578413  7872     39       23              0                    30                12     2014  23.0            1459.0          12226.0             152.0                    24.733137            0.0                 12185.0             186.717074          1.0                 3318.500000          0.0                 14610.0             2763.487910         2513.5              7494.178627              0.0                     18876.0                 6554.126641             4659.0                  148.356443                    0.0                          2561.0                       272.644752                   68.0                         0.407246                  0                        20                       1.654933                 0                        179.408696  1.065084               0.060809               0.992292                        24.0                  28.0                  42.0                  962.0                 638.0                 707.0                 10125.0                   14655.0                   10130.0                   86.0                           98.0                           65.0                         \n",
       "1281862  3916     42       28              3                    55                5      2015  22.0            4343.0          6353.0              284.0                    11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        299.000000  0.074205               0.374615               0.186686                        0.0                   0.0                   0.0                   4648.0                4133.0                4063.0                7162.0                    8881.0                    8036.0                    363.0                          419.0                          363.0                        \n",
       "4601519  13712    22       27              0                    69                4      2015  3.0             1664.0          704.0               20.0                     12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        899.000000  0.029591               0.166081               0.072311                        4.0                   19.0                  0.0                   1547.0                1301.0                1255.0                981.0                     1681.0                    1123.0                    20.0                           19.0                           7.0                          \n",
       "2725938  8365     56       22              0                    58                11     2014  0.0             2473.0          583.0               25.0                     16.610000            0.0                 7721.0              140.568551          0.0                 2298.428571          0.0                 9865.0              1812.345575         1822.5              5516.103137              0.0                     14815.0                 5303.540630             2378.0                  106.721195                   -1.0                          2346.0                       210.379108                   33.0                         0.255215                  0                        20                       1.299405                 0                        0.000000    0.004359               0.194684               0.014896                        0.0                   0.0                   0.0                   1881.0                1879.0                3012.0                402.0                     437.0                     375.0                     10.0                           12.0                           21.0                         \n",
       "922836   3002     16       8               0                    76                9      2013  0.0             1924.0          121.0               0.0                      10.670196            0.0                 7945.0              162.635180          0.0                 2556.809524          0.0                 11204.0             2232.224182         2135.5              7286.702549              0.0                     23348.0                 8658.483072             1129.0                  137.576410                    0.0                          2635.0                       284.366972                   12.0                         0.134692                  0                        20                       1.023150                 0                        0.000000    0.005217               0.166568               0.021762                        0.0                   0.0                   0.0                   2457.0                2898.0                3075.0                105.0                     100.0                     98.0                      0.0                            0.0                            0.0                          \n",
       "1486950  4414     18       28              0                    58                5      2015  0.0             1434.0          299.0               29.0                     11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.000000    0.018088               0.170863               0.015353                        0.0                   0.0                   0.0                   1515.0                1435.0                1420.0                389.0                     506.0                     473.0                     19.0                           7.0                            4.0                          \n",
       "2900173  8845     57       7               1                    37                8      2013  7.0             5413.0          8680.0              445.0                    7.801569             0.0                 7296.0              106.848673          0.0                 2415.738095          0.0                 9882.0              2031.852097         2054.5              7485.237451              0.0                     24130.0                 8829.063655             1384.0                  142.008333                    0.0                          2553.0                       286.850134                   16.0                         0.122530                  0                        20                       0.847572                 0                        399.000000  0.276614               0.341782               0.118683                        14.0                  10.0                  10.0                  5827.0                5987.0                5233.0                8623.0                    6763.0                    5802.0                    467.0                          319.0                          247.0                        \n",
       "3713739  11409    42       21              0                    40                10     2014  0.0             3915.0          13639.0             503.0                    13.808431            0.0                 6547.0              104.918265          0.0                 1926.547619          0.0                 8457.0              1587.773679         1464.0              5176.736275              0.0                     14655.0                 4869.002586             3035.0                  96.386690                     0.0                          3035.0                       208.687215                   27.0                         0.213735                  0                        20                       1.130777                 0                        0.000000    0.016351               0.374615               0.135828                        0.0                   0.0                   0.0                   3740.0                3769.0                3663.0                12748.0                   15821.0                   14452.0                   498.0                          565.0                          537.0                        \n",
       "2024064  6025     26       10              0                    58                11     2013  0.0             2409.0          444.0               7.0                      10.616078            0.0                 8537.0              133.563161          0.0                 2515.428571          0.0                 11710.0             2188.593000         2183.0              7283.984314              0.0                     25433.0                 8841.156546             1764.0                  138.385205                    0.0                          3018.0                       299.165545                   21.0                         0.155840                  0                        20                       1.071967                 0                        0.000000    0.008666               0.173984               0.015008                        0.0                   0.0                   0.0                   2441.0                3206.0                2452.0                374.0                     320.0                     284.0                     0.0                            1.0                            3.0                          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding',\n",
    "    'item_cnt_block_lag_1',\n",
    "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "       'shop_cnt_block_lag_1',\n",
    "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    " 'category_cnt_block_lag_1',\n",
    "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "     'shop_category_cnt_block_lag_1',\n",
    "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "#'item_cnt_block_lag_1',\n",
    " #      'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "  #     'shop_cnt_block_lag_1',\n",
    "   #    'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    "    #   'category_cnt_block_lag_1',\n",
    "     #  'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "      # 'shop_category_cnt_block_lag_1',\n",
    "       #'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "      \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "#windows = [list(range(4,11)), list(range(27,34)), list(range(16,23))]\n",
    "windows = [list(range(0,34))]\n",
    "\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block',\n",
       " 'shop_cnt_block',\n",
       " 'category_cnt_block',\n",
       " 'shop_category_cnt_block']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 33, 16)            1344      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 33, 16)            64        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,713\n",
      "Trainable params: 7,681\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 183s 951us/step - loss: 1.1356 - mean_squared_error: 1.1356 - val_loss: 1.0270 - val_mean_squared_error: 1.0270\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 178s 921us/step - loss: 1.0555 - mean_squared_error: 1.0555 - val_loss: 0.9784 - val_mean_squared_error: 0.9784\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 182s 945us/step - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 0.9641 - val_mean_squared_error: 0.9641\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 177s 920us/step - loss: 1.0024 - mean_squared_error: 1.0024 - val_loss: 0.9424 - val_mean_squared_error: 0.9424\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 183s 950us/step - loss: 0.9946 - mean_squared_error: 0.9946 - val_loss: 0.9220 - val_mean_squared_error: 0.9220\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 202s 1ms/step - loss: 0.9741 - mean_squared_error: 0.9741 - val_loss: 0.9208 - val_mean_squared_error: 0.9208\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 196s 1ms/step - loss: 0.9715 - mean_squared_error: 0.9715 - val_loss: 0.9008 - val_mean_squared_error: 0.9008\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 193s 1000us/step - loss: 0.9521 - mean_squared_error: 0.9521 - val_loss: 0.8913 - val_mean_squared_error: 0.8913\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 200s 1ms/step - loss: 0.9424 - mean_squared_error: 0.9424 - val_loss: 0.8860 - val_mean_squared_error: 0.8860\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 204s 1ms/step - loss: 0.9410 - mean_squared_error: 0.9410 - val_loss: 0.8820 - val_mean_squared_error: 0.8820\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 195s 1ms/step - loss: 0.9327 - mean_squared_error: 0.9327 - val_loss: 0.9055 - val_mean_squared_error: 0.9055\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.9515637833144726\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional,SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "dropout=0.5\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(LSTM(use_bias = True,units = 16,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features)),return_sequences=True))\n",
    "my_model.add(BatchNormalization())\n",
    "my_model.add(LSTM(use_bias = True,units = 32,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "filepath = \"lstm_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_mean_squared_error',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=1, verbose=0)#,checkpoint\n",
    "]\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=128, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique()))]#\\\n",
    "                       # & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3,4,5,6,7,8][::-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data_ = [sample[1:] for sample in test_lstm_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5828924 ],\n",
       "       [0.17468357],\n",
       "       [0.6842437 ],\n",
       "       ...,\n",
       "       [0.17713594],\n",
       "       [0.15788922],\n",
       "       [0.14600304]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29764888\n",
      "11.539448\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
