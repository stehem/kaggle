{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "#sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prices = sales_train.groupby(['item_id','date_block_num'])['item_price'].mean().reset_index()\n",
    "training = training.merge(mean_prices, on=['item_id','date_block_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_block 1\n",
      "item_cnt_block 2\n",
      "item_cnt_block 3\n",
      "item_cnt_block 6\n",
      "shop_cnt_block 1\n",
      "shop_cnt_block 2\n",
      "shop_cnt_block 3\n",
      "shop_cnt_block 6\n",
      "category_cnt_block 1\n",
      "category_cnt_block 2\n",
      "category_cnt_block 3\n",
      "category_cnt_block 6\n",
      "shop_category_cnt_block 1\n",
      "shop_category_cnt_block 2\n",
      "shop_category_cnt_block 3\n",
      "shop_category_cnt_block 6\n"
     ]
    }
   ],
   "source": [
    "def add_lags(df, cols, name, lags = [1,2,3]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "\n",
    "        df = df.merge(result, on=cols, how='left')\n",
    "        df[lag_name].fillna(0,inplace=True)\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "training = add_lags(training, ['item_id','date_block_num'], 'item_cnt_block')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_cnt_block')\n",
    "training = add_lags(training, ['item_category_id','date_block_num'], 'category_cnt_block')                                        \n",
    "training = add_lags(training, ['shop_id', 'item_category_id','date_block_num'], 'shop_category_cnt_block')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "      <th>item_cnt_block_lag_1</th>\n",
       "      <th>item_cnt_block_lag_2</th>\n",
       "      <th>item_cnt_block_lag_3</th>\n",
       "      <th>item_cnt_block_lag_6</th>\n",
       "      <th>shop_cnt_block_lag_1</th>\n",
       "      <th>shop_cnt_block_lag_2</th>\n",
       "      <th>shop_cnt_block_lag_3</th>\n",
       "      <th>shop_cnt_block_lag_6</th>\n",
       "      <th>category_cnt_block_lag_1</th>\n",
       "      <th>category_cnt_block_lag_2</th>\n",
       "      <th>category_cnt_block_lag_3</th>\n",
       "      <th>category_cnt_block_lag_6</th>\n",
       "      <th>shop_category_cnt_block_lag_1</th>\n",
       "      <th>shop_category_cnt_block_lag_2</th>\n",
       "      <th>shop_category_cnt_block_lag_3</th>\n",
       "      <th>shop_category_cnt_block_lag_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11190</th>\n",
       "      <td>234</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>249.000</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.136456</td>\n",
       "      <td>0.031204</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>842.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11035</th>\n",
       "      <td>226</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.028526</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582622</th>\n",
       "      <td>8690</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>4913.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.115771</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>6474.0</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403335</th>\n",
       "      <td>20605</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>2499.000</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.179008</td>\n",
       "      <td>0.134524</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>1187.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489983</th>\n",
       "      <td>22013</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>199.000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.265846</td>\n",
       "      <td>0.161515</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022049</th>\n",
       "      <td>14969</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>0.197390</td>\n",
       "      <td>0.132708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931564</th>\n",
       "      <td>13593</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>2299.000</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.071874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288690</th>\n",
       "      <td>4192</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2352.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>0.108787</td>\n",
       "      <td>0.398595</td>\n",
       "      <td>0.033034</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>2408.0</td>\n",
       "      <td>2860.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284375</th>\n",
       "      <td>18653</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.194980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402986</th>\n",
       "      <td>5837</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>62.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>298.989</td>\n",
       "      <td>1.387665</td>\n",
       "      <td>0.117633</td>\n",
       "      <td>0.412964</td>\n",
       "      <td>70.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_price  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding  item_cnt_block_lag_1  item_cnt_block_lag_2  item_cnt_block_lag_3  item_cnt_block_lag_6  shop_cnt_block_lag_1  shop_cnt_block_lag_2  shop_cnt_block_lag_3  shop_cnt_block_lag_6  category_cnt_block_lag_1  category_cnt_block_lag_2  category_cnt_block_lag_3  category_cnt_block_lag_6  shop_category_cnt_block_lag_1  shop_category_cnt_block_lag_2  shop_category_cnt_block_lag_3  shop_category_cnt_block_lag_6\n",
       "11190    234      4        31              0                    45                8      2015  2.0             947.0           69.0                0.0                      11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        249.000     0.069106               0.136456               0.031204                        3.0                   3.0                   5.0                   0.0                   842.0                 793.0                 893.0                 0.0                   78.0                      55.0                      60.0                      0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "11035    226      36       30              0                    45                7      2015  0.0             0.0             78.0                0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.000       0.021834               0.009216               0.028526                        4.0                   0.0                   1.0                   0.0                   0.0                   0.0                   0.0                   0.0                   55.0                      60.0                      84.0                      0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "582622   8690     45       32              0                    55                9      2015  5.0             654.0           4913.0              48.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        229.000     0.194444               0.115771               0.193425                        11.0                  15.0                  10.0                  0.0                   710.0                 675.0                 622.0                 0.0                   6022.0                    6474.0                    6017.0                    0.0                       70.0                           90.0                           61.0                           0.0                          \n",
       "1403335  20605    16       30              0                    72                7      2015  9.0             1114.0          1411.0              21.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        2499.000    0.157258               0.179008               0.134524                        10.0                  8.0                   10.0                  0.0                   1038.0                1187.0                1110.0                0.0                   1323.0                    1195.0                    1122.0                    0.0                       14.0                           33.0                           15.0                           0.0                          \n",
       "1489983  22013    58       32              0                    38                9      2015  5.0             1319.0          1102.0              8.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        199.000     0.121951               0.265846               0.161515                        6.0                   2.0                   8.0                   0.0                   1738.0                1689.0                1554.0                0.0                   723.0                     702.0                     640.0                     0.0                       8.0                            9.0                            4.0                            0.0                          \n",
       "1022049  14969    24       27              0                    47                4      2015  0.0             1297.0          116.0               0.0                      12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.000       0.065502               0.197390               0.132708                        0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                       0.0                       0.0                       0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "931564   13593    38       31              0                    61                8      2015  3.0             1781.0          786.0               28.0                     11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        2299.000    0.029289               0.229336               0.071874                        0.0                   0.0                   0.0                   0.0                   1354.0                1141.0                1257.0                0.0                   516.0                     605.0                     447.0                     0.0                       13.0                           18.0                           17.0                           0.0                          \n",
       "288690   4192     57       30              0                    76                7      2015  5.0             2352.0          185.0               0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        1490.000    0.108787               0.398595               0.033034                        6.0                   2.0                   3.0                   0.0                   2440.0                2408.0                2860.0                0.0                   156.0                     143.0                     197.0                     0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "1284375  18653    41       28              0                    40                5      2015  0.0             931.0           9208.0              121.0                    11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.000       0.004292               0.131813               0.194980                        1.0                   0.0                   0.0                   0.0                   725.0                 0.0                   0.0                   0.0                   10683.0                   0.0                       0.0                       0.0                       78.0                           0.0                            0.0                            0.0                          \n",
       "402986   5837     45       30              0                    65                7      2015  62.0            675.0           1817.0              35.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        298.989     1.387665               0.117633               0.412964                        70.0                  104.0                 114.0                 0.0                   622.0                 762.0                 862.0                 0.0                   1941.0                    1635.0                    1540.0                    0.0                       16.0                           21.0                           13.0                           0.0                          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med', 'item_price',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding', 'item_cnt_block_lag_1',\n",
       "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
       "       'item_cnt_block_lag_6', 'shop_cnt_block_lag_1',\n",
       "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
       "       'shop_cnt_block_lag_6', 'category_cnt_block_lag_1',\n",
       "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
       "       'category_cnt_block_lag_6', 'shop_category_cnt_block_lag_1',\n",
       "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
       "       'shop_category_cnt_block_lag_6'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding',\n",
    "    'item_cnt_block_lag_1',\n",
    "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "       'item_cnt_block_lag_6', 'shop_cnt_block_lag_1',\n",
    "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    "       'shop_cnt_block_lag_6', 'category_cnt_block_lag_1',\n",
    "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "       'category_cnt_block_lag_6', 'shop_category_cnt_block_lag_1',\n",
    "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "       'shop_category_cnt_block_lag_6'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "'item_cnt_block_lag_1',\n",
    "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "       'shop_cnt_block_lag_1',\n",
    "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    "       'category_cnt_block_lag_1',\n",
    "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "       'shop_category_cnt_block_lag_1',\n",
    "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "      \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int8, float32, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int8, float32, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(16,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "#windows = [list(range(4,11)), list(range(27,34)), list(range(16,23))]\n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block',\n",
       " 'shop_cnt_block',\n",
       " 'category_cnt_block',\n",
       " 'shop_category_cnt_block',\n",
       " 'item_cnt_block_lag_1',\n",
       " 'item_cnt_block_lag_2',\n",
       " 'item_cnt_block_lag_3',\n",
       " 'shop_cnt_block_lag_1',\n",
       " 'shop_cnt_block_lag_2',\n",
       " 'shop_cnt_block_lag_3',\n",
       " 'category_cnt_block_lag_1',\n",
       " 'category_cnt_block_lag_2',\n",
       " 'category_cnt_block_lag_3',\n",
       " 'shop_category_cnt_block_lag_1',\n",
       " 'shop_category_cnt_block_lag_2',\n",
       " 'shop_category_cnt_block_lag_3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3)                 240       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 244\n",
      "Trainable params: 244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 83s 430us/step - loss: 1.0468 - mean_squared_error: 1.0468 - val_loss: 0.9797 - val_mean_squared_error: 0.9797\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.97971, saving model to lstm_best.hdf5\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 82s 423us/step - loss: 0.9692 - mean_squared_error: 0.9692 - val_loss: 0.9359 - val_mean_squared_error: 0.9359\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.97971 to 0.93586, saving model to lstm_best.hdf5\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 73s 377us/step - loss: 0.9235 - mean_squared_error: 0.9235 - val_loss: 0.9061 - val_mean_squared_error: 0.9061\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.93586 to 0.90610, saving model to lstm_best.hdf5\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 64s 332us/step - loss: 0.9069 - mean_squared_error: 0.9069 - val_loss: 0.8901 - val_mean_squared_error: 0.8901\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.90610 to 0.89007, saving model to lstm_best.hdf5\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 70s 362us/step - loss: 0.8874 - mean_squared_error: 0.8874 - val_loss: 0.8719 - val_mean_squared_error: 0.8719\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.89007 to 0.87188, saving model to lstm_best.hdf5\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 86s 446us/step - loss: 0.8740 - mean_squared_error: 0.8740 - val_loss: 0.8573 - val_mean_squared_error: 0.8573\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.87188 to 0.85733, saving model to lstm_best.hdf5\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 77s 399us/step - loss: 0.8712 - mean_squared_error: 0.8712 - val_loss: 0.8551 - val_mean_squared_error: 0.8551\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.85733 to 0.85507, saving model to lstm_best.hdf5\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 81s 420us/step - loss: 0.8572 - mean_squared_error: 0.8572 - val_loss: 0.8548 - val_mean_squared_error: 0.8548\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.85507 to 0.85483, saving model to lstm_best.hdf5\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 90s 468us/step - loss: 0.8549 - mean_squared_error: 0.8549 - val_loss: 0.8441 - val_mean_squared_error: 0.8441\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 0.85483 to 0.84407, saving model to lstm_best.hdf5\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 86s 448us/step - loss: 0.8452 - mean_squared_error: 0.8452 - val_loss: 0.8403 - val_mean_squared_error: 0.8403\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.84407 to 0.84027, saving model to lstm_best.hdf5\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 88s 458us/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.8360 - val_mean_squared_error: 0.8360\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.84027 to 0.83603, saving model to lstm_best.hdf5\n",
      "Epoch 12/100\n",
      "192780/192780 [==============================] - 72s 373us/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.8355 - val_mean_squared_error: 0.8355\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 0.83603 to 0.83549, saving model to lstm_best.hdf5\n",
      "Epoch 13/100\n",
      "192780/192780 [==============================] - 89s 462us/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.8300 - val_mean_squared_error: 0.8300\n",
      "\n",
      "Epoch 00013: val_mean_squared_error improved from 0.83549 to 0.83001, saving model to lstm_best.hdf5\n",
      "Epoch 14/100\n",
      "192780/192780 [==============================] - 86s 445us/step - loss: 0.8403 - mean_squared_error: 0.8403 - val_loss: 0.8355 - val_mean_squared_error: 0.8355\n",
      "\n",
      "Epoch 00014: val_mean_squared_error did not improve from 0.83001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.914052449339114\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional,SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(LSTM(use_bias = True,units = 3,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "filepath = \"lstm_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_mean_squared_error',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=0, verbose=0),checkpoint\n",
    "]\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique()))\\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data_ = [sample[1:] for sample in test_lstm_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0416589 ],\n",
       "       [0.38839185],\n",
       "       [1.164058  ],\n",
       "       ...,\n",
       "       [0.31230557],\n",
       "       [0.23070586],\n",
       "       [0.17178261]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35675383\n",
      "13.731727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
