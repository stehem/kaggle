{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "#sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prices = sales_train.groupby(['item_id','date_block_num'])['item_price'].mean().reset_index()\n",
    "training = training.merge(mean_prices, on=['item_id','date_block_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_block 1\n",
      "item_cnt_block 2\n",
      "item_cnt_block 3\n",
      "item_cnt_block 6\n",
      "shop_cnt_block 1\n",
      "shop_cnt_block 2\n",
      "shop_cnt_block 3\n",
      "shop_cnt_block 6\n",
      "category_cnt_block 1\n",
      "category_cnt_block 2\n",
      "category_cnt_block 3\n",
      "category_cnt_block 6\n",
      "shop_category_cnt_block 1\n",
      "shop_category_cnt_block 2\n",
      "shop_category_cnt_block 3\n",
      "shop_category_cnt_block 6\n"
     ]
    }
   ],
   "source": [
    "def add_lags(df, cols, name, lags = [1,2,3]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "\n",
    "        df = df.merge(result, on=cols, how='left')\n",
    "        df[lag_name].fillna(0,inplace=True)\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "training = add_lags(training, ['item_id','date_block_num'], 'item_cnt_block')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_cnt_block')\n",
    "training = add_lags(training, ['item_category_id','date_block_num'], 'category_cnt_block')                                        \n",
    "training = add_lags(training, ['shop_id', 'item_category_id','date_block_num'], 'shop_category_cnt_block')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "      <th>item_cnt_block_lag_1</th>\n",
       "      <th>item_cnt_block_lag_2</th>\n",
       "      <th>item_cnt_block_lag_3</th>\n",
       "      <th>item_cnt_block_lag_6</th>\n",
       "      <th>shop_cnt_block_lag_1</th>\n",
       "      <th>shop_cnt_block_lag_2</th>\n",
       "      <th>shop_cnt_block_lag_3</th>\n",
       "      <th>shop_cnt_block_lag_6</th>\n",
       "      <th>category_cnt_block_lag_1</th>\n",
       "      <th>category_cnt_block_lag_2</th>\n",
       "      <th>category_cnt_block_lag_3</th>\n",
       "      <th>category_cnt_block_lag_6</th>\n",
       "      <th>shop_category_cnt_block_lag_1</th>\n",
       "      <th>shop_category_cnt_block_lag_2</th>\n",
       "      <th>shop_category_cnt_block_lag_3</th>\n",
       "      <th>shop_category_cnt_block_lag_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11190</th>\n",
       "      <td>234</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>249.000</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.136456</td>\n",
       "      <td>0.031204</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>842.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11035</th>\n",
       "      <td>226</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.028526</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582622</th>\n",
       "      <td>8690</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>4913.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>229.000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.115771</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>6474.0</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403335</th>\n",
       "      <td>20605</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>2499.000</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.179008</td>\n",
       "      <td>0.134524</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>1187.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489983</th>\n",
       "      <td>22013</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>199.000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.265846</td>\n",
       "      <td>0.161515</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022049</th>\n",
       "      <td>14969</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>0.197390</td>\n",
       "      <td>0.132708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931564</th>\n",
       "      <td>13593</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>2299.000</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.071874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288690</th>\n",
       "      <td>4192</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2352.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>0.108787</td>\n",
       "      <td>0.398595</td>\n",
       "      <td>0.033034</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>2408.0</td>\n",
       "      <td>2860.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284375</th>\n",
       "      <td>18653</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.194980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402986</th>\n",
       "      <td>5837</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>62.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>298.989</td>\n",
       "      <td>1.387665</td>\n",
       "      <td>0.117633</td>\n",
       "      <td>0.412964</td>\n",
       "      <td>70.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_price  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding  item_cnt_block_lag_1  item_cnt_block_lag_2  item_cnt_block_lag_3  item_cnt_block_lag_6  shop_cnt_block_lag_1  shop_cnt_block_lag_2  shop_cnt_block_lag_3  shop_cnt_block_lag_6  category_cnt_block_lag_1  category_cnt_block_lag_2  category_cnt_block_lag_3  category_cnt_block_lag_6  shop_category_cnt_block_lag_1  shop_category_cnt_block_lag_2  shop_category_cnt_block_lag_3  shop_category_cnt_block_lag_6\n",
       "11190    234      4        31              0                    45                8      2015  2.0             947.0           69.0                0.0                      11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        249.000     0.069106               0.136456               0.031204                        3.0                   3.0                   5.0                   0.0                   842.0                 793.0                 893.0                 0.0                   78.0                      55.0                      60.0                      0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "11035    226      36       30              0                    45                7      2015  0.0             0.0             78.0                0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.000       0.021834               0.009216               0.028526                        4.0                   0.0                   1.0                   0.0                   0.0                   0.0                   0.0                   0.0                   55.0                      60.0                      84.0                      0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "582622   8690     45       32              0                    55                9      2015  5.0             654.0           4913.0              48.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        229.000     0.194444               0.115771               0.193425                        11.0                  15.0                  10.0                  0.0                   710.0                 675.0                 622.0                 0.0                   6022.0                    6474.0                    6017.0                    0.0                       70.0                           90.0                           61.0                           0.0                          \n",
       "1403335  20605    16       30              0                    72                7      2015  9.0             1114.0          1411.0              21.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        2499.000    0.157258               0.179008               0.134524                        10.0                  8.0                   10.0                  0.0                   1038.0                1187.0                1110.0                0.0                   1323.0                    1195.0                    1122.0                    0.0                       14.0                           33.0                           15.0                           0.0                          \n",
       "1489983  22013    58       32              0                    38                9      2015  5.0             1319.0          1102.0              8.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        199.000     0.121951               0.265846               0.161515                        6.0                   2.0                   8.0                   0.0                   1738.0                1689.0                1554.0                0.0                   723.0                     702.0                     640.0                     0.0                       8.0                            9.0                            4.0                            0.0                          \n",
       "1022049  14969    24       27              0                    47                4      2015  0.0             1297.0          116.0               0.0                      12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.000       0.065502               0.197390               0.132708                        0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                   0.0                       0.0                       0.0                       0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "931564   13593    38       31              0                    61                8      2015  3.0             1781.0          786.0               28.0                     11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        2299.000    0.029289               0.229336               0.071874                        0.0                   0.0                   0.0                   0.0                   1354.0                1141.0                1257.0                0.0                   516.0                     605.0                     447.0                     0.0                       13.0                           18.0                           17.0                           0.0                          \n",
       "288690   4192     57       30              0                    76                7      2015  5.0             2352.0          185.0               0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        1490.000    0.108787               0.398595               0.033034                        6.0                   2.0                   3.0                   0.0                   2440.0                2408.0                2860.0                0.0                   156.0                     143.0                     197.0                     0.0                       0.0                            0.0                            0.0                            0.0                          \n",
       "1284375  18653    41       28              0                    40                5      2015  0.0             931.0           9208.0              121.0                    11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.000       0.004292               0.131813               0.194980                        1.0                   0.0                   0.0                   0.0                   725.0                 0.0                   0.0                   0.0                   10683.0                   0.0                       0.0                       0.0                       78.0                           0.0                            0.0                            0.0                          \n",
       "402986   5837     45       30              0                    65                7      2015  62.0            675.0           1817.0              35.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        298.989     1.387665               0.117633               0.412964                        70.0                  104.0                 114.0                 0.0                   622.0                 762.0                 862.0                 0.0                   1941.0                    1635.0                    1540.0                    0.0                       16.0                           21.0                           13.0                           0.0                          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med', 'item_price',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding', 'item_cnt_block_lag_1',\n",
       "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
       "       'item_cnt_block_lag_6', 'shop_cnt_block_lag_1',\n",
       "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
       "       'shop_cnt_block_lag_6', 'category_cnt_block_lag_1',\n",
       "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
       "       'category_cnt_block_lag_6', 'shop_category_cnt_block_lag_1',\n",
       "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
       "       'shop_category_cnt_block_lag_6'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding',\n",
    "    'item_cnt_block_lag_1',\n",
    "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "       'item_cnt_block_lag_6', 'shop_cnt_block_lag_1',\n",
    "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    "       'shop_cnt_block_lag_6', 'category_cnt_block_lag_1',\n",
    "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "       'category_cnt_block_lag_6', 'shop_category_cnt_block_lag_1',\n",
    "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "       'shop_category_cnt_block_lag_6'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "'item_cnt_block_lag_1',\n",
    "       'item_cnt_block_lag_2', 'item_cnt_block_lag_3',\n",
    "       'shop_cnt_block_lag_1',\n",
    "       'shop_cnt_block_lag_2', 'shop_cnt_block_lag_3',\n",
    "       'category_cnt_block_lag_1',\n",
    "       'category_cnt_block_lag_2', 'category_cnt_block_lag_3',\n",
    "       'shop_category_cnt_block_lag_1',\n",
    "       'shop_category_cnt_block_lag_2', 'shop_category_cnt_block_lag_3',\n",
    "      \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int8, float32, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int8, float32, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(16,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "#windows = [list(range(4,11)), list(range(27,34)), list(range(16,23))]\n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block',\n",
       " 'shop_cnt_block',\n",
       " 'category_cnt_block',\n",
       " 'shop_category_cnt_block',\n",
       " 'item_cnt_block_lag_1',\n",
       " 'item_cnt_block_lag_2',\n",
       " 'item_cnt_block_lag_3',\n",
       " 'shop_cnt_block_lag_1',\n",
       " 'shop_cnt_block_lag_2',\n",
       " 'shop_cnt_block_lag_3',\n",
       " 'category_cnt_block_lag_1',\n",
       " 'category_cnt_block_lag_2',\n",
       " 'category_cnt_block_lag_3',\n",
       " 'shop_category_cnt_block_lag_1',\n",
       " 'shop_category_cnt_block_lag_2',\n",
       " 'shop_category_cnt_block_lag_3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_7 (GRU)                  (None, 6)                 414       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 421\n",
      "Trainable params: 421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 38s 199us/step - loss: 1.0267 - mean_squared_error: 1.0267 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.97268, saving model to lstm_best.hdf5\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 37s 190us/step - loss: 0.9785 - mean_squared_error: 0.9785 - val_loss: 0.9419 - val_mean_squared_error: 0.9419\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.97268 to 0.94189, saving model to lstm_best.hdf5\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 37s 192us/step - loss: 0.9554 - mean_squared_error: 0.9554 - val_loss: 0.9250 - val_mean_squared_error: 0.9250\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.94189 to 0.92503, saving model to lstm_best.hdf5\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 38s 199us/step - loss: 0.9384 - mean_squared_error: 0.9384 - val_loss: 0.9168 - val_mean_squared_error: 0.9168\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.92503 to 0.91681, saving model to lstm_best.hdf5\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 31s 163us/step - loss: 0.9299 - mean_squared_error: 0.9299 - val_loss: 0.9013 - val_mean_squared_error: 0.9013\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.91681 to 0.90135, saving model to lstm_best.hdf5\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 43s 221us/step - loss: 0.9142 - mean_squared_error: 0.9142 - val_loss: 0.8889 - val_mean_squared_error: 0.8889\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.90135 to 0.88893, saving model to lstm_best.hdf5\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 32s 166us/step - loss: 0.9085 - mean_squared_error: 0.9085 - val_loss: 0.8823 - val_mean_squared_error: 0.8823\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.88893 to 0.88234, saving model to lstm_best.hdf5\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 41s 215us/step - loss: 0.9053 - mean_squared_error: 0.9053 - val_loss: 0.8679 - val_mean_squared_error: 0.8679\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.88234 to 0.86794, saving model to lstm_best.hdf5\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 42s 220us/step - loss: 0.8987 - mean_squared_error: 0.8987 - val_loss: 0.8657 - val_mean_squared_error: 0.8657\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 0.86794 to 0.86565, saving model to lstm_best.hdf5\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 42s 216us/step - loss: 0.8856 - mean_squared_error: 0.8856 - val_loss: 0.8594 - val_mean_squared_error: 0.8594\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.86565 to 0.85943, saving model to lstm_best.hdf5\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 41s 215us/step - loss: 0.8832 - mean_squared_error: 0.8832 - val_loss: 0.8579 - val_mean_squared_error: 0.8579\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.85943 to 0.85792, saving model to lstm_best.hdf5\n",
      "Epoch 12/100\n",
      "192780/192780 [==============================] - 43s 222us/step - loss: 0.8811 - mean_squared_error: 0.8811 - val_loss: 0.8606 - val_mean_squared_error: 0.8606\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 0.85792\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.9276958928001229\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional,SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(GRU(use_bias = True,units = 6,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "filepath = \"lstm_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_mean_squared_error',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=1, verbose=0),checkpoint\n",
    "]\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=64, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique()))\\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data_ = [sample[1:] for sample in test_lstm_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5611627 ],\n",
       "       [0.2307058 ],\n",
       "       [0.6389833 ],\n",
       "       ...,\n",
       "       [0.1343044 ],\n",
       "       [0.12297392],\n",
       "       [0.20248175]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2821101\n",
      "10.324653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
