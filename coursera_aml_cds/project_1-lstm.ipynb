{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175775</th>\n",
       "      <td>2863</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.198523</td>\n",
       "      <td>0.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274804</th>\n",
       "      <td>4049</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>23.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.119139</td>\n",
       "      <td>0.410866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360819</th>\n",
       "      <td>19973</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130846</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686924</th>\n",
       "      <td>10334</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>14.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>0.194823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511801</th>\n",
       "      <td>7580</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.168571</td>\n",
       "      <td>0.162562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541264</th>\n",
       "      <td>8005</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.116132</td>\n",
       "      <td>0.411594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056743</th>\n",
       "      <td>15298</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>13.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248908</td>\n",
       "      <td>0.197074</td>\n",
       "      <td>0.247976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471001</th>\n",
       "      <td>21732</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.200891</td>\n",
       "      <td>0.194660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930681</th>\n",
       "      <td>13588</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.221706</td>\n",
       "      <td>0.073659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948974</th>\n",
       "      <td>13720</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>12.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.648627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>61.946153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1719.523810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6867.0</td>\n",
       "      <td>1596.048841</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>2829.167059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7107.0</td>\n",
       "      <td>2461.715660</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>66.414052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>114.964885</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.246452</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.125223</td>\n",
       "      <td>0.165533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding\n",
       "175775   2863     53       32              0                    25                9      2015  12.0            1229.0          258.0               3.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.317391               0.198523               0.185245                      \n",
       "274804   4049     45       32              0                    23                9      2015  23.0            654.0           3590.0              48.0                     11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.493333               0.119139               0.410866                      \n",
       "1360819  19973    41       32              0                    61                9      2015  0.0             686.0           554.0               4.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.000000               0.130846               0.073659                      \n",
       "686924   10334    34       27              0                    40                4      2015  14.0            424.0           10683.0             16.0                     12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.219178               0.071288               0.194823                      \n",
       "511801   7580     50       30              0                    64                7      2015  0.0             1126.0          1076.0              19.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.008368               0.168571               0.162562                      \n",
       "541264   8005     3        30              0                    23                7      2015  1.0             535.0           2759.0              49.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.004292               0.116132               0.411594                      \n",
       "1056743  15298    24       29              0                    63                6      2015  13.0            882.0           1425.0              8.0                      10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.248908               0.197074               0.247976                      \n",
       "1471001  21732    26       27              0                    40                4      2015  10.0            1527.0          10683.0             222.0                    12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.103896               0.200891               0.194660                      \n",
       "930681   13588    38       30              0                    61                7      2015  0.0             1354.0          516.0               13.0                     10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.037037               0.221706               0.073659                      \n",
       "948974   13720    49       32              0                    69                9      2015  12.0            567.0           665.0               8.0                      11.648627            0.0                 3390.0              61.946153           3.0                 1719.523810          0.0                 6867.0              1596.048841         1230.0              2829.167059              0.0                     7107.0                  2461.715660             1670.0                  66.414052                     0.0                          1079.0                       114.964885                   29.0                         0.246452                  0                        20                       1.115817                 0                        0.416667               0.125223               0.165533                      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', \n",
    "    'category_cnt_block',\n",
    "    'shop_category_cnt_block',\n",
    "      'item_cnt_block_mean', \n",
    "    'shop_cnt_block_mean'\n",
    "    \n",
    "]\n",
    "\n",
    "#features = all_features\n",
    "\n",
    "#features = ['pca0', 'pca1', 'pca2', 'pca3',  'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3478112  0.18928401 0.10767918 0.08343084 0.07857975]\n",
      "0.8067849771669491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "training_pca = pca.transform(training[features])\n",
    "\n",
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_cnt_block',\n",
       " 'shop_cnt_block',\n",
       " 'category_cnt_block',\n",
       " 'shop_category_cnt_block',\n",
       " 'item_cnt_block_mean',\n",
       " 'shop_cnt_block_mean']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "        \n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632988\n",
      "601517\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192780, 6, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55323, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 3)                 120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 124\n",
      "Trainable params: 124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 105s 546us/step - loss: 1.0443 - mean_squared_error: 1.0443 - val_loss: 0.9820 - val_mean_squared_error: 0.9820\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.98204, saving model to lstm_best.hdf5\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 104s 540us/step - loss: 0.9740 - mean_squared_error: 0.9740 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 0.98204 to 0.94597, saving model to lstm_best.hdf5\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 105s 543us/step - loss: 0.9466 - mean_squared_error: 0.9466 - val_loss: 0.9277 - val_mean_squared_error: 0.9277\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 0.94597 to 0.92770, saving model to lstm_best.hdf5\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 105s 543us/step - loss: 0.9385 - mean_squared_error: 0.9385 - val_loss: 0.9134 - val_mean_squared_error: 0.9134\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 0.92770 to 0.91342, saving model to lstm_best.hdf5\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 104s 540us/step - loss: 0.9273 - mean_squared_error: 0.9273 - val_loss: 0.9074 - val_mean_squared_error: 0.9074\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 0.91342 to 0.90745, saving model to lstm_best.hdf5\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 105s 544us/step - loss: 0.9140 - mean_squared_error: 0.9140 - val_loss: 0.9037 - val_mean_squared_error: 0.9037\n",
      "\n",
      "Epoch 00006: val_mean_squared_error improved from 0.90745 to 0.90372, saving model to lstm_best.hdf5\n",
      "Epoch 7/100\n",
      "192780/192780 [==============================] - 104s 542us/step - loss: 0.9076 - mean_squared_error: 0.9076 - val_loss: 0.8933 - val_mean_squared_error: 0.8933\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 0.90372 to 0.89328, saving model to lstm_best.hdf5\n",
      "Epoch 8/100\n",
      "192780/192780 [==============================] - 104s 540us/step - loss: 0.9101 - mean_squared_error: 0.9101 - val_loss: 0.8889 - val_mean_squared_error: 0.8889\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 0.89328 to 0.88885, saving model to lstm_best.hdf5\n",
      "Epoch 9/100\n",
      "192780/192780 [==============================] - 104s 542us/step - loss: 0.9065 - mean_squared_error: 0.9065 - val_loss: 0.8881 - val_mean_squared_error: 0.8881\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 0.88885 to 0.88815, saving model to lstm_best.hdf5\n",
      "Epoch 10/100\n",
      "192780/192780 [==============================] - 104s 542us/step - loss: 0.9059 - mean_squared_error: 0.9059 - val_loss: 0.8772 - val_mean_squared_error: 0.8772\n",
      "\n",
      "Epoch 00010: val_mean_squared_error improved from 0.88815 to 0.87720, saving model to lstm_best.hdf5\n",
      "Epoch 11/100\n",
      "192780/192780 [==============================] - 104s 541us/step - loss: 0.8901 - mean_squared_error: 0.8901 - val_loss: 0.8746 - val_mean_squared_error: 0.8746\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 0.87720 to 0.87459, saving model to lstm_best.hdf5\n",
      "Epoch 12/100\n",
      "192780/192780 [==============================] - 104s 542us/step - loss: 0.8994 - mean_squared_error: 0.8994 - val_loss: 0.8733 - val_mean_squared_error: 0.8733\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 0.87459 to 0.87329, saving model to lstm_best.hdf5\n",
      "Epoch 13/100\n",
      "192780/192780 [==============================] - 105s 543us/step - loss: 0.8974 - mean_squared_error: 0.8974 - val_loss: 0.8669 - val_mean_squared_error: 0.8669\n",
      "\n",
      "Epoch 00013: val_mean_squared_error improved from 0.87329 to 0.86694, saving model to lstm_best.hdf5\n",
      "Epoch 14/100\n",
      "192780/192780 [==============================] - 104s 539us/step - loss: 0.8902 - mean_squared_error: 0.8902 - val_loss: 0.8641 - val_mean_squared_error: 0.8641\n",
      "\n",
      "Epoch 00014: val_mean_squared_error improved from 0.86694 to 0.86411, saving model to lstm_best.hdf5\n",
      "Epoch 15/100\n",
      "192780/192780 [==============================] - 104s 542us/step - loss: 0.8853 - mean_squared_error: 0.8853 - val_loss: 0.8573 - val_mean_squared_error: 0.8573\n",
      "\n",
      "Epoch 00015: val_mean_squared_error improved from 0.86411 to 0.85730, saving model to lstm_best.hdf5\n",
      "Epoch 16/100\n",
      "192780/192780 [==============================] - 104s 541us/step - loss: 0.8920 - mean_squared_error: 0.8920 - val_loss: 0.8669 - val_mean_squared_error: 0.8669\n",
      "\n",
      "Epoch 00016: val_mean_squared_error did not improve from 0.85730\n",
      "Epoch 17/100\n",
      "192780/192780 [==============================] - 104s 540us/step - loss: 0.8838 - mean_squared_error: 0.8838 - val_loss: 0.8587 - val_mean_squared_error: 0.8587\n",
      "\n",
      "Epoch 00017: val_mean_squared_error did not improve from 0.85730\n",
      "Epoch 18/100\n",
      "192780/192780 [==============================] - 104s 539us/step - loss: 0.8835 - mean_squared_error: 0.8835 - val_loss: 0.8590 - val_mean_squared_error: 0.8590\n",
      "\n",
      "Epoch 00018: val_mean_squared_error did not improve from 0.85730\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.9268115109354452\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "reg = L1L2(l1=0.01,l2=0.01)\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 3,\\\n",
    "                  #kernel_regularizer=reg, \\\n",
    "                  dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "filepath = \"lstm_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_mean_squared_error',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0),checkpoint\n",
    "]\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique())) \\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 6, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4044437],\n",
       "       [0.       ],\n",
       "       [1.61209  ],\n",
       "       ...,\n",
       "       [0.       ],\n",
       "       [0.       ],\n",
       "       [0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29090926\n",
      "13.49122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3542884257097005\n",
      "16.49019305497366\n"
     ]
    }
   ],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
