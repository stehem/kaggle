{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "#sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train[sales_train['date_block_num'] > 26]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "#ids_keep = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()\n",
    "ids_keep = sums[(sums['item_total_sales'] > 0)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "#train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "#train_item_ids = ids_keep\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "#all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_item_ids = test_item_ids\n",
    "\n",
    "#all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))\n",
    "all_shop_ids = test_shop_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    #item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    item_ids = all_item_ids\n",
    "    #dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    dbn_combos = list(product(all_shop_ids, item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['shop_item_cnt_block'] = training['shop_item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n",
    "training['year'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['year']), downcast='unsigned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"category_cnt_block\"})\n",
    "\n",
    "\n",
    "training = training.merge(ys, on=['item_category_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "ys = sales_train.groupby(['shop_id', 'item_category_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"shop_category_cnt_block\"})\n",
    "\n",
    "training = training.merge(ys, on=['shop_id', 'item_category_id', 'date_block_num'], how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_cnt_block_mean'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.mean)\n",
    "training['item_cnt_block_min'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.min)\n",
    "training['item_cnt_block_max'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.max)\n",
    "training['item_cnt_block_std'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.std)\n",
    "training['item_cnt_block_med'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.mean)\n",
    "training['shop_cnt_block_min'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.min)\n",
    "training['shop_cnt_block_max'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.max)\n",
    "training['shop_cnt_block_std'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.std)\n",
    "training['shop_cnt_block_med'] = training.groupby(['date_block_num'])['shop_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['category_cnt_block_mean'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.mean)\n",
    "training['category_cnt_block_min'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.min)\n",
    "training['category_cnt_block_max'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.max)\n",
    "training['category_cnt_block_std'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.std)\n",
    "training['category_cnt_block_med'] = training.groupby(['date_block_num'])['category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_category_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.mean)\n",
    "training['shop_category_cnt_block_min'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.min)\n",
    "training['shop_category_cnt_block_max'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.max)\n",
    "training['shop_category_cnt_block_std'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.std)\n",
    "training['shop_category_cnt_block_med'] = training.groupby(['date_block_num'])['shop_category_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['shop_item_cnt_block_mean'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.mean)\n",
    "training['shop_item_cnt_block_min'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.min)\n",
    "training['shop_item_cnt_block_max'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.max)\n",
    "training['shop_item_cnt_block_std'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.std)\n",
    "training['shop_item_cnt_block_med'] = training.groupby(['date_block_num'])['shop_item_cnt_block'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"shop_item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['shop_item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_cnt_block</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_cnt_block</th>\n",
       "      <th>shop_cnt_block</th>\n",
       "      <th>category_cnt_block</th>\n",
       "      <th>shop_category_cnt_block</th>\n",
       "      <th>item_cnt_block_mean</th>\n",
       "      <th>item_cnt_block_min</th>\n",
       "      <th>item_cnt_block_max</th>\n",
       "      <th>item_cnt_block_std</th>\n",
       "      <th>item_cnt_block_med</th>\n",
       "      <th>shop_cnt_block_mean</th>\n",
       "      <th>shop_cnt_block_min</th>\n",
       "      <th>shop_cnt_block_max</th>\n",
       "      <th>shop_cnt_block_std</th>\n",
       "      <th>shop_cnt_block_med</th>\n",
       "      <th>category_cnt_block_mean</th>\n",
       "      <th>category_cnt_block_min</th>\n",
       "      <th>category_cnt_block_max</th>\n",
       "      <th>category_cnt_block_std</th>\n",
       "      <th>category_cnt_block_med</th>\n",
       "      <th>shop_category_cnt_block_mean</th>\n",
       "      <th>shop_category_cnt_block_min</th>\n",
       "      <th>shop_category_cnt_block_max</th>\n",
       "      <th>shop_category_cnt_block_std</th>\n",
       "      <th>shop_category_cnt_block_med</th>\n",
       "      <th>shop_item_cnt_block_mean</th>\n",
       "      <th>shop_item_cnt_block_min</th>\n",
       "      <th>shop_item_cnt_block_max</th>\n",
       "      <th>shop_item_cnt_block_std</th>\n",
       "      <th>shop_item_cnt_block_med</th>\n",
       "      <th>item_id_mean_encoding</th>\n",
       "      <th>shop_id_mean_encoding</th>\n",
       "      <th>item_category_id_mean_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>601017</th>\n",
       "      <td>8874</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.207054</td>\n",
       "      <td>0.194402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677947</th>\n",
       "      <td>10215</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233480</td>\n",
       "      <td>0.406424</td>\n",
       "      <td>0.045556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209177</th>\n",
       "      <td>3273</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.123882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107553</th>\n",
       "      <td>15922</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4181.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>13.289608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>75.376194</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1553.785714</td>\n",
       "      <td>330.0</td>\n",
       "      <td>6247.0</td>\n",
       "      <td>1336.398262</td>\n",
       "      <td>1161.0</td>\n",
       "      <td>3228.865882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7680.0</td>\n",
       "      <td>2663.646850</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>73.363525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>135.813744</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.255649</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.089856</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054852</td>\n",
       "      <td>0.219265</td>\n",
       "      <td>0.136608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112674</th>\n",
       "      <td>1866</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>58.426138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1430.904762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>1194.835848</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>3318.272157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>3228.111861</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>74.266709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>150.766699</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>0.212279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387800</th>\n",
       "      <td>5656</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.138278</td>\n",
       "      <td>0.633298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252430</th>\n",
       "      <td>18140</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>11.840196</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3551.0</td>\n",
       "      <td>57.770225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1551.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>1090.984155</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>3253.890392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8513.0</td>\n",
       "      <td>2995.889840</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>75.921218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>128.648912</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.260037</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.031644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.364807</td>\n",
       "      <td>0.264612</td>\n",
       "      <td>0.194649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170557</th>\n",
       "      <td>16549</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>6474.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.808824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3347.0</td>\n",
       "      <td>53.842045</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1427.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>1114.915109</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>3335.517843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>3239.632607</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>75.583501</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>141.362130</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.073963</td>\n",
       "      <td>0.192821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646031</th>\n",
       "      <td>9580</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5712.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>11.949804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>89.593827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1592.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6327.0</td>\n",
       "      <td>1311.452767</td>\n",
       "      <td>1211.5</td>\n",
       "      <td>3428.631373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9208.0</td>\n",
       "      <td>3303.055672</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>75.539594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>147.855428</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.213301</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.018289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.750298</td>\n",
       "      <td>0.195932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900487</th>\n",
       "      <td>13313</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.535490</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>124.515785</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1711.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7341.0</td>\n",
       "      <td>1447.095173</td>\n",
       "      <td>1309.5</td>\n",
       "      <td>4010.790784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14751.0</td>\n",
       "      <td>4102.264551</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>88.694935</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>195.630747</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.079221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193343</td>\n",
       "      <td>0.130881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  shop_id  date_block_num  shop_item_cnt_block  item_category_id  month  year  item_cnt_block  shop_cnt_block  category_cnt_block  shop_category_cnt_block  item_cnt_block_mean  item_cnt_block_min  item_cnt_block_max  item_cnt_block_std  item_cnt_block_med  shop_cnt_block_mean  shop_cnt_block_min  shop_cnt_block_max  shop_cnt_block_std  shop_cnt_block_med  category_cnt_block_mean  category_cnt_block_min  category_cnt_block_max  category_cnt_block_std  category_cnt_block_med  shop_category_cnt_block_mean  shop_category_cnt_block_min  shop_category_cnt_block_max  shop_category_cnt_block_std  shop_category_cnt_block_med  shop_item_cnt_block_mean  shop_item_cnt_block_min  shop_item_cnt_block_max  shop_item_cnt_block_std  shop_item_cnt_block_med  item_id_mean_encoding  shop_id_mean_encoding  item_category_id_mean_encoding\n",
       "601017   8874     18       31              0                    55                8      2015  4.0             1184.0          6022.0              124.0                    11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        0.139831               0.207054               0.194402                      \n",
       "677947   10215    57       31              0                    31                8      2015  8.0             2780.0          724.0               0.0                      11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        0.233480               0.406424               0.045556                      \n",
       "209177   3273     34       30              0                    56                7      2015  0.0             460.0           162.0               0.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.000000               0.073834               0.123882                      \n",
       "1107553  15922    12       33              0                    72                10     2015  1.0             4181.0          1976.0              44.0                     13.289608            0.0                 4078.0              75.376194           4.0                 1553.785714          330.0               6247.0              1336.398262         1161.0              3228.865882              0.0                     7680.0                  2663.646850             2290.0                  73.363525                     0.0                          2524.0                       135.813744                   30.0                         0.255649                  0                        20                       1.089856                 0                        0.054852               0.219265               0.136608                      \n",
       "112674   1866     16       29              0                    24                6      2015  15.0            1038.0          877.0               4.0                      10.833333           -1.0                 3473.0              58.426138           2.0                 1430.904762          0.0                 6160.0              1194.835848         1058.0              3318.272157              0.0                     9304.0                  3228.111861             1919.0                  74.266709                     0.0                          1529.0                       150.766699                   29.0                         0.221471                  0                        20                       1.008618                 0                        0.168831               0.183266               0.212279                      \n",
       "387800   5656     4        27              0                    3                 4      2015  0.0             899.0           746.0               8.0                      12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.148936               0.138278               0.633298                      \n",
       "1252430  18140    58       31              1                    55                8      2015  7.0             1738.0          6022.0              145.0                    11.840196           -1.0                 3551.0              57.770225           3.0                 1551.500000          0.0                 5714.0              1090.984155         1271.0              3253.890392              0.0                     8513.0                  2995.889840             1788.0                  75.921218                     0.0                          1189.0                       128.648912                   32.0                         0.260037                  0                        20                       1.031644                 0                        0.364807               0.264612               0.194649                      \n",
       "1170557  16549    34       30              0                    55                7      2015  4.0             460.0           6474.0              5.0                      10.808824           -1.0                 3347.0              53.842045           3.0                 1427.642857          0.0                 5987.0              1114.915109         1108.0              3335.517843              0.0                     9283.0                  3239.632607             1890.0                  75.583501                    -1.0                          1475.0                       141.362130                   32.0                         0.227605                  0                        20                       0.949685                 0                        0.094650               0.073963               0.192821                      \n",
       "646031   9580     25       28              0                    40                5      2015  0.0             5712.0          9208.0              873.0                    11.949804            0.0                 3768.0              89.593827           2.0                 1592.166667          0.0                 6327.0              1311.452767         1211.5              3428.631373              0.0                     9208.0                  3303.055672             1635.0                  75.539594                    -1.0                          2005.0                       147.855428                   26.0                         0.213301                  0                        20                       1.018289                 0                        0.008772               0.750298               0.195932                      \n",
       "900487   13313    55       27              0                    47                4      2015  0.0             3422.0          116.0               0.0                      12.535490           -1.0                 7300.0              124.515785          2.0                 1711.166667          0.0                 7341.0              1447.095173         1309.5              4010.790784              0.0                     14751.0                 4102.264551             2285.0                  88.694935                    -1.0                          2506.0                       195.630747                   26.0                         0.215145                  0                        20                       1.079221                 0                        0.000000               0.193343               0.130881                      "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'shop_item_cnt_block',\n",
       "       'item_category_id', 'month', 'year', 'item_cnt_block',\n",
       "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
       "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
       "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
       "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
       "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
       "       'category_cnt_block_min', 'category_cnt_block_max',\n",
       "       'category_cnt_block_std', 'category_cnt_block_med',\n",
       "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
       "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
       "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
       "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
       "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
       "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
       "       'item_category_id_mean_encoding', 'pca0', 'pca1', 'pca2', 'pca3',\n",
       "       'pca4'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    \n",
    "    'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "       'item_cnt_block_mean', 'item_cnt_block_min', 'item_cnt_block_max',\n",
    "       'item_cnt_block_std', 'item_cnt_block_med', 'shop_cnt_block_mean',\n",
    "       'shop_cnt_block_min', 'shop_cnt_block_max', 'shop_cnt_block_std',\n",
    "       'shop_cnt_block_med', 'category_cnt_block_mean',\n",
    "       'category_cnt_block_min', 'category_cnt_block_max',\n",
    "       'category_cnt_block_std', 'category_cnt_block_med',\n",
    "       'shop_category_cnt_block_mean', 'shop_category_cnt_block_min',\n",
    "       'shop_category_cnt_block_max', 'shop_category_cnt_block_std',\n",
    "       'shop_category_cnt_block_med', 'shop_item_cnt_block_mean',\n",
    "       'shop_item_cnt_block_min', 'shop_item_cnt_block_max',\n",
    "       'shop_item_cnt_block_std', 'shop_item_cnt_block_med',\n",
    "       'item_id_mean_encoding', 'shop_id_mean_encoding',\n",
    "       'item_category_id_mean_encoding'\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    \n",
    "     'item_cnt_block',\n",
    "       'shop_cnt_block', 'category_cnt_block', 'shop_category_cnt_block',\n",
    "      \n",
    "    \n",
    "]\n",
    "\n",
    "features = all_features\n",
    "\n",
    "features = ['pca0', 'pca1', 'pca2', 'pca3',\n",
    "       'pca4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/stephane/.local/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "\n",
    "\n",
    "training[all_features] = StandardScaler().fit_transform(training[all_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[all_features] = training[all_features].apply(pd.to_numeric, downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3478112  0.18928401 0.10767918 0.08343084 0.07857975]\n",
      "0.8067849771669491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5).fit(training[features])\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pca = pca.transform(training[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,component in enumerate(pca.explained_variance_ratio_):\n",
    "    name = 'pca%d' % (i)\n",
    "    training[name] = np.array(training_pca).T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca0    7.59529\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[['pca0']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 28, 29, 30, 31, 32, 33]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "dbns = sorted(training.date_block_num.unique())\n",
    "\n",
    "windows = []\n",
    "for i,_ in enumerate(dbns):\n",
    "    if (i+window_size) <= len(dbns):\n",
    "        window = dbns[i:i+window_size]\n",
    "        windows.append(window)  \n",
    " \n",
    "windows = [list(range(27,34))]\n",
    "\n",
    "print(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pca0', 'pca1', 'pca2', 'pca3', 'pca4']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 28, 29, 30, 31, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "        \n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_sample\n",
    "importlib.reload(build_sample)\n",
    "\n",
    "from build_sample import build_sample_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_sample_f,args=[window, training, features]) for window in windows]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "lstm_data = []\n",
    "lstm_y = []\n",
    "\n",
    "for result in res:\n",
    "    for idx, sample in enumerate(result.get()[0]):\n",
    "        lstm_data.append(sample)\n",
    "        lstm_y.append(result.get()[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data)\n",
    "small_y = np.array(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632988\n",
      "601517\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_y))\n",
    "\n",
    "print(len([y for y in lstm_y if y == 0]))\n",
    "\n",
    "zeros_indices = {}\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    \n",
    "    if y == 0:\n",
    "        zeros_indices[idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_no_zeros = []\n",
    "lstm_y_no_zeros = []\n",
    "for idx,sample in enumerate(lstm_data):\n",
    "    if idx not in zeros_indices:\n",
    "        lstm_data_no_zeros.append(sample)\n",
    "        lstm_y_no_zeros.append(lstm_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_zeros = []\n",
    "for idx,y in enumerate(lstm_y):\n",
    "    if idx in zeros_indices:\n",
    "        lstm_zeros.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(lstm_data_no_zeros))\n",
    "\n",
    "for zero_idx in np.random.choice(lstm_zeros,30000,replace=False):\n",
    "    lstm_data_no_zeros.append(lstm_data[zero_idx])\n",
    "    lstm_y_no_zeros.append(lstm_y[zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array(lstm_data_no_zeros)\n",
    "small_y = np.array(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(small_data, small_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lstm_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192780, 6, 5)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55323, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lstm_y_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 3)                 108       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 124\n",
      "Trainable params: 118\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "Train on 192780 samples, validate on 21420 samples\n",
      "Epoch 1/100\n",
      "192780/192780 [==============================] - 127s 657us/step - loss: 1.1300 - mean_squared_error: 1.1300 - val_loss: 0.9640 - val_mean_squared_error: 0.9640\n",
      "Epoch 2/100\n",
      "192780/192780 [==============================] - 127s 660us/step - loss: 1.0011 - mean_squared_error: 1.0011 - val_loss: 0.9298 - val_mean_squared_error: 0.9298\n",
      "Epoch 3/100\n",
      "192780/192780 [==============================] - 170s 880us/step - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 0.9260 - val_mean_squared_error: 0.9260\n",
      "Epoch 4/100\n",
      "192780/192780 [==============================] - 137s 710us/step - loss: 0.9703 - mean_squared_error: 0.9703 - val_loss: 0.9108 - val_mean_squared_error: 0.9108\n",
      "Epoch 5/100\n",
      "192780/192780 [==============================] - 119s 616us/step - loss: 0.9577 - mean_squared_error: 0.9577 - val_loss: 0.9068 - val_mean_squared_error: 0.9068\n",
      "Epoch 6/100\n",
      "192780/192780 [==============================] - 119s 616us/step - loss: 0.9528 - mean_squared_error: 0.9528 - val_loss: 0.9139 - val_mean_squared_error: 0.9139\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmse val: 0.955984786116819\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x_train_scaled = MinMaxScaler().fit_transform(x_train[features])\n",
    "\n",
    "\n",
    "#x_reshaped = np.reshape(x_train_scaled, (x_train_scaled.shape[0], 10, x_train_scaled.shape[1]))\n",
    "    \n",
    "#x_val_scaled_reshaped = np.reshape(x_val_scaled, (x_val_scaled.shape[0], 1, x_val_scaled.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "my_model = Sequential()\n",
    "#bi directional?\n",
    "\n",
    "#my_model.add(BatchNormalization())\n",
    "#my_model.add(CuDNNLSTM(units = 32,\\\n",
    "                 #input_shape = (small_data.shape[1],len(features)), return_sequences=True))\n",
    "#my_model.add(BatchNormalization())\n",
    "#my_model.add(Dropout(dropout))\n",
    "#my_model.add(GRU(use_bias = True,units = 8, dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "#my_model.add(BatchNormalization())\n",
    "#my_model.add(Dropout(dropout))\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 3, dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "my_model.add(BatchNormalization())\n",
    "#my_model.add(Dropout(dropout))\n",
    "#my_model.add(CuDNNLSTM(units = 8,input_shape = (small_data.shape[1],len(features)), return_sequences=True))\n",
    "#my_model.add(CuDNNLSTM(units = 16,input_shape = (small_data.shape[1],len(features))))\n",
    "\n",
    "#my_model.add(BatchNormalization())\n",
    "#my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 4, dropout=dropout,recurrent_dropout=dropout,input_shape = (small_data.shape[1],len(features))))\n",
    "#my_model.add(Dropout(dropout))\n",
    "#my_model.add(BatchNormalization())\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "#lstmd_dataa = np.array(np.array(lstm_data)).reshape(271148,8,51)\n",
    "#print(lstmd_dataa.shape)\n",
    "#print(lstmd_dataa)\n",
    "#lstm_yy = np.array([np.array([y]) for y in lstm_y[0:100]])\n",
    "#print(lstm_yy.shape)\n",
    "#print(lstm_yy)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=1, verbose=0)\n",
    "]\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=32, epochs=100,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                      )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "import math\n",
    "print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = training[(training['shop_id'].isin(test['shop_id'].unique()))\\\n",
    "                         & (training['item_id'].isin(test['item_id'].unique())) \\\n",
    "                        & (training['date_block_num'].isin(windows[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21420),\n",
       " (21420, 42840),\n",
       " (42840, 64260),\n",
       " (64260, 85680),\n",
       " (85680, 107100),\n",
       " (107100, 128520),\n",
       " (128520, 149940),\n",
       " (149940, 171360),\n",
       " (171360, 192780),\n",
       " (192780, 214200)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0, 235620, 21420))\n",
    "b = list(range(21420, 257040, 21420))\n",
    "intervals = list(zip(a,b))[:-1]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 21420)\n",
      "(21420, 42840)\n",
      "(42840, 64260)\n",
      "(64260, 85680)\n",
      "(85680, 107100)\n",
      "(107100, 128520)\n",
      "(128520, 149940)\n",
      "(149940, 171360)\n",
      "(171360, 192780)\n",
      "(192780, 214200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import importlib\n",
    "import build_test\n",
    "importlib.reload(build_test)\n",
    "\n",
    "window_size = len(windows[0])\n",
    "\n",
    "from build_test import build_test_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_test_f,args=[interval, test, training_test, features, window_size]) for interval in intervals]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = []\n",
    "\n",
    "for interval in intervals:\n",
    "    for re in res:\n",
    "        if interval in re.get():\n",
    "            for sample in re.get()[interval]:\n",
    "                test_lstm_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data = np.array(test_lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63652056],\n",
       "       [0.        ],\n",
       "       [0.62134945],\n",
       "       ...,\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = my_model.predict(np.array(test_lstm_data),batch_size=len(test_lstm_data))\n",
    "preds.clip(0,20,out=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10308865\n",
      "19.105051\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))\n",
    "\n",
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpreds = pd.read_csv('submissionbest.csv')['item_cnt_month']\n",
    "print(np.mean(bestpreds))\n",
    "print(np.max(bestpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg110.csv')['item_cnt_month']\n",
    "#cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (lg_preds * 0.50) + (lr_preds * 0.50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
