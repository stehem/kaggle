{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "sales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "sales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "sales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "ids_reject = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['item_cnt_block'] = training['item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"shop_cat\"] = training[\"shop_id\"].astype(str) + \"_\" + training[\"item_category_id\"].astype(str)\n",
    "training[\"shop_item\"] = training[\"shop_id\"].astype(str) + \"_\" + training[\"item_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shop_cats = training['shop_cat'].unique()\n",
    "shop_cats = dict(list(zip(unique_shop_cats, range(1,len(unique_shop_cats)))))\n",
    "\n",
    "def get_shop_cat_int(x):\n",
    "    if x in shop_cats:\n",
    "        return shop_cats[x]\n",
    "\n",
    "training['shop_cat_int'] = training['shop_cat'].apply(lambda x: get_shop_cat_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shop_items = training['shop_item'].unique()\n",
    "shop_items = dict(list(zip(unique_shop_items, range(1,len(unique_shop_items)))))\n",
    "\n",
    "def get_shop_item_int(x):\n",
    "    if x in shop_items:\n",
    "        return shop_items[x]\n",
    "\n",
    "training['shop_item_int'] = training['shop_item'].apply(lambda x: get_shop_item_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\", \"month\", \"shop_cat\", \"shop_item\", \"date_block_num\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/merge.py:970: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
      "  'representation', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_block\n",
      "cat_block\n",
      "shop_cat_block\n",
      "shop_item_block\n"
     ]
    }
   ],
   "source": [
    "def add_block_units_stats(df, cols, name):\n",
    "    print(name)\n",
    "    name_units = name + '_units'\n",
    "    name_mean = name + '_mean'\n",
    "    name_median = name + '_median'\n",
    "    name_max = name + '_max'\n",
    "    name_min = name + '_min'\n",
    "    name_std = name + '_std'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df.drop(columns=[name_units, name_mean, name_median],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    block_units = df.groupby(cols,as_index=False)['item_cnt_block'].sum()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_units})\n",
    "    df = df.merge(block_units, on=cols, how='left')\n",
    "    df[name_units].fillna(0,inplace=True)\n",
    "    df[name_units] = pd.to_numeric(df[name_units].astype(int),downcast='unsigned')\n",
    "    del block_units\n",
    "    \n",
    "    block_units_med = df.groupby(cols,as_index=False)['item_cnt_block'].median()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_median})\n",
    "    df = df.merge(block_units_med, on=cols, how='left')\n",
    "    df[name_median].fillna(0,inplace=True)\n",
    "    df[name_median] = pd.to_numeric(df[name_median].astype(int),downcast='unsigned')\n",
    "    del block_units_med\n",
    "    \n",
    "    block_means = df.groupby(cols,as_index=False)['item_cnt_block'].mean()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_mean})\n",
    "    df = df.merge(block_means, on=cols, how='left')\n",
    "    df[name_mean].fillna(0,inplace=True)\n",
    "    df[name_mean] = pd.to_numeric(df[name_mean],downcast='float')\n",
    "    del block_means\n",
    "    \n",
    "    block_max = df.groupby(cols,as_index=False)['item_cnt_block'].max()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_max})\n",
    "    df = df.merge(block_max, on=cols, how='left')\n",
    "    df[name_max].fillna(0,inplace=True)\n",
    "    df[name_max] = pd.to_numeric(df[name_max],downcast='float')\n",
    "    del block_max\n",
    "    \n",
    "    block_min = df.groupby(cols,as_index=False)['item_cnt_block'].min()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_min})\n",
    "    df = df.merge(block_min, on=cols, how='left')\n",
    "    df[name_min].fillna(0,inplace=True)\n",
    "    df[name_min] = pd.to_numeric(df[name_min],downcast='float')\n",
    "    del block_min\n",
    "    \n",
    "    block_std = df.groupby(cols,as_index=False)['item_cnt_block'].std()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_std})\n",
    "    df = df.merge(block_std, on=cols, how='left')\n",
    "    df[name_std].fillna(0,inplace=True)\n",
    "    df[name_std] = pd.to_numeric(df[name_std],downcast='float')\n",
    "    del block_std\n",
    "    \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "training = add_block_units_stats(training, ['item_id','date_block_num'], 'item_block')\n",
    "training = add_block_units_stats(training, ['shop_id','date_block_num'], 'shop_block')\n",
    "training = add_block_units_stats(training, ['item_category_id','date_block_num'], 'cat_block')\n",
    "training = add_block_units_stats(training, ['shop_id', 'item_category_id','date_block_num'], 'shop_cat_block')\n",
    "training = add_block_units_stats(training, ['shop_id', 'item_id','date_block_num'], 'shop_item_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_cat_block_units 3\n",
      "shop_cat_block_mean 3\n",
      "shop_cat_block_median 3\n",
      "shop_cat_block_min 3\n",
      "shop_cat_block_max 3\n",
      "shop_cat_block_std 3\n"
     ]
    }
   ],
   "source": [
    "def add_rolls(df, cols, name, rolls = [3]):\n",
    "    for roll in rolls:\n",
    "        print(name, roll)\n",
    "        roll_name = name+\"_rolling_\" + str(roll)\n",
    "        roll_name_tmp = roll_name + \"_tmp\"\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[roll_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "    \n",
    "        block_units_rolling_temp = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].rolling(roll,min_periods=2).mean().reset_index()\\\n",
    "            .rename(columns={name:roll_name_tmp})\\\n",
    "            [cols+[roll_name_tmp]]\n",
    "        \n",
    "    \n",
    "        df = df.merge(block_units_rolling_temp, on=cols, how='left')\n",
    "        #print(df.columns.values)\n",
    "        del block_units_rolling_temp\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "        block_units_rolling = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [roll_name_tmp].shift(1)\\\n",
    "            .rename(columns={roll_name_tmp:roll_name}).reset_index()\n",
    "\n",
    "        df = df.merge(block_units_rolling, on=cols, how='left')\n",
    "        df[roll_name].fillna(0,inplace=True)\n",
    "        df[roll_name] = pd.to_numeric(df[roll_name], downcast='float')\n",
    "        df.drop(columns=[roll_name_tmp], inplace=True)\n",
    "        del block_units_rolling\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_units')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_mean')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_median')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_min')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_max')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_std')\n",
    "\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_units')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_median')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_min')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_units')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_mean')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_median')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_min')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_max')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_std')\n",
    "\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')\n",
    "#training = add_rolls(training, ['shop_id','item_id','date_block_num'], 'shop_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training = add_rolls(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['block_total'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.sum)\n",
    "\n",
    "training['item_share_block'] = training['item_block_units'] * 100 / training['block_total']\n",
    "training['shop_share_block'] = training['shop_block_units'] * 100 / training['block_total']\n",
    "training['comp2'] = training['item_share_block'] * training['shop_share_block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_block_units 1\n",
      "shop_block_mean 1\n",
      "shop_block_median 1\n",
      "shop_block_min 1\n",
      "shop_block_max 1\n",
      "shop_block_std 1\n",
      "shop_cat_block_units 1\n",
      "shop_cat_block_mean 1\n",
      "shop_cat_block_median 1\n",
      "shop_cat_block_min 1\n",
      "shop_cat_block_max 1\n",
      "shop_cat_block_std 1\n",
      "shop_item_block_units 1\n",
      "shop_item_block_mean 1\n",
      "shop_item_block_median 1\n",
      "shop_item_block_min 1\n",
      "shop_item_block_max 1\n",
      "shop_item_block_std 1\n"
     ]
    }
   ],
   "source": [
    "def add_lags(df, cols, name, lags = [1]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "\n",
    "        df = df.merge(result, on=cols, how='left')\n",
    "        df[lag_name].fillna(0,inplace=True)\n",
    "        if \"mean\" in name or \"std\" in name:\n",
    "            df[lag_name] = pd.to_numeric(df[lag_name], downcast='float')\n",
    "        else:\n",
    "            df[lag_name] = pd.to_numeric(df[lag_name].astype(int), downcast='unsigned')\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_units')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_mean')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_median')                                        \n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_min')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_max')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_units')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_median')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_min')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_units')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_mean')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_median')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_min')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_max')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_median')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_min')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_max')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_share_block 1\n",
      "shop_share_block 1\n",
      "comp2 1\n"
     ]
    }
   ],
   "source": [
    "training = add_lags(training, ['item_id','date_block_num'], 'item_share_block')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_share_block')\n",
    "training = add_lags(training, ['shop_id', 'item_id', 'date_block_num'], 'comp2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_shops = training.groupby('shop_id')['item_cnt_block'].sum().sum()\n",
    "training['shop_share'] = training.groupby('shop_id')['item_cnt_block'].transform(np.sum) *100 / total_sum_shops\n",
    "\n",
    "total_sum_items = training.groupby('item_id')['item_cnt_block'].sum().sum()\n",
    "training['item_share'] = training.groupby('item_id')['item_cnt_block'].transform(np.sum) *100 / total_sum_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['comp1'] = training['shop_share'] * training['item_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_cnt_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>31</td>\n",
       "      <td>834.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>722.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  date_block_num  item_cnt_block\n",
       "0       41              24           974.0\n",
       "1       41              25           687.0\n",
       "2       41              26           670.0\n",
       "3       41              27           528.0\n",
       "4       41              28           726.0\n",
       "5       41              29           583.0\n",
       "6       41              30           655.0\n",
       "7       41              31           834.0\n",
       "8       41              32           629.0\n",
       "9       41              33           722.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = training[training['shop_id'] == 41].groupby(['shop_id', 'date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "shop_models = {}\n",
    "\n",
    "for shop_id in all_shop_ids:\n",
    "    \n",
    "    shop_data = training[training['shop_id'] == shop_id].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "\n",
    "    X = shop_data['date_block_num'].values.reshape(len(shop_data),1)\n",
    "    y = shop_data['item_cnt_block'].values.reshape(len(shop_data),1)\n",
    "    #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "    shop_models[shop_id] = regr\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "#print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "#print(\"Mean squared error: %.2f\"\n",
    " #     % mean_squared_error(X, preds))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "#print('Variance score: %.2f' % r2_score(X, preds))\n",
    "\n",
    "# Plot outputs\n",
    "#plt.scatter(X, y,  color='black')\n",
    "#plt.plot(X, preds, color='blue', linewidth=3)\n",
    "\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(shop_id, dbn):\n",
    "    return shop_models[shop_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['shop_pred'] = training.apply(lambda row: predict(row['shop_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "shop_cat_models = {}\n",
    "\n",
    "for shop_id in all_shop_ids:\n",
    "    shop_cat_models[shop_id] = {}\n",
    "    for cat_id in training['item_category_id'].unique():\n",
    "    \n",
    "        shop_cat_data = training[(training['shop_id'] == shop_id) & (training['item_category_id'] == cat_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "        if len(shop_cat_data) == 0:\n",
    "            continue\n",
    "\n",
    "        regr = linear_model.Ridge()\n",
    "\n",
    "        X = shop_cat_data['date_block_num'].values.reshape(len(shop_cat_data),1)\n",
    "        y = shop_cat_data['item_cnt_block'].values.reshape(len(shop_cat_data),1)\n",
    "        \n",
    "        #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X, y)\n",
    "        shop_cat_models[shop_id][cat_id] = regr\n",
    "            \n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(shop_id, cat_id, dbn):\n",
    "    return shop_cat_models[shop_id][cat_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['shop_cat_pred'] = training.apply(lambda row: predict(row['shop_id'],row['item_category_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "cat_models = {}\n",
    "\n",
    "for cat_id in training['item_category_id'].unique():\n",
    "    \n",
    "    cat_data = training[(training['item_category_id'] == cat_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "    if len(cat_data) == 0:\n",
    "        continue\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "\n",
    "    X = cat_data['date_block_num'].values.reshape(len(cat_data),1)\n",
    "    y = cat_data['item_cnt_block'].values.reshape(len(cat_data),1)\n",
    "\n",
    "    #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "    cat_models[cat_id] = regr\n",
    "\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(cat_id, dbn):\n",
    "    return cat_models[cat_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['cat_pred'] = training.apply(lambda row: predict(row['item_category_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.predict(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)\n",
    "training = training.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_val_len 29202\n",
      "zeros_keep_indices_val 116808\n",
      "non_zeros_val_indices 29202\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "ZEROS_KEEP=0.25\n",
    "\n",
    "#x_train = training[(training['date_block_num'] < 33) & (training['val_ignore'] == False)]\n",
    "x_train = training[(training['date_block_num'] < 33)]\n",
    "y_train = x_train['item_cnt_block']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_val = training[training['date_block_num'] == 33]\n",
    "y_val = x_val['item_cnt_block']\n",
    "\n",
    "pos_val_len = len(y_val[y_val != 0])\n",
    "print(\"pos_val_len\", pos_val_len)\n",
    "\n",
    "zeros_keep_indices_val = y_val[y_val == 0].sample(int(pos_val_len/ZEROS_KEEP)).index\n",
    "print(\"zeros_keep_indices_val\", len(zeros_keep_indices_val))\n",
    "non_zeros_val_indices = y_val[y_val != 0].index\n",
    "print(\"non_zeros_val_indices\", len(non_zeros_val_indices))\n",
    "\n",
    "val_indices = np.append(np.array(zeros_keep_indices_val), np.array(non_zeros_val_indices))\n",
    "\n",
    "y_val = y_val.loc[val_indices]\n",
    "x_val = x_val.loc[val_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \n",
    "'item_id_mean_encoding',\n",
    "       'shop_id_mean_encoding', #'item_category_id_mean_encoding',\n",
    "#       'month_mean_encoding', 'shop_cat_mean_encoding',\n",
    "#       'shop_item_mean_encoding', 'date_block_num_mean_encoding',\n",
    " \n",
    "#       'item_block_units_rolling_3', 'item_block_mean_rolling_3',\n",
    "#       'item_block_median_rolling_3', 'item_block_min_rolling_3',\n",
    "#       'item_block_max_rolling_3', 'item_block_std_rolling_3',\n",
    "    \n",
    "#       'shop_block_units_rolling_3', 'shop_block_mean_rolling_3',\n",
    "#       'shop_block_median_rolling_3', 'shop_block_min_rolling_3',\n",
    "#       'shop_block_max_rolling_3', 'shop_block_std_rolling_3',\n",
    "    \n",
    "#       'cat_block_units_rolling_3', 'cat_block_mean_rolling_3',\n",
    "#       'cat_block_median_rolling_3', 'cat_block_min_rolling_3',\n",
    "#       'cat_block_max_rolling_3', 'cat_block_std_rolling_3',\n",
    "    \n",
    "       'shop_cat_block_units_rolling_3', 'shop_cat_block_mean_rolling_3',\n",
    "       'shop_cat_block_median_rolling_3', 'shop_cat_block_min_rolling_3',\n",
    "       'shop_cat_block_max_rolling_3', 'shop_cat_block_std_rolling_3',\n",
    "    \n",
    "    \n",
    "#       'item_block_units_lag_1', 'item_block_mean_lag_1',\n",
    "#       'item_block_median_lag_1', 'item_block_min_lag_1',\n",
    "#       'item_block_max_lag_1', 'item_block_std_lag_1',\n",
    "    \n",
    "#       'shop_block_units_lag_1', 'shop_block_mean_lag_1',\n",
    "#       'shop_block_median_lag_1', 'shop_block_min_lag_1',\n",
    "       'shop_block_max_lag_1', 'shop_block_std_lag_1',\n",
    "    \n",
    "#      'cat_block_units_lag_1', 'cat_block_mean_lag_1',\n",
    "#       'cat_block_median_lag_1', 'cat_block_min_lag_1',\n",
    "#       'cat_block_max_lag_1', 'cat_block_std_lag_1',\n",
    "    \n",
    "       'shop_cat_block_units_lag_1', 'shop_cat_block_mean_lag_1',\n",
    "#       'shop_cat_block_median_lag_1', 'shop_cat_block_min_lag_1',\n",
    "#       'shop_cat_block_max_lag_1', 'shop_cat_block_std_lag_1',\n",
    "    \n",
    "       'shop_item_block_units_lag_1', 'shop_item_block_mean_lag_1',\n",
    "#       'shop_item_block_median_lag_1', 'shop_item_block_min_lag_1',\n",
    "#       'shop_item_block_max_lag_1', 'shop_item_block_std_lag_1',\n",
    "    \n",
    "       'item_share_block_lag_1', 'shop_share_block_lag_1', 'comp2_lag_1',\n",
    "    \n",
    "    'shop_pred', 'shop_cat_pred', 'cat_pred'\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['shop_item_mean_encoding',\n",
    " 'item_share_block',\n",
    " 'shop_pred',\n",
    " 'item_id_mean_encoding',\n",
    " 'shop_cat_mean_encoding',\n",
    " 'shop_cat_pred',\n",
    " 'comp2',\n",
    " 'comp1',\n",
    " 'item_category_id_mean_encoding',\n",
    " 'item_share',\n",
    " 'shop_id_mean_encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train_scaled = MinMaxScaler().fit_transform(x_train[features])\n",
    "x_val_scaled = MinMaxScaler().fit_transform(x_val[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  1.0696937476141704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression,BayesianRidge, HuberRegressor\n",
    "\n",
    "\n",
    "lr_model =  Ridge(alpha=0)\n",
    "lr_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "lr_val_preds = lr_model.predict(x_val_scaled)\n",
    "lr_val_preds.clip(0,20,out=lr_val_preds)\n",
    "rms = sqrt(mean_squared_error(y_val, lr_val_preds))\n",
    "print(\"rmse: \", rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.9808537 ,  12.343482  ,   1.84399312,   0.01577794,\n",
       "         8.16228545,   0.03875036,  -0.93422397,  13.64690203,\n",
       "        -2.98891795,  -7.24822478,  -1.77640351])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test            = pd.read_csv('test.csv.gz')\n",
    "test = test.set_index('item_id').join(items.set_index('item_id'))\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(shop_id):\n",
    "    return shop_models[shop_id].predict([[34]])[0][0]\n",
    "\n",
    "test['shop_pred'] = test.apply(lambda row: predict(row['shop_id']), axis=1)\n",
    "\n",
    "def predict(shop_id, cat_id):\n",
    "    if shop_id in shop_cat_models and cat_id in shop_cat_models[shop_id]:\n",
    "        return shop_cat_models[shop_id][cat_id].predict([[34]])[0][0]\n",
    "\n",
    "test['shop_cat_pred'] = test.apply(lambda row: predict(row['shop_id'],row['item_category_id']), axis=1)\n",
    "\n",
    "def predict(cat_id):\n",
    "    if cat_id in cat_models:\n",
    "        return cat_models[cat_id].predict([[34]])[0][0]\n",
    "\n",
    "test['cat_pred'] = test.apply(lambda row: predict(row['item_category_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = [ \n",
    "    'item_id_mean_encoding'\n",
    "                ]\n",
    "\n",
    "merge_col = ['item_id']\n",
    "cols=item_features+merge_col\n",
    "\n",
    "test = test.merge(training.drop_duplicates('item_id')[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_features = [\n",
    "        'shop_id_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id']\n",
    "cols=shop_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "        'item_category_id_mean_encoding'#,'cat_me_real'\n",
    "]\n",
    "\n",
    "merge_col = ['item_category_id']\n",
    "cols=cat_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_features = [\n",
    "        'shop_item_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id', 'item_id']\n",
    "cols=shop_item_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_cat_features = [\n",
    "        'shop_cat_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id', 'item_id']\n",
    "cols=shop_cat_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_block_mean 3\n",
      "[['item_id', 'item_block_mean_rolling_3']]\n",
      "item_block_units 3\n",
      "[['item_id', 'item_block_units_rolling_3']]\n",
      "shop_block_mean 3\n",
      "[['shop_id', 'shop_block_mean_rolling_3']]\n",
      "shop_cat_block_mean 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_mean_rolling_3']]\n",
      "shop_cat_block_units 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_units_rolling_3']]\n",
      "shop_cat_block_mean 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_mean_rolling_3']]\n",
      "shop_cat_block_median 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_median_rolling_3']]\n",
      "shop_cat_block_min 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_min_rolling_3']]\n",
      "shop_cat_block_max 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_max_rolling_3']]\n",
      "shop_cat_block_std 3\n",
      "[['shop_id', 'item_category_id', 'shop_cat_block_std_rolling_3']]\n"
     ]
    }
   ],
   "source": [
    "def add_rolls_test(df, cols, name, rolls = [3]):\n",
    "    for roll in rolls:\n",
    "        print(name, roll)\n",
    "        roll_name = name+\"_rolling_\" + str(roll)\n",
    "        roll_name_tmp = roll_name + \"_tmp\"\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[roll_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "    \n",
    "        block_units_rolling_temp = training\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].rolling(roll,min_periods=2).mean().reset_index()\\\n",
    "            .rename(columns={name:roll_name})\\\n",
    "            [cols+[roll_name]]\n",
    "        \n",
    "        print([cols[0:len(cols)-1]+[roll_name]])\n",
    "        thirty_three = block_units_rolling_temp[block_units_rolling_temp['date_block_num'] == 33].drop_duplicates(cols)\\\n",
    "                [cols[0:len(cols)-1]+[roll_name]]\n",
    "        df = df.merge(thirty_three, on=cols[0:len(cols)-1], how='left')\n",
    "    \n",
    "\n",
    "        del block_units_rolling_temp\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "test = add_rolls_test(test, ['item_id','date_block_num'], 'item_block_mean')\n",
    "test = add_rolls_test(test, ['item_id','date_block_num'], 'item_block_units')\n",
    "test = add_rolls_test(test, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_block_mean 3\n",
      "[['item_category_id', 'cat_block_mean_rolling_3']]\n"
     ]
    }
   ],
   "source": [
    "test = add_rolls_test(test, ['item_category_id','date_block_num'], 'cat_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_item_block_mean 3\n",
      "[['shop_id', 'item_id', 'shop_item_block_mean_rolling_3']]\n"
     ]
    }
   ],
   "source": [
    "test = add_rolls_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_block_mean 1\n",
      "item_block_units 1\n",
      "shop_block_mean 1\n",
      "shop_cat_block_mean 1\n",
      "shop_block_max 1\n",
      "shop_block_std 1\n",
      "shop_cat_block_units 1\n",
      "shop_item_block_units 1\n",
      "shop_item_block_mean 1\n",
      "item_share_block 1\n",
      "shop_share_block 1\n",
      "comp2 1\n"
     ]
    }
   ],
   "source": [
    "def add_lags_test(df, cols, name, lags = [1]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = training\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "        \n",
    "        thirty_three = result[result['date_block_num'] == 33].drop_duplicates(cols)\\\n",
    "                [cols[0:len(cols)-1] + [lag_name]]\n",
    "        df = df.merge(thirty_three, on=cols[0:len(cols)-1], how='left')\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_block_mean')\n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_block_units')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "test = add_lags_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "\n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_share_block')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_share_block')\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'comp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_item_block_mean 1\n"
     ]
    }
   ],
   "source": [
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "#test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_block_units 1\n"
     ]
    }
   ],
   "source": [
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['item_share_block' 'comp2' 'comp1' 'item_share' 'shop_share_block'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-3784f8d2a431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['item_share_block' 'comp2' 'comp1' 'item_share' 'shop_share_block'] not in index\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "test_scaled = MinMaxScaler().fit_transform(test[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (214200,20) and (41,) not aligned: 20 (dim 1) != 41 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-5a953265f685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    803\u001b[0m                                    dense_output=True) + self.intercept_\n\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElasticNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 198\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (214200,20) and (41,) not aligned: 20 (dim 1) != 41 (dim 0)"
     ]
    }
   ],
   "source": [
    "preds = lr_model.predict(test_scaled)\n",
    "preds.clip(0,20,out=preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.344650091219\n",
      "17.2619253419\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(preds))\n",
    "print(np.max(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(np.array([lg_preds, lr_preds]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr110.csv')['item_cnt_month']\n",
    "#lg_preds = pd.read_csv('lg111.csv')['item_cnt_month']\n",
    "cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (cb_preds * 0.75) + (lr_preds * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_id', 'shop_id', 'date_block_num', 'item_cnt_block',\n",
       "       'item_category_id', 'month', 'shop_cat', 'shop_item',\n",
       "       'shop_cat_int', 'shop_item_int', 'item_id_mean_encoding',\n",
       "       'shop_id_mean_encoding', 'item_category_id_mean_encoding',\n",
       "       'month_mean_encoding', 'shop_cat_mean_encoding',\n",
       "       'shop_item_mean_encoding', 'date_block_num_mean_encoding',\n",
       "       'item_block_units', 'item_block_median', 'item_block_mean',\n",
       "       'item_block_max', 'item_block_min', 'item_block_std',\n",
       "       'shop_block_units', 'shop_block_median', 'shop_block_mean',\n",
       "       'shop_block_max', 'shop_block_min', 'shop_block_std',\n",
       "       'cat_block_units', 'cat_block_median', 'cat_block_mean',\n",
       "       'cat_block_max', 'cat_block_min', 'cat_block_std',\n",
       "       'shop_cat_block_units', 'shop_cat_block_median',\n",
       "       'shop_cat_block_mean', 'shop_cat_block_max', 'shop_cat_block_min',\n",
       "       'shop_cat_block_std', 'shop_item_block_units',\n",
       "       'shop_item_block_median', 'shop_item_block_mean',\n",
       "       'shop_item_block_max', 'shop_item_block_min', 'shop_item_block_std',\n",
       "       'shop_cat_block_units_rolling_3', 'shop_cat_block_mean_rolling_3',\n",
       "       'shop_cat_block_median_rolling_3', 'shop_cat_block_min_rolling_3',\n",
       "       'shop_cat_block_max_rolling_3', 'shop_cat_block_std_rolling_3',\n",
       "       'block_total', 'item_share_block', 'shop_share_block', 'comp2',\n",
       "       'shop_block_units_lag_1', 'shop_block_mean_lag_1',\n",
       "       'shop_block_median_lag_1', 'shop_block_min_lag_1',\n",
       "       'shop_block_max_lag_1', 'shop_block_std_lag_1',\n",
       "       'shop_cat_block_units_lag_1', 'shop_cat_block_mean_lag_1',\n",
       "       'shop_cat_block_median_lag_1', 'shop_cat_block_min_lag_1',\n",
       "       'shop_cat_block_max_lag_1', 'shop_cat_block_std_lag_1',\n",
       "       'shop_item_block_units_lag_1', 'shop_item_block_mean_lag_1',\n",
       "       'shop_item_block_median_lag_1', 'shop_item_block_min_lag_1',\n",
       "       'shop_item_block_max_lag_1', 'shop_item_block_std_lag_1',\n",
       "       'item_share_block_lag_1', 'shop_share_block_lag_1', 'comp2_lag_1',\n",
       "       'shop_share', 'item_share', 'comp1', 'shop_pred', 'shop_cat_pred',\n",
       "       'cat_pred'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['item_id_mean_encoding',\n",
    "       'shop_id_mean_encoding', 'item_category_id_mean_encoding',\n",
    "       'month_mean_encoding', 'shop_cat_mean_encoding',\n",
    "       'shop_item_mean_encoding', 'date_block_num_mean_encoding',\n",
    "       'shop_cat_block_units_rolling_3', 'shop_cat_block_mean_rolling_3',\n",
    "       'shop_cat_block_median_rolling_3', 'shop_cat_block_min_rolling_3',\n",
    "       'shop_cat_block_max_rolling_3', 'shop_cat_block_std_rolling_3',\n",
    "       'comp2',\n",
    "       'shop_block_units_lag_1', 'shop_block_mean_lag_1',\n",
    "       'shop_block_median_lag_1', 'shop_block_min_lag_1',\n",
    "       'shop_block_max_lag_1', 'shop_block_std_lag_1',\n",
    "       'shop_cat_block_units_lag_1', 'shop_cat_block_mean_lag_1',\n",
    "       'shop_cat_block_median_lag_1', 'shop_cat_block_min_lag_1',\n",
    "       'shop_cat_block_max_lag_1', 'shop_cat_block_std_lag_1',\n",
    "       'shop_item_block_units_lag_1', 'shop_item_block_mean_lag_1',\n",
    "       'shop_item_block_median_lag_1', 'shop_item_block_min_lag_1',\n",
    "       'shop_item_block_max_lag_1', 'shop_item_block_std_lag_1',\n",
    "       'item_share_block_lag_1', 'shop_share_block_lag_1', 'comp2_lag_1',\n",
    "       'shop_share', 'item_share', 'comp1', 'shop_pred', 'shop_cat_pred',\n",
    "       'cat_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('item_id_mean_encoding', 0.046899917575753645)\n",
      "('shop_id_mean_encoding', 0.01311228648091441)\n",
      "('item_category_id_mean_encoding', 0.020159844697712443)\n",
      "('month_mean_encoding', 0.008151567813614766)\n",
      "('shop_cat_mean_encoding', 0.041484980410335326)\n",
      "('shop_item_mean_encoding', 0.37336418393939563)\n",
      "('date_block_num_mean_encoding', 0.008552125026527687)\n",
      "('shop_cat_block_units_rolling_3', 0.0068876810411972657)\n",
      "('shop_cat_block_mean_rolling_3', 0.0072239199897510403)\n",
      "('shop_cat_block_median_rolling_3', 0.0018358771559011742)\n",
      "('shop_cat_block_min_rolling_3', 0.00035568470494806091)\n",
      "('shop_cat_block_max_rolling_3', 0.0062678723017735262)\n",
      "('shop_cat_block_std_rolling_3', 0.0)\n",
      "('block_total', 0.0046403995685780561)\n",
      "('item_share_block', 0.2125236300417109)\n",
      "('shop_share_block', 0.0087984627674770454)\n",
      "('comp2', 0.02124897059744495)\n",
      "('shop_block_units_lag_1', 0.0064782552979276414)\n",
      "('shop_block_mean_lag_1', 0.006855511862743224)\n",
      "('shop_block_median_lag_1', 0.00013452944549075292)\n",
      "('shop_block_min_lag_1', 0.0)\n",
      "('shop_block_max_lag_1', 0.00085592663927022835)\n",
      "('shop_block_std_lag_1', 0.0)\n",
      "('shop_cat_block_units_lag_1', 0.0082378566189737819)\n",
      "('shop_cat_block_mean_lag_1', 0.010651928397563942)\n",
      "('shop_cat_block_median_lag_1', 0.0010628444023632884)\n",
      "('shop_cat_block_min_lag_1', 0.00028605669503926938)\n",
      "('shop_cat_block_max_lag_1', 0.0053243740337729382)\n",
      "('shop_cat_block_std_lag_1', 0.0)\n",
      "('shop_item_block_units_lag_1', 0.0038839034955234471)\n",
      "('shop_item_block_mean_lag_1', 0.0055412707440267621)\n",
      "('shop_item_block_median_lag_1', 0.0047967494603419229)\n",
      "('shop_item_block_min_lag_1', 0.0039456081897330077)\n",
      "('shop_item_block_max_lag_1', 0.0036538697753439064)\n",
      "('shop_item_block_std_lag_1', 0.0)\n",
      "('item_share_block_lag_1', 2.2292658398091258e-05)\n",
      "('shop_share_block_lag_1', 0.00084841600249562182)\n",
      "('comp2_lag_1', 1.5308336622900053e-05)\n",
      "('shop_share', 0.0054468991832277817)\n",
      "('item_share', 0.018695670324374695)\n",
      "('comp1', 0.021212541511230736)\n",
      "('shop_pred', 0.066771361169548279)\n",
      "('shop_cat_pred', 0.032293409889917343)\n",
      "('cat_pred', 0.011478011753034584)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestRegressor(n_estimators=10, random_state=0, n_jobs=8)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(x_train[all_features], y_train)\n",
    "\n",
    "# Print the name and gini importance of each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, uint16, float32, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, uint16, float32, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  1.4094862094418104\n"
     ]
    }
   ],
   "source": [
    "f_ = sorted(list(zip(all_features, clf.feature_importances_)), key=lambda x: x[1])[::-1][0:]\n",
    "\n",
    "features = [f[0] for f in f_]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train_scaled = MinMaxScaler().fit_transform(x_train[features])\n",
    "x_val_scaled = MinMaxScaler().fit_transform(x_val[features])\n",
    "\n",
    "\n",
    "lr_model =  Lasso()#(alpha=0)\n",
    "lr_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "lr_val_preds = lr_model.predict(x_val_scaled)\n",
    "lr_val_preds.clip(0,20,out=lr_val_preds)\n",
    "rms = sqrt(mean_squared_error(y_val, lr_val_preds))\n",
    "print(\"rmse: \", rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f = all_features\n",
    "for feature in all_f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "       -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
