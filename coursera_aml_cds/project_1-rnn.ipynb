{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-32d652d11334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c19059620915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "items           = pd.read_csv('items.csv',usecols=[\"item_id\", \"item_category_id\"])\n",
    "item_categories = pd.read_csv('item_categories.csv')\n",
    "shops           = pd.read_csv('shops.csv')\n",
    "sales_train     = pd.read_csv('sales_train.csv.gz')\n",
    "test            = pd.read_csv('test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train[['day','month', 'year']] = sales_train['date'].str.split('.', expand=True).astype(int)\n",
    "sales_train = sales_train[sales_train['year'].isin([2013,2014]) == False]\n",
    "sales_train = sales_train.set_index('item_id').join(items.set_index('item_id'))\n",
    "sales_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "sales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "sales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "sales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales=1000\n",
    "sums = sales_train.groupby('item_id')['item_cnt_day'].sum().reset_index().rename(columns={\"item_cnt_day\":\"item_total_sales\"}).sort_values(by='item_total_sales')\n",
    "\n",
    "ids_reject = sums[(sums['item_total_sales'] > 0) & (sums['item_total_sales'] < max_sales)]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids = sales_train['item_id'].unique()\n",
    "train_item_ids = np.setdiff1d(train_item_ids, ids_reject)\n",
    "train_shop_ids = sales_train['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "test_shop_ids = test['shop_id'].unique()\n",
    "train_blocks = sales_train['date_block_num'].unique()\n",
    "\n",
    "all_item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "all_shop_ids = np.unique(np.append(train_shop_ids,test_shop_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for dbn in range(np.min(train_blocks), np.max(train_blocks)+1):\n",
    "    sales = sales_train[sales_train.date_block_num==dbn]\n",
    "    item_ids = np.intersect1d(sales.item_id.unique(), test_item_ids)\n",
    "    dbn_combos = list(product(sales.shop_id.unique(), item_ids, [dbn]))\n",
    "    for combo in dbn_combos:\n",
    "        combinations.append(combo)\n",
    "        \n",
    "all_combos = pd.DataFrame(np.unique(np.vstack([combinations]), axis=0), columns=['shop_id','item_id','date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)['item_cnt_day']\\\n",
    "                .sum().rename(columns={\"item_cnt_day\":\"item_cnt_block\"})\n",
    "\n",
    "training = all_combos.merge(ys, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "\n",
    "\n",
    "training['item_cnt_block'] = training['item_cnt_block'].clip(0,20).astype('int8')\n",
    "\n",
    "training = training.set_index('item_id').join(items.set_index('item_id'))\n",
    "training.reset_index(inplace=True)\n",
    "\n",
    "for col in ['item_id', 'shop_id', 'item_category_id']:\n",
    "    training[col] = pd.to_numeric(training[col], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train[['date_block_num', 'month', 'year']].drop_duplicates(['date_block_num', 'month', 'year'])\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "for index,row in dates.iterrows():\n",
    "    dates_dict[row['date_block_num']] = {\"month\": row['month'], \"year\": row['year']}\n",
    "    \n",
    "training['month'] = pd.to_numeric(training['date_block_num'].apply(lambda block: dates_dict[block]['month']), downcast='unsigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"shop_cat\"] = training[\"shop_id\"].astype(str) + \"_\" + training[\"item_category_id\"].astype(str)\n",
    "training[\"shop_item\"] = training[\"shop_id\"].astype(str) + \"_\" + training[\"item_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shop_cats = training['shop_cat'].unique()\n",
    "shop_cats = dict(list(zip(unique_shop_cats, range(1,len(unique_shop_cats)))))\n",
    "\n",
    "def get_shop_cat_int(x):\n",
    "    if x in shop_cats:\n",
    "        return shop_cats[x]\n",
    "\n",
    "training['shop_cat_int'] = training['shop_cat'].apply(lambda x: get_shop_cat_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shop_items = training['shop_item'].unique()\n",
    "shop_items = dict(list(zip(unique_shop_items, range(1,len(unique_shop_items)))))\n",
    "\n",
    "def get_shop_item_int(x):\n",
    "    if x in shop_items:\n",
    "        return shop_items[x]\n",
    "\n",
    "training['shop_item_int'] = training['shop_item'].apply(lambda x: get_shop_item_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#https://maxhalford.github.io/blog/target-encoding-done-the-right-way/\n",
    "#https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "columns = [\"item_id\", \"shop_id\", \"item_category_id\", \"month\", \"shop_cat\", \"shop_item\", \"date_block_num\"]\n",
    "\n",
    "\n",
    "\n",
    "y_train = training[\"item_cnt_block\"].values\n",
    "folds = KFold(n_splits = 5, shuffle=True).split(training)\n",
    "\n",
    "i=1\n",
    "for in_fold_index, out_of_fold_index in folds:\n",
    "    print(\"fold\", i)\n",
    "    #print(np.intersect1d(training.loc[in_fold_index][\"shop_id\"].unique(), training.loc[out_of_fold_index][\"shop_id\"].unique()))\n",
    "    #print(len(in_fold_index))\n",
    "    for column in columns:\n",
    "        means = training.iloc[in_fold_index].groupby(column)['item_cnt_block'].mean()\n",
    "            #x_validation[column + \"_mean_target\"] = means\\\n",
    "        name = column + '_mean_encoding'\n",
    "        training.loc[out_of_fold_index,name] = training.loc[out_of_fold_index][column].map(means)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/.local/lib/python3.6/site-packages/pandas/core/reshape/merge.py:946: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
      "  'representation', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_block\n",
      "cat_block\n",
      "shop_cat_block\n",
      "shop_item_block\n"
     ]
    }
   ],
   "source": [
    "def add_block_units_stats(df, cols, name):\n",
    "    print(name)\n",
    "    name_units = name + '_units'\n",
    "    name_mean = name + '_mean'\n",
    "    name_median = name + '_median'\n",
    "    name_max = name + '_max'\n",
    "    name_min = name + '_min'\n",
    "    name_std = name + '_std'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df.drop(columns=[name_units, name_mean, name_median],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    block_units = df.groupby(cols,as_index=False)['item_cnt_block'].sum()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_units})\n",
    "    df = df.merge(block_units, on=cols, how='left')\n",
    "    df[name_units].fillna(0,inplace=True)\n",
    "    df[name_units] = pd.to_numeric(df[name_units].astype(int),downcast='unsigned')\n",
    "    del block_units\n",
    "    \n",
    "    block_units_med = df.groupby(cols,as_index=False)['item_cnt_block'].median()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_median})\n",
    "    df = df.merge(block_units_med, on=cols, how='left')\n",
    "    df[name_median].fillna(0,inplace=True)\n",
    "    df[name_median] = pd.to_numeric(df[name_median].astype(int),downcast='unsigned')\n",
    "    del block_units_med\n",
    "    \n",
    "    block_means = df.groupby(cols,as_index=False)['item_cnt_block'].mean()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_mean})\n",
    "    df = df.merge(block_means, on=cols, how='left')\n",
    "    df[name_mean].fillna(0,inplace=True)\n",
    "    df[name_mean] = pd.to_numeric(df[name_mean],downcast='float')\n",
    "    del block_means\n",
    "    \n",
    "    block_max = df.groupby(cols,as_index=False)['item_cnt_block'].max()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_max})\n",
    "    df = df.merge(block_max, on=cols, how='left')\n",
    "    df[name_max].fillna(0,inplace=True)\n",
    "    df[name_max] = pd.to_numeric(df[name_max],downcast='float')\n",
    "    del block_max\n",
    "    \n",
    "    block_min = df.groupby(cols,as_index=False)['item_cnt_block'].min()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_min})\n",
    "    df = df.merge(block_min, on=cols, how='left')\n",
    "    df[name_min].fillna(0,inplace=True)\n",
    "    df[name_min] = pd.to_numeric(df[name_min],downcast='float')\n",
    "    del block_min\n",
    "    \n",
    "    block_std = df.groupby(cols,as_index=False)['item_cnt_block'].std()\\\n",
    "                        .drop_duplicates(cols)\\\n",
    "                        .rename(columns={'item_cnt_block':name_std})\n",
    "    df = df.merge(block_std, on=cols, how='left')\n",
    "    df[name_std].fillna(0,inplace=True)\n",
    "    df[name_std] = pd.to_numeric(df[name_std],downcast='float')\n",
    "    del block_std\n",
    "    \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "training = add_block_units_stats(training, ['item_id','date_block_num'], 'item_block')\n",
    "training = add_block_units_stats(training, ['shop_id','date_block_num'], 'shop_block')\n",
    "training = add_block_units_stats(training, ['item_category_id','date_block_num'], 'cat_block')\n",
    "training = add_block_units_stats(training, ['shop_id', 'item_category_id','date_block_num'], 'shop_cat_block')\n",
    "training = add_block_units_stats(training, ['shop_id', 'item_id','date_block_num'], 'shop_item_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_cat_block_units 3\n",
      "shop_cat_block_mean 3\n",
      "shop_cat_block_median 3\n",
      "shop_cat_block_min 3\n",
      "shop_cat_block_max 3\n",
      "shop_cat_block_std 3\n"
     ]
    }
   ],
   "source": [
    "def add_rolls(df, cols, name, rolls = [3]):\n",
    "    for roll in rolls:\n",
    "        print(name, roll)\n",
    "        roll_name = name+\"_rolling_\" + str(roll)\n",
    "        roll_name_tmp = roll_name + \"_tmp\"\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[roll_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "    \n",
    "        block_units_rolling_temp = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].rolling(roll,min_periods=2).mean().reset_index()\\\n",
    "            .rename(columns={name:roll_name_tmp})\\\n",
    "            [cols+[roll_name_tmp]]\n",
    "        \n",
    "    \n",
    "        df = df.merge(block_units_rolling_temp, on=cols, how='left')\n",
    "        #print(df.columns.values)\n",
    "        del block_units_rolling_temp\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "        block_units_rolling = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [roll_name_tmp].shift(1)\\\n",
    "            .rename(columns={roll_name_tmp:roll_name}).reset_index()\n",
    "\n",
    "        df = df.merge(block_units_rolling, on=cols, how='left')\n",
    "        df[roll_name].fillna(0,inplace=True)\n",
    "        df[roll_name] = pd.to_numeric(df[roll_name], downcast='float')\n",
    "        df.drop(columns=[roll_name_tmp], inplace=True)\n",
    "        del block_units_rolling\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_units')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_mean')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_median')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_min')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_max')\n",
    "#training = add_rolls(training, ['item_id','date_block_num'], 'item_block_std')\n",
    "\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_units')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_median')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_min')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "#training = add_rolls(training, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_units')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_mean')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_median')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_min')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_max')\n",
    "#training = add_rolls(training, ['item_category_id','date_block_num'], 'cat_block_std')\n",
    "\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "training = add_rolls(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')\n",
    "#training = add_rolls(training, ['shop_id','item_id','date_block_num'], 'shop_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training = add_rolls(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['block_total'] = training.groupby(['date_block_num'])['item_cnt_block'].transform(np.sum)\n",
    "\n",
    "training['item_share_block'] = training['item_block_units'] * 100 / training['block_total']\n",
    "training['shop_share_block'] = training['shop_block_units'] * 100 / training['block_total']\n",
    "training['comp2'] = training['item_share_block'] * training['shop_share_block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop_block_units 1\n",
      "shop_block_mean 1\n",
      "shop_block_median 1\n",
      "shop_block_min 1\n",
      "shop_block_max 1\n",
      "shop_block_std 1\n",
      "shop_cat_block_units 1\n",
      "shop_cat_block_mean 1\n",
      "shop_cat_block_median 1\n",
      "shop_cat_block_min 1\n",
      "shop_cat_block_max 1\n",
      "shop_cat_block_std 1\n",
      "shop_item_block_units 1\n",
      "shop_item_block_mean 1\n",
      "shop_item_block_median 1\n",
      "shop_item_block_min 1\n",
      "shop_item_block_max 1\n",
      "shop_item_block_std 1\n"
     ]
    }
   ],
   "source": [
    "def add_lags(df, cols, name, lags = [1]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = df\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "\n",
    "        df = df.merge(result, on=cols, how='left')\n",
    "        df[lag_name].fillna(0,inplace=True)\n",
    "        if \"mean\" in name or \"std\" in name:\n",
    "            df[lag_name] = pd.to_numeric(df[lag_name], downcast='float')\n",
    "        else:\n",
    "            df[lag_name] = pd.to_numeric(df[lag_name].astype(int), downcast='unsigned')\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_units')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_mean')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_median')                                        \n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_min')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_max')\n",
    "#training = add_lags(training, ['item_id','date_block_num'], 'item_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_units')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_median')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_min')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_units')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_mean')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_median')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_min')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_max')\n",
    "#training = add_lags(training, ['item_category_id','date_block_num'], 'cat_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "training = add_lags(training, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')\n",
    "\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_median')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_min')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_max')\n",
    "training = add_lags(training, ['shop_id','item_id','date_block_num'], 'shop_item_block_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_share_block 1\n",
      "shop_share_block 1\n",
      "comp2 1\n"
     ]
    }
   ],
   "source": [
    "training = add_lags(training, ['item_id','date_block_num'], 'item_share_block')\n",
    "training = add_lags(training, ['shop_id','date_block_num'], 'shop_share_block')\n",
    "training = add_lags(training, ['shop_id', 'item_id', 'date_block_num'], 'comp2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_shops = training.groupby('shop_id')['item_cnt_block'].sum().sum()\n",
    "training['shop_share'] = training.groupby('shop_id')['item_cnt_block'].transform(np.sum) *100 / total_sum_shops\n",
    "\n",
    "total_sum_items = training.groupby('item_id')['item_cnt_block'].sum().sum()\n",
    "training['item_share'] = training.groupby('item_id')['item_cnt_block'].transform(np.sum) *100 / total_sum_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['comp1'] = training['shop_share'] * training['item_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_cnt_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>31</td>\n",
       "      <td>834.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>722.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  date_block_num  item_cnt_block\n",
       "0       41              24           974.0\n",
       "1       41              25           687.0\n",
       "2       41              26           670.0\n",
       "3       41              27           528.0\n",
       "4       41              28           726.0\n",
       "5       41              29           583.0\n",
       "6       41              30           655.0\n",
       "7       41              31           834.0\n",
       "8       41              32           629.0\n",
       "9       41              33           722.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = training[training['shop_id'] == 41].groupby(['shop_id', 'date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "shop_models = {}\n",
    "\n",
    "for shop_id in all_shop_ids:\n",
    "    \n",
    "    shop_data = training[training['shop_id'] == shop_id].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "\n",
    "    X = shop_data['date_block_num'].values.reshape(len(shop_data),1)\n",
    "    y = shop_data['item_cnt_block'].values.reshape(len(shop_data),1)\n",
    "    #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "    shop_models[shop_id] = regr\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "#print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "#print(\"Mean squared error: %.2f\"\n",
    " #     % mean_squared_error(X, preds))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "#print('Variance score: %.2f' % r2_score(X, preds))\n",
    "\n",
    "# Plot outputs\n",
    "#plt.scatter(X, y,  color='black')\n",
    "#plt.plot(X, preds, color='blue', linewidth=3)\n",
    "\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(shop_id, dbn):\n",
    "    return shop_models[shop_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['shop_pred'] = training.apply(lambda row: predict(row['shop_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "shop_cat_models = {}\n",
    "\n",
    "for shop_id in all_shop_ids:\n",
    "    shop_cat_models[shop_id] = {}\n",
    "    for cat_id in training['item_category_id'].unique():\n",
    "    \n",
    "        shop_cat_data = training[(training['shop_id'] == shop_id) & (training['item_category_id'] == cat_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "        if len(shop_cat_data) == 0:\n",
    "            continue\n",
    "\n",
    "        regr = linear_model.Ridge()\n",
    "\n",
    "        X = shop_cat_data['date_block_num'].values.reshape(len(shop_cat_data),1)\n",
    "        y = shop_cat_data['item_cnt_block'].values.reshape(len(shop_cat_data),1)\n",
    "        \n",
    "        #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X, y)\n",
    "        shop_cat_models[shop_id][cat_id] = regr\n",
    "            \n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(shop_id, cat_id, dbn):\n",
    "    return shop_cat_models[shop_id][cat_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['shop_cat_pred'] = training.apply(lambda row: predict(row['shop_id'],row['item_category_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "shop_item_models = {}\n",
    "\n",
    "for shop_id in all_shop_ids:\n",
    "    print(shop_id)\n",
    "    shop_item_models[shop_id] = {}\n",
    "    for item_id in training['item_id'].unique():\n",
    "    \n",
    "        shop_item_data = training[(training['shop_id'] == shop_id) & (training['item_id'] == item_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "        if len(shop_item_data) == 0:\n",
    "            continue\n",
    "\n",
    "        regr = linear_model.Ridge()\n",
    "\n",
    "        X = shop_item_data['date_block_num'].values.reshape(len(shop_item_data),1)\n",
    "        y = shop_item_data['item_cnt_block'].values.reshape(len(shop_item_data),1)\n",
    "        \n",
    "        #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X, y)\n",
    "        shop_item_models[shop_id][item_id] = regr\n",
    "            \n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(shop_id, item_id, dbn):\n",
    "    return shop_item_models[shop_id][item_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['shop_item_pred'] = training.apply(lambda row: predict(row['shop_id'],row['item_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "`\n",
    "cat_models = {}\n",
    "\n",
    "for cat_id in training['item_category_id'].unique():\n",
    "    \n",
    "    cat_data = training[(training['item_category_id'] == cat_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "    if len(cat_data) == 0:\n",
    "        continue\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "\n",
    "    X = cat_data['date_block_num'].values.reshape(len(cat_data),1)\n",
    "    y = cat_data['item_cnt_block'].values.reshape(len(cat_data),1)\n",
    "\n",
    "    #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "    cat_models[cat_id] = regr\n",
    "\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(cat_id, dbn):\n",
    "    return cat_models[cat_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['cat_pred'] = training.apply(lambda row: predict(row['item_category_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "item_models = {}\n",
    "\n",
    "for item_id in training['item_id'].unique():\n",
    "    \n",
    "    item_data = training[(training['item_id'] == item_id)].groupby(['date_block_num'],as_index=False)['item_cnt_block'].sum()\n",
    "    if len(item_data) == 0:\n",
    "        continue\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "\n",
    "    X = item_data['date_block_num'].values.reshape(len(item_data),1)\n",
    "    y = item_data['item_cnt_block'].values.reshape(len(item_data),1)\n",
    "\n",
    "    #y = MinMaxScaler().fit_transform(y)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "    item_models[item_id] = regr\n",
    "\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    #preds = regr.predict(X)\n",
    "\n",
    "\n",
    "print(\"applying\")\n",
    "\n",
    "def predict(item_id, dbn):\n",
    "    if item_id in item_models:\n",
    "        return item_models[item_id].predict([[dbn]])[0][0]\n",
    "\n",
    "training['item_pred'] = training.apply(lambda row: predict(row['item_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['pred_comp1'] = training['item_pred'] * training['shop_pred']\n",
    "training['pred_comp2'] = training['shop_pred'] * training['cat_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['blocks_without_sales'] = training['item_id'].map(training[training['item_cnt_block'] == 0].groupby(['item_id'])['date_block_num'].unique().apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train['item_days_of_activity'] = pd.to_numeric(sales_train.groupby(['item_id'])['date'].transform(\"nunique\"), downcast='unsigned') \n",
    "sales_train['item_blocks_of_activity'] = pd.to_numeric(sales_train.groupby(['item_id'])['date_block_num'].transform(\"nunique\"), downcast='unsigned') \n",
    "\n",
    "def get_number_of_days_since_start(day,month, year):\n",
    "    days = 0\n",
    "    if year == 2015:\n",
    "        days = 365\n",
    "    def is_even(num):\n",
    "        return num % 2 == 0\n",
    "    half_of_month = int(month/2)\n",
    "    even = (30*half_of_month) + (31*half_of_month)\n",
    "    if is_even(month):\n",
    "        days = days + even - 30 - day\n",
    "    else:\n",
    "        days = days + even + day\n",
    "    return days\n",
    "\n",
    "sales_train['item_days_since_start'] = pd.to_numeric(sales_train.apply(lambda row: get_number_of_days_since_start(row['day'],row['month'], row['year']),axis=1), downcast='unsigned') \n",
    "\n",
    "def get_average_days_between_sales(days):\n",
    "    days = sorted(np.unique(days))\n",
    "    if len(days) == 0:\n",
    "        return 9999\n",
    "    if len(days) == 1:\n",
    "        return 999\n",
    "    return np.mean(np.ediff1d(days)) / len(days)\n",
    "\n",
    "average_days_between_sales = sales_train.groupby(['item_id'])['item_days_since_start'].apply(list).apply(lambda x: get_average_days_between_sales(x))\n",
    "\n",
    "sales_train['item_mean_day_between_activity'] = pd.to_numeric(sales_train['item_id'].map(average_days_between_sales), downcast='unsigned') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['item_mean_day_between_activity'] = training['item_id'].map(sales_train.drop_duplicates('item_id').set_index('item_id')['item_mean_day_between_activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.fillna(0,inplace=True)\n",
    "training = training.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "ZEROS_KEEP=0.25\n",
    "\n",
    "#x_train = training[(training['date_block_num'] < 33) & (training['val_ignore'] == False)]\n",
    "x_train = training[(training['date_block_num'] < 33)]\n",
    "y_train = x_train['item_cnt_block']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_val = training[training['date_block_num'] == 33]\n",
    "y_val = x_val['item_cnt_block']\n",
    "\n",
    "pos_val_len = len(y_val[y_val != 0])\n",
    "print(\"pos_val_len\", pos_val_len)\n",
    "\n",
    "zeros_keep_indices_val = y_val[y_val == 0].sample(int(pos_val_len/ZEROS_KEEP)).index\n",
    "print(\"zeros_keep_indices_val\", len(zeros_keep_indices_val))\n",
    "non_zeros_val_indices = y_val[y_val != 0].index\n",
    "print(\"non_zeros_val_indices\", len(non_zeros_val_indices))\n",
    "\n",
    "val_indices = np.append(np.array(zeros_keep_indices_val), np.array(non_zeros_val_indices))\n",
    "\n",
    "y_val = y_val.loc[val_indices]\n",
    "x_val = x_val.loc[val_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \n",
    "'item_id_mean_encoding',\n",
    "       'shop_id_mean_encoding', #'item_category_id_mean_encoding',\n",
    "#       'month_mean_encoding', 'shop_cat_mean_encoding',\n",
    "#       'shop_item_mean_encoding', 'date_block_num_mean_encoding',\n",
    " \n",
    "#       'item_block_units_rolling_3', 'item_block_mean_rolling_3',\n",
    "#       'item_block_median_rolling_3', 'item_block_min_rolling_3',\n",
    "#       'item_block_max_rolling_3', 'item_block_std_rolling_3',\n",
    "    \n",
    "#       'shop_block_units_rolling_3', 'shop_block_mean_rolling_3',\n",
    "#       'shop_block_median_rolling_3', 'shop_block_min_rolling_3',\n",
    "#       'shop_block_max_rolling_3', 'shop_block_std_rolling_3',\n",
    "    \n",
    "#       'cat_block_units_rolling_3', 'cat_block_mean_rolling_3',\n",
    "#       'cat_block_median_rolling_3', 'cat_block_min_rolling_3',\n",
    "#       'cat_block_max_rolling_3', 'cat_block_std_rolling_3',\n",
    "    \n",
    "       'shop_cat_block_units_rolling_3', 'shop_cat_block_mean_rolling_3',\n",
    "       'shop_cat_block_median_rolling_3', 'shop_cat_block_min_rolling_3',\n",
    "       'shop_cat_block_max_rolling_3', 'shop_cat_block_std_rolling_3',\n",
    "    \n",
    "    \n",
    "#       'item_block_units_lag_1', 'item_block_mean_lag_1',\n",
    "#       'item_block_median_lag_1', 'item_block_min_lag_1',\n",
    "#       'item_block_max_lag_1', 'item_block_std_lag_1',\n",
    "    \n",
    "#       'shop_block_units_lag_1', 'shop_block_mean_lag_1',\n",
    "#       'shop_block_median_lag_1', 'shop_block_min_lag_1',\n",
    "       'shop_block_max_lag_1', 'shop_block_std_lag_1',\n",
    "    \n",
    "#      'cat_block_units_lag_1', 'cat_block_mean_lag_1',\n",
    "#       'cat_block_median_lag_1', 'cat_block_min_lag_1',\n",
    "#       'cat_block_max_lag_1', 'cat_block_std_lag_1',\n",
    "    \n",
    "       'shop_cat_block_units_lag_1', 'shop_cat_block_mean_lag_1',\n",
    "#       'shop_cat_block_median_lag_1', 'shop_cat_block_min_lag_1',\n",
    "#       'shop_cat_block_max_lag_1', 'shop_cat_block_std_lag_1',\n",
    "    \n",
    "       'shop_item_block_units_lag_1', 'shop_item_block_mean_lag_1',\n",
    "#       'shop_item_block_median_lag_1', 'shop_item_block_min_lag_1',\n",
    "#       'shop_item_block_max_lag_1', 'shop_item_block_std_lag_1',\n",
    "    \n",
    "       'item_share_block_lag_1', 'shop_share_block_lag_1', 'comp2_lag_1',\n",
    "    \n",
    "    'shop_pred', \n",
    "    'shop_cat_pred', \n",
    "    'cat_pred',\n",
    "    #'item_pred',\n",
    "    #'pred_comp1',\n",
    "    'pred_comp2',\n",
    "    'blocks_without_sales',\n",
    "    #'item_mean_day_between_activity'\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train_scaled = MinMaxScaler().fit_transform(x_train[features])\n",
    "x_val_scaled = MinMaxScaler().fit_transform(x_val[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression,BayesianRidge, HuberRegressor\n",
    "\n",
    "\n",
    "lr_model =  Ridge(alpha=0.1)\n",
    "lr_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "lr_val_preds = lr_model.predict(x_val_scaled)\n",
    "lr_val_preds.clip(0,20,out=lr_val_preds)\n",
    "rms = sqrt(mean_squared_error(y_val, lr_val_preds))\n",
    "print(\"rmse: \", rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test            = pd.read_csv('test.csv.gz')\n",
    "test = test.set_index('item_id').join(items.set_index('item_id'))\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = [ \n",
    "    'item_id_mean_encoding'\n",
    "                ]\n",
    "\n",
    "merge_col = ['item_id']\n",
    "cols=item_features+merge_col\n",
    "\n",
    "test = test.merge(training.drop_duplicates('item_id')[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_features = [\n",
    "        'shop_id_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id']\n",
    "cols=shop_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "        'item_category_id_mean_encoding'#,'cat_me_real'\n",
    "]\n",
    "\n",
    "merge_col = ['item_category_id']\n",
    "cols=cat_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_features = [\n",
    "        'shop_item_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id', 'item_id']\n",
    "cols=shop_item_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_cat_features = [\n",
    "        'shop_cat_mean_encoding'\n",
    "]\n",
    "\n",
    "merge_col = ['shop_id', 'item_id']\n",
    "cols=shop_cat_features+merge_col\n",
    "\n",
    "\n",
    "test = test.merge(training.drop_duplicates(merge_col)[cols], on=merge_col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolls_test(df, cols, name, rolls = [3]):\n",
    "    for roll in rolls:\n",
    "        print(name, roll)\n",
    "        roll_name = name+\"_rolling_\" + str(roll)\n",
    "        roll_name_tmp = roll_name + \"_tmp\"\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[roll_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "    \n",
    "        block_units_rolling_temp = training\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].rolling(roll,min_periods=2).mean().reset_index()\\\n",
    "            .rename(columns={name:roll_name})\\\n",
    "            [cols+[roll_name]]\n",
    "        \n",
    "        print([cols[0:len(cols)-1]+[roll_name]])\n",
    "        thirty_three = block_units_rolling_temp[block_units_rolling_temp['date_block_num'] == 33].drop_duplicates(cols)\\\n",
    "                [cols[0:len(cols)-1]+[roll_name]]\n",
    "        df = df.merge(thirty_three, on=cols[0:len(cols)-1], how='left')\n",
    "    \n",
    "\n",
    "        del block_units_rolling_temp\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "test = add_rolls_test(test, ['item_id','date_block_num'], 'item_block_mean')\n",
    "test = add_rolls_test(test, ['item_id','date_block_num'], 'item_block_units')\n",
    "test = add_rolls_test(test, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_median')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_min')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_max')\n",
    "test = add_rolls_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = add_rolls_test(test, ['item_category_id','date_block_num'], 'cat_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = add_rolls_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags_test(df, cols, name, lags = [1]):\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(name, lag)\n",
    "        lag_name = name + \"_lag_\" + str(lag)\n",
    "        \n",
    "        try:\n",
    "            df.drop(columns=[lag_name],inplace=True)\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "        result = training\\\n",
    "            .drop_duplicates(cols)\\\n",
    "            .sort_values(cols)\\\n",
    "            .set_index(cols)\\\n",
    "            .groupby(cols[0:len(cols)-1],as_index=False)\\\n",
    "            [name].shift(lag)\\\n",
    "            .rename(columns={name:lag_name}).reset_index()\n",
    "        \n",
    "        thirty_three = result[result['date_block_num'] == 33].drop_duplicates(cols)\\\n",
    "                [cols[0:len(cols)-1] + [lag_name]]\n",
    "        df = df.merge(thirty_three, on=cols[0:len(cols)-1], how='left')\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "    return df\n",
    "                                         \n",
    "\n",
    "                                        \n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_block_mean')\n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_block_units')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_mean')\n",
    "test = add_lags_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_mean')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_max')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_std')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','item_category_id','date_block_num'], 'shop_cat_block_units')\n",
    "\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "\n",
    "test = add_lags_test(test, ['item_id','date_block_num'], 'item_share_block')\n",
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_share_block')\n",
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'comp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_mean')\n",
    "#test = add_lags_test(test, ['shop_id','item_id','date_block_num'], 'shop_item_block_units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = add_lags_test(test, ['shop_id','date_block_num'], 'shop_block_units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(shop_id):\n",
    "    return shop_models[shop_id].predict(34)[0][0]\n",
    "\n",
    "test['shop_pred'] = test.apply(lambda row: predict(row['shop_id']), axis=1)\n",
    "#training['shop_cat_pred'] = training.apply(lambda row: predict(row['shop_id'],row['item_category_id'], row['date_block_num']), axis=1)\n",
    "#training['cat_pred'] = training.apply(lambda row: predict(row['item_category_id'], row['date_block_num']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(shop_id, cat_id):\n",
    "    if shop_id in shop_cat_models and cat_id in shop_cat_models[shop_id]:\n",
    "        return shop_cat_models[shop_id][cat_id].predict(34)[0][0]\n",
    "\n",
    "test['shop_cat_pred'] = test.apply(lambda row: predict(row['shop_id'],row['item_category_id']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(cat_id):\n",
    "    if cat_id in cat_models:\n",
    "        return cat_models[cat_id].predict(34)[0][0]\n",
    "\n",
    "test['cat_pred'] = test.apply(lambda row: predict(row['item_category_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "test_scaled = MinMaxScaler().fit_transform(test[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_model.predict(test_scaled)\n",
    "lr_preds.clip(0,20,out=lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(lr_preds))\n",
    "print(np.max(lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lg_preds.conc lr_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(np.array([lg_preds, lr_preds]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.loc[:,['ID']]\n",
    "submission['item_cnt_month'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = pd.read_csv('lr111.csv')['item_cnt_month']\n",
    "lg_preds = pd.read_csv('lg111.csv')['item_cnt_month']\n",
    "cb_preds = pd.read_csv('cb102.csv')['item_cnt_month']\n",
    "\n",
    "\n",
    "#preds = np.mean(np.array([lr_preds, lg_preds]),axis=0)\n",
    "\n",
    "preds = (cb_preds * 0.7) + (lr_preds * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestRegressor(n_estimators=10, random_state=0, n_jobs=8)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(x_train[features], y_train)\n",
    "\n",
    "# Print the name and gini importance of each feature\n",
    "for feature in zip(features, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
