{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,Conv1D,TimeDistributed,MaxPooling1D,Flatten,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tail -n +2 train.csv | split -l 150000\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "TRAIN_SPLITS='train'\n",
    "splits = [f for f in listdir(TRAIN_SPLITS) if isfile(join(TRAIN_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "columns = ['acoustic_data','time_to_failure']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(splits, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS=150000\n",
    "BATCH_SIZE=32\n",
    "NUMBER_OF_BATCHES = int(np.ceil(len(train_data)/BATCH_SIZE))\n",
    "NUMBER_OF_VALIDATION_STEPS = int(np.ceil(len(val_data) / BATCH_SIZE))\n",
    "NUMBER_OF_FEATURES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_batch = np.array_split(train_data, NUMBER_OF_BATCHES)\n",
    "val_data_batch = np.array_split(val_data, NUMBER_OF_VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE=0.75\n",
    "\n",
    "def add_noise(dff, pct=NOISE):\n",
    "    mu = dff['acoustic_data'].mean()\n",
    "    sigma = dff['acoustic_data'].std()\n",
    "\n",
    "    indices = np.random.choice(dff.index.values, int(len(dff)*pct))\n",
    "    dff.loc[indices, 'acoustic_data'] = np.random.normal(mu, sigma, len(indices)) \n",
    "    return dff\n",
    "\n",
    "\n",
    "def get_batch(list_of_files, valid=False):\n",
    "#     batch = np.empty((len(list_of_files),TIMESTEPS,1),dtype=float)\n",
    "#     target = np.empty((len(list_of_files),1),dtype=float)\n",
    "    #print(list_of_files)\n",
    "    batch = []\n",
    "    target = []\n",
    "\n",
    "    for idx, file in enumerate(list_of_files):\n",
    "        #print(idx,file)\n",
    "        path = f'train/{file}'\n",
    "        df = pd.read_csv(path, float_precision='round_trip', header=None)\n",
    "        df.columns = columns\n",
    "        df[['acoustic_data']] = StandardScaler().fit_transform(df[['acoustic_data']].astype('float'))\n",
    "        df['roll_1000'] = df['acoustic_data'].rolling(1000,min_periods=1000).mean()\n",
    "        df['roll_1000'].fillna(df['roll_1000'][1000],inplace=True)\n",
    "        df['shifted_1'] = df['roll_1000'].shift(1)\n",
    "        df['shifted_1'].fillna(df['roll_1000'][1000],inplace=True)\n",
    "        df[\"roll_diff_1\"] = df['shifted_1'] - df['roll_1000']\n",
    "        #df['shifted_1000'] = df['roll_1000'].shift(1000)\n",
    "        #df['shifted_1000'].fillna(df['roll_1000'][1000],inplace=True)\n",
    "        #df[\"roll_diff_1000\"] = df['shifted_1000'] - df['roll_1000']\n",
    "        #print(df.head())\n",
    "        #print(len(batch))\n",
    "        batch.append(df[['acoustic_data', 'roll_1000', 'roll_diff_1']].values)\n",
    "        target.append(df['time_to_failure'].values[-1])\n",
    "        #print(df_noise.head())\n",
    "        #if not valid:\n",
    "            #df_noise = add_noise(df)\n",
    "            #batch.append(df_noise['acoustic_data'].values)#.reshape(-1,TIMESTEPS,1))\n",
    "            #target.append(df_noise['time_to_failure'].values[-1])#.reshape(-1,1))\n",
    "        #print(np.array(batch).reshape(-1,TIMESTEPS,1).shape)\n",
    "        #batch = np.array(batch).reshape(-1,TIMESTEPS,1)\n",
    "        #target = np.array(target).reshape(-1,1)\n",
    "    return (batch, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class MY_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, list_of_files, steps,name):\n",
    "        self.list_of_files = list_of_files\n",
    "        self.steps = steps\n",
    "        self.name = name\n",
    "\n",
    "    #This function computes the number of batches that this generator is supposed to produce. \n",
    "    #So, we divide the number of total samples by the batch_size and return that value.    \n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    #Here, given the batch numberidx you need to put together a list that consists of data \n",
    "    #batch and the ground-truth (GT). In this example, we read a batch images of size \n",
    "    #self.batch and return an array of form[image_batch, GT]\n",
    "    def __getitem__(self, idx):\n",
    "#         if self.name == 'val':\n",
    "        #print('idx', idx)\n",
    "        #print(\"DEBUG\", self.list_of_files[idx])\n",
    "        #if idx == len(self.list_of_files):\n",
    "            #print(idx, self.list_of_files)\n",
    "        if self.name == 'val':\n",
    "            valid = True\n",
    "        else:\n",
    "            valid=False\n",
    "        train,Y = get_batch(self.list_of_files[idx], valid)\n",
    "            \n",
    "        #print(np.array(train).reshape(-1,TIMESTEPS,1).shape)\n",
    "        #print(\"idx\",idx)\n",
    "        #print(\"LOLILOL\")\n",
    "        #print(train.shape, Y.shape)\n",
    "        train = np.array(train).reshape(-1,TIMESTEPS,NUMBER_OF_FEATURES)\n",
    "        #print(train.shape)\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "        return (train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 30000, 3)          48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15000, 3)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15000, 3)          12        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3000, 3)           48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1500, 3)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1500, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 300, 3)            48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 150, 3)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 150, 3)            12        \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 4)                 96        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 377\n",
      "Trainable params: 339\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "118/118 [==============================] - 68s 573ms/step - loss: 6.1742 - mean_absolute_error: 5.4452 - val_loss: 5.6505 - val_mean_absolute_error: 5.1338\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 5.13376, saving model to weights.01-5.65.hdf5\n",
      "Epoch 2/1000\n",
      "118/118 [==============================] - 58s 492ms/step - loss: 5.1910 - mean_absolute_error: 4.8052 - val_loss: 4.7315 - val_mean_absolute_error: 4.4453\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 5.13376 to 4.44526, saving model to weights.02-4.73.hdf5\n",
      "Epoch 3/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 4.2375 - mean_absolute_error: 4.0086 - val_loss: 4.3272 - val_mean_absolute_error: 4.1425\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 4.44526 to 4.14254, saving model to weights.03-4.33.hdf5\n",
      "Epoch 4/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 3.3539 - mean_absolute_error: 3.1939 - val_loss: 4.1004 - val_mean_absolute_error: 3.9610\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 4.14254 to 3.96095, saving model to weights.04-4.10.hdf5\n",
      "Epoch 5/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.7482 - mean_absolute_error: 2.6223 - val_loss: 3.0398 - val_mean_absolute_error: 2.9256\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 3.96095 to 2.92560, saving model to weights.05-3.04.hdf5\n",
      "Epoch 6/1000\n",
      "118/118 [==============================] - 59s 500ms/step - loss: 2.5783 - mean_absolute_error: 2.4735 - val_loss: 2.8026 - val_mean_absolute_error: 2.7065\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 2.92560 to 2.70651, saving model to weights.06-2.80.hdf5\n",
      "Epoch 7/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.5313 - mean_absolute_error: 2.4427 - val_loss: 2.5608 - val_mean_absolute_error: 2.4792\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 2.70651 to 2.47921, saving model to weights.07-2.56.hdf5\n",
      "Epoch 8/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.4986 - mean_absolute_error: 2.4222 - val_loss: 2.5123 - val_mean_absolute_error: 2.4408\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 2.47921 to 2.44076, saving model to weights.08-2.51.hdf5\n",
      "Epoch 9/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.4693 - mean_absolute_error: 2.4026 - val_loss: 2.6123 - val_mean_absolute_error: 2.5502\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 2.44076\n",
      "Epoch 10/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.4641 - mean_absolute_error: 2.4057 - val_loss: 2.4839 - val_mean_absolute_error: 2.4288\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error improved from 2.44076 to 2.42881, saving model to weights.10-2.48.hdf5\n",
      "Epoch 11/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.4302 - mean_absolute_error: 2.3780 - val_loss: 2.4709 - val_mean_absolute_error: 2.4209\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error improved from 2.44076 to 2.42881, saving model to weights.10-2.48.hdf5\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error improved from 2.42881 to 2.42093, saving model to weights.11-2.47.hdf5\n",
      "Epoch 12/1000\n",
      "118/118 [==============================] - 61s 514ms/step - loss: 2.4325 - mean_absolute_error: 2.3851 - val_loss: 2.4921 - val_mean_absolute_error: 2.4466\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 2.42093\n",
      "Epoch 13/1000\n",
      "118/118 [==============================] - 61s 518ms/step - loss: 2.4308 - mean_absolute_error: 2.3870 - val_loss: 2.4576 - val_mean_absolute_error: 2.4156\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error improved from 2.42093 to 2.41560, saving model to weights.13-2.46.hdf5\n",
      "Epoch 14/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.4247 - mean_absolute_error: 2.3845 - val_loss: 2.4320 - val_mean_absolute_error: 2.3937\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error improved from 2.41560 to 2.39367, saving model to weights.14-2.43.hdf5\n",
      "Epoch 15/1000\n",
      "118/118 [==============================] - 61s 520ms/step - loss: 2.3951 - mean_absolute_error: 2.3584 - val_loss: 2.4757 - val_mean_absolute_error: 2.4404\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 2.39367\n",
      "Epoch 16/1000\n",
      "118/118 [==============================] - 61s 513ms/step - loss: 2.4052 - mean_absolute_error: 2.3711 - val_loss: 2.6126 - val_mean_absolute_error: 2.5795\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 2.39367\n",
      "Epoch 17/1000\n",
      "118/118 [==============================] - 61s 517ms/step - loss: 2.4044 - mean_absolute_error: 2.3716 - val_loss: 2.6269 - val_mean_absolute_error: 2.5947\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 2.39367\n",
      "Epoch 18/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.3862 - mean_absolute_error: 2.3550 - val_loss: 2.5367 - val_mean_absolute_error: 2.5061\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 2.39367\n",
      "Epoch 19/1000\n",
      "118/118 [==============================] - 63s 531ms/step - loss: 2.3813 - mean_absolute_error: 2.3508 - val_loss: 2.4128 - val_mean_absolute_error: 2.3823\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error improved from 2.39367 to 2.38232, saving model to weights.19-2.41.hdf5\n",
      "Epoch 20/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 60s 512ms/step - loss: 2.3839 - mean_absolute_error: 2.3539 - val_loss: 2.4635 - val_mean_absolute_error: 2.4340\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 2.38232\n",
      "Epoch 21/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.3594 - mean_absolute_error: 2.3305 - val_loss: 2.3986 - val_mean_absolute_error: 2.3702\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error improved from 2.38232 to 2.37020, saving model to weights.21-2.40.hdf5\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.3697 - mean_absolute_error: 2.3415 - val_loss: 2.5095 - val_mean_absolute_error: 2.4819\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 23/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.3784 - mean_absolute_error: 2.3508 - val_loss: 2.5984 - val_mean_absolute_error: 2.5710\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 24/1000\n",
      "118/118 [==============================] - 61s 515ms/step - loss: 2.3668 - mean_absolute_error: 2.3401 - val_loss: 2.4062 - val_mean_absolute_error: 2.3803\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 25/1000\n",
      "118/118 [==============================] - 61s 517ms/step - loss: 2.3398 - mean_absolute_error: 2.3137 - val_loss: 2.3967 - val_mean_absolute_error: 2.3709\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 26/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.3568 - mean_absolute_error: 2.3310 - val_loss: 2.4202 - val_mean_absolute_error: 2.3947\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 27/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.3472 - mean_absolute_error: 2.3219 - val_loss: 2.5442 - val_mean_absolute_error: 2.5187\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 28/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.3424 - mean_absolute_error: 2.3170 - val_loss: 2.4271 - val_mean_absolute_error: 2.4018\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 29/1000\n",
      "118/118 [==============================] - 61s 516ms/step - loss: 2.3566 - mean_absolute_error: 2.3316 - val_loss: 2.4259 - val_mean_absolute_error: 2.4005\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 2.37020\n",
      "Epoch 30/1000\n",
      "118/118 [==============================] - 62s 525ms/step - loss: 2.3422 - mean_absolute_error: 2.3169 - val_loss: 2.3941 - val_mean_absolute_error: 2.3689\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error improved from 2.37020 to 2.36891, saving model to weights.30-2.39.hdf5\n",
      "Epoch 31/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.3355 - mean_absolute_error: 2.3105 - val_loss: 2.4102 - val_mean_absolute_error: 2.3853\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 2.36891\n",
      "Epoch 32/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.3159 - mean_absolute_error: 2.2915 - val_loss: 2.4198 - val_mean_absolute_error: 2.3957\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 2.36891\n",
      "Epoch 33/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.3303 - mean_absolute_error: 2.3061 - val_loss: 2.4123 - val_mean_absolute_error: 2.3882\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 2.36891\n",
      "Epoch 34/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.3331 - mean_absolute_error: 2.3091 - val_loss: 2.3856 - val_mean_absolute_error: 2.3615\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error improved from 2.36891 to 2.36154, saving model to weights.34-2.39.hdf5\n",
      "Epoch 35/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.3233 - mean_absolute_error: 2.2989 - val_loss: 2.4893 - val_mean_absolute_error: 2.4650\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 36/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.3167 - mean_absolute_error: 2.2921 - val_loss: 2.4637 - val_mean_absolute_error: 2.4391\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 36/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.3319 - mean_absolute_error: 2.3076 - val_loss: 2.4513 - val_mean_absolute_error: 2.4268\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 38/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.3210 - mean_absolute_error: 2.2967 - val_loss: 2.7546 - val_mean_absolute_error: 2.7308\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 39/1000\n",
      "118/118 [==============================] - 61s 514ms/step - loss: 2.3116 - mean_absolute_error: 2.2879 - val_loss: 2.7455 - val_mean_absolute_error: 2.7212\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 40/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.3124 - mean_absolute_error: 2.2881 - val_loss: 2.4094 - val_mean_absolute_error: 2.3851\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 41/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.3039 - mean_absolute_error: 2.2798 - val_loss: 2.5692 - val_mean_absolute_error: 2.5450\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 42/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.3114 - mean_absolute_error: 2.2872 - val_loss: 2.6707 - val_mean_absolute_error: 2.6468\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 43/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.3075 - mean_absolute_error: 2.2837 - val_loss: 2.4639 - val_mean_absolute_error: 2.4399\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 44/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.3010 - mean_absolute_error: 2.2770 - val_loss: 2.5058 - val_mean_absolute_error: 2.4817\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 45/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.3100 - mean_absolute_error: 2.2857 - val_loss: 2.4059 - val_mean_absolute_error: 2.3816\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 2.36154\n",
      "Epoch 46/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.3036 - mean_absolute_error: 2.2794 - val_loss: 2.3382 - val_mean_absolute_error: 2.3141\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error improved from 2.36154 to 2.31412, saving model to weights.46-2.34.hdf5\n",
      "Epoch 47/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.3026 - mean_absolute_error: 2.2787 - val_loss: 2.3320 - val_mean_absolute_error: 2.3084\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error improved from 2.31412 to 2.30840, saving model to weights.47-2.33.hdf5\n",
      "Epoch 48/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.2876 - mean_absolute_error: 2.2639 - val_loss: 2.3031 - val_mean_absolute_error: 2.2793\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error improved from 2.30840 to 2.27934, saving model to weights.48-2.30.hdf5\n",
      "Epoch 49/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.2833 - mean_absolute_error: 2.2597 - val_loss: 2.3164 - val_mean_absolute_error: 2.2928\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 50/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.2903 - mean_absolute_error: 2.2665 - val_loss: 2.3343 - val_mean_absolute_error: 2.3106\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 51/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.2868 - mean_absolute_error: 2.2630 - val_loss: 2.6742 - val_mean_absolute_error: 2.6505\n",
      "\n",
      "Epoch 00051: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 52/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.2922 - mean_absolute_error: 2.2687 - val_loss: 2.3209 - val_mean_absolute_error: 2.2974\n",
      "\n",
      "Epoch 00052: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 60s 505ms/step - loss: 2.2835 - mean_absolute_error: 2.2599 - val_loss: 2.3295 - val_mean_absolute_error: 2.3054\n",
      "\n",
      "Epoch 00053: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 54/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.2850 - mean_absolute_error: 2.2610 - val_loss: 2.3821 - val_mean_absolute_error: 2.3585\n",
      "\n",
      "Epoch 00054: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 55/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.2862 - mean_absolute_error: 2.2624 - val_loss: 2.4179 - val_mean_absolute_error: 2.3939\n",
      "\n",
      "Epoch 00055: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 56/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.2894 - mean_absolute_error: 2.2650 - val_loss: 2.5219 - val_mean_absolute_error: 2.4977\n",
      "\n",
      "Epoch 00056: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 57/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.2762 - mean_absolute_error: 2.2521 - val_loss: 2.3627 - val_mean_absolute_error: 2.3386\n",
      "\n",
      "Epoch 00057: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 58/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.2728 - mean_absolute_error: 2.2486 - val_loss: 2.3126 - val_mean_absolute_error: 2.2885\n",
      "\n",
      "Epoch 00058: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 59/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.2649 - mean_absolute_error: 2.2407 - val_loss: 2.3056 - val_mean_absolute_error: 2.2816\n",
      "\n",
      "Epoch 00059: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 60/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.2737 - mean_absolute_error: 2.2496 - val_loss: 2.3080 - val_mean_absolute_error: 2.2840\n",
      "\n",
      "Epoch 00060: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 61/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.2503 - mean_absolute_error: 2.2265 - val_loss: 2.4057 - val_mean_absolute_error: 2.3823\n",
      "\n",
      "Epoch 00061: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 62/1000\n",
      "118/118 [==============================] - 59s 498ms/step - loss: 2.2743 - mean_absolute_error: 2.2504 - val_loss: 2.3224 - val_mean_absolute_error: 2.2982\n",
      "\n",
      "Epoch 00062: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 63/1000\n",
      "118/118 [==============================] - 61s 513ms/step - loss: 2.2495 - mean_absolute_error: 2.2252 - val_loss: 2.3612 - val_mean_absolute_error: 2.3371\n",
      "\n",
      "Epoch 00063: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 64/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.2392 - mean_absolute_error: 2.2152 - val_loss: 2.4710 - val_mean_absolute_error: 2.4469\n",
      "\n",
      "Epoch 00064: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 65/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.2477 - mean_absolute_error: 2.2238 - val_loss: 2.4106 - val_mean_absolute_error: 2.3865\n",
      "\n",
      "Epoch 00065: val_mean_absolute_error did not improve from 2.27934\n",
      "Epoch 66/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.2559 - mean_absolute_error: 2.2318 - val_loss: 2.2945 - val_mean_absolute_error: 2.2704\n",
      "\n",
      "Epoch 00066: val_mean_absolute_error improved from 2.27934 to 2.27044, saving model to weights.66-2.29.hdf5\n",
      "Epoch 67/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.2339 - mean_absolute_error: 2.2099 - val_loss: 2.9511 - val_mean_absolute_error: 2.9273\n",
      "\n",
      "Epoch 00067: val_mean_absolute_error did not improve from 2.27044\n",
      "Epoch 68/1000\n",
      "118/118 [==============================] - 59s 499ms/step - loss: 2.2216 - mean_absolute_error: 2.1979 - val_loss: 2.2525 - val_mean_absolute_error: 2.2287\n",
      "\n",
      "Epoch 00068: val_mean_absolute_error improved from 2.27044 to 2.22869, saving model to weights.68-2.25.hdf5\n",
      "Epoch 69/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.2537 - mean_absolute_error: 2.2298\n",
      "Epoch 00068: val_mean_absolute_error improved from 2.27044 to 2.22869, saving model to weights.68-2.25.hdf5\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.2557 - mean_absolute_error: 2.2317 - val_loss: 2.2586 - val_mean_absolute_error: 2.2346\n",
      "\n",
      "Epoch 00069: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 70/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.2272 - mean_absolute_error: 2.2033 - val_loss: 2.2998 - val_mean_absolute_error: 2.2762\n",
      "\n",
      "Epoch 00070: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 71/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.2201 - mean_absolute_error: 2.1967 - val_loss: 2.5371 - val_mean_absolute_error: 2.5137\n",
      "\n",
      "Epoch 00071: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 72/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.2007 - mean_absolute_error: 2.1776 - val_loss: 2.2625 - val_mean_absolute_error: 2.2393\n",
      "\n",
      "Epoch 00072: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 73/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1879 - mean_absolute_error: 2.1647 - val_loss: 2.4075 - val_mean_absolute_error: 2.3842\n",
      "\n",
      "Epoch 00073: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 74/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1858 - mean_absolute_error: 2.1625 - val_loss: 2.5182 - val_mean_absolute_error: 2.4951\n",
      "\n",
      "Epoch 00074: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 75/1000\n",
      "118/118 [==============================] - 59s 499ms/step - loss: 2.2007 - mean_absolute_error: 2.1774 - val_loss: 2.4271 - val_mean_absolute_error: 2.4036\n",
      "\n",
      "Epoch 00075: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 76/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.2003 - mean_absolute_error: 2.1770\n",
      "Epoch 00075: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 76/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1978 - mean_absolute_error: 2.1746 - val_loss: 2.3167 - val_mean_absolute_error: 2.2940\n",
      "\n",
      "Epoch 00076: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 77/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1918 - mean_absolute_error: 2.169\n",
      "Epoch 00076: val_mean_absolute_error did not improve from 2.22869\n",
      "Epoch 77/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.1910 - mean_absolute_error: 2.1682 - val_loss: 2.2061 - val_mean_absolute_error: 2.1832\n",
      "\n",
      "Epoch 00077: val_mean_absolute_error improved from 2.22869 to 2.18316, saving model to weights.77-2.21.hdf5\n",
      "Epoch 78/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1877 - mean_absolute_error: 2.1648 - val_loss: 2.2691 - val_mean_absolute_error: 2.2460\n",
      "\n",
      "Epoch 00078: val_mean_absolute_error did not improve from 2.18316\n",
      "Epoch 79/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1923 - mean_absolute_error: 2.1695 - val_loss: 2.2991 - val_mean_absolute_error: 2.2766\n",
      "\n",
      "Epoch 00079: val_mean_absolute_error did not improve from 2.18316\n",
      "Epoch 80/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.1984 - mean_absolute_error: 2.1761 - val_loss: 2.4734 - val_mean_absolute_error: 2.4512\n",
      "\n",
      "Epoch 00080: val_mean_absolute_error did not improve from 2.18316\n",
      "Epoch 81/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1693 - mean_absolute_error: 2.1473 - val_loss: 2.2416 - val_mean_absolute_error: 2.2196\n",
      "\n",
      "Epoch 00081: val_mean_absolute_error did not improve from 2.18316\n",
      "Epoch 82/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1716 - mean_absolute_error: 2.1497 - val_loss: 2.2775 - val_mean_absolute_error: 2.2557\n",
      "\n",
      "Epoch 00082: val_mean_absolute_error did not improve from 2.18316\n",
      "Epoch 83/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1729 - mean_absolute_error: 2.1512 - val_loss: 2.1928 - val_mean_absolute_error: 2.1711\n",
      "\n",
      "Epoch 00083: val_mean_absolute_error improved from 2.18316 to 2.17112, saving model to weights.83-2.19.hdf5\n",
      "Epoch 84/1000\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 2.1749 - mean_absolute_error: 2.1533 - val_loss: 2.8620 - val_mean_absolute_error: 2.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 85/1000\n",
      "118/118 [==============================] - 59s 498ms/step - loss: 2.1742 - mean_absolute_error: 2.1530 - val_loss: 2.8279 - val_mean_absolute_error: 2.8068\n",
      "\n",
      "Epoch 00085: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 86/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1714 - mean_absolute_error: 2.1506\n",
      "Epoch 86/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1708 - mean_absolute_error: 2.1499 - val_loss: 2.4430 - val_mean_absolute_error: 2.4219\n",
      "\n",
      "Epoch 00086: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 87/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1693 - mean_absolute_error: 2.1483 - val_loss: 2.2416 - val_mean_absolute_error: 2.2206\n",
      "\n",
      "Epoch 00087: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 88/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1682 - mean_absolute_error: 2.1473 - val_loss: 2.7170 - val_mean_absolute_error: 2.6963\n",
      "\n",
      "Epoch 00088: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 89/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.1688 - mean_absolute_error: 2.1482 - val_loss: 2.6871 - val_mean_absolute_error: 2.6663\n",
      "\n",
      "Epoch 00089: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 90/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1666 - mean_absolute_error: 2.1459 - val_loss: 2.6555 - val_mean_absolute_error: 2.6350\n",
      "\n",
      "Epoch 00090: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 91/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.1552 - mean_absolute_error: 2.1348 - val_loss: 2.2050 - val_mean_absolute_error: 2.1845\n",
      "\n",
      "Epoch 00091: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 92/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1528 - mean_absolute_error: 2.1322 - val_loss: 2.2821 - val_mean_absolute_error: 2.2616\n",
      "\n",
      "Epoch 00092: val_mean_absolute_error did not improve from 2.17112\n",
      "Epoch 93/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1705 - mean_absolute_error: 2.1501 - val_loss: 2.1649 - val_mean_absolute_error: 2.1446\n",
      "\n",
      "Epoch 00093: val_mean_absolute_error improved from 2.17112 to 2.14460, saving model to weights.93-2.16.hdf5\n",
      "Epoch 94/1000\n",
      "118/118 [==============================] - 59s 499ms/step - loss: 2.1627 - mean_absolute_error: 2.1425 - val_loss: 2.2158 - val_mean_absolute_error: 2.1956\n",
      "\n",
      "Epoch 00094: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 95/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1814 - mean_absolute_error: 2.1611 - val_loss: 2.6193 - val_mean_absolute_error: 2.5991\n",
      "\n",
      "Epoch 00095: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 96/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.1488 - mean_absolute_error: 2.1286 - val_loss: 2.1922 - val_mean_absolute_error: 2.1722\n",
      "\n",
      "Epoch 00096: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 97/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.1677 - mean_absolute_error: 2.1480 - val_loss: 2.3483 - val_mean_absolute_error: 2.3285\n",
      "\n",
      "Epoch 00097: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 98/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1699 - mean_absolute_error: 2.1501 - val_loss: 2.3205 - val_mean_absolute_error: 2.3009\n",
      "\n",
      "Epoch 00098: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 99/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1583 - mean_absolute_error: 2.1387 - val_loss: 2.3256 - val_mean_absolute_error: 2.3059\n",
      "\n",
      "Epoch 00099: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 100/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1454 - mean_absolute_error: 2.1259 - val_loss: 2.4947 - val_mean_absolute_error: 2.4751\n",
      "\n",
      "Epoch 00100: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 101/1000\n",
      "118/118 [==============================] - 59s 500ms/step - loss: 2.1519 - mean_absolute_error: 2.1322 - val_loss: 2.6680 - val_mean_absolute_error: 2.6478\n",
      "\n",
      "Epoch 00101: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 102/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1679 - mean_absolute_error: 2.1480 - val_loss: 2.6868 - val_mean_absolute_error: 2.6668\n",
      "\n",
      "Epoch 00102: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 103/1000\n",
      "118/118 [==============================] - 61s 517ms/step - loss: 2.1548 - mean_absolute_error: 2.1348 - val_loss: 2.5336 - val_mean_absolute_error: 2.5132\n",
      "\n",
      "Epoch 00103: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 104/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.1604 - mean_absolute_error: 2.1404 - val_loss: 2.2116 - val_mean_absolute_error: 2.1921\n",
      "\n",
      "Epoch 00104: val_mean_absolute_error did not improve from 2.14460\n",
      "Epoch 105/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1399 - mean_absolute_error: 2.1204 - val_loss: 2.1628 - val_mean_absolute_error: 2.1430\n",
      "\n",
      "Epoch 00105: val_mean_absolute_error improved from 2.14460 to 2.14300, saving model to weights.105-2.16.hdf5\n",
      "Epoch 106/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1766 - mean_absolute_error: 2.1568 - val_loss: 2.3658 - val_mean_absolute_error: 2.3463\n",
      "\n",
      "Epoch 00106: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 107/1000\n",
      "118/118 [==============================] - 61s 520ms/step - loss: 2.1578 - mean_absolute_error: 2.1383 - val_loss: 2.5368 - val_mean_absolute_error: 2.5175\n",
      "\n",
      "Epoch 00107: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 108/1000\n",
      "118/118 [==============================] - 62s 530ms/step - loss: 2.1670 - mean_absolute_error: 2.1478 - val_loss: 2.9002 - val_mean_absolute_error: 2.8811\n",
      "\n",
      "Epoch 00108: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 109/1000\n",
      "118/118 [==============================] - 62s 526ms/step - loss: 2.1587 - mean_absolute_error: 2.1394 - val_loss: 2.4655 - val_mean_absolute_error: 2.4460\n",
      "\n",
      "Epoch 00109: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 110/1000\n",
      "118/118 [==============================] - 65s 548ms/step - loss: 2.1405 - mean_absolute_error: 2.1208 - val_loss: 2.3735 - val_mean_absolute_error: 2.3538\n",
      "\n",
      "Epoch 00110: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 111/1000\n",
      "118/118 [==============================] - 63s 537ms/step - loss: 2.1639 - mean_absolute_error: 2.1442 - val_loss: 2.5729 - val_mean_absolute_error: 2.5530\n",
      "\n",
      "Epoch 00111: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 112/1000\n",
      "118/118 [==============================] - 61s 515ms/step - loss: 2.1510 - mean_absolute_error: 2.1315 - val_loss: 2.4964 - val_mean_absolute_error: 2.4770\n",
      "\n",
      "Epoch 00112: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 113/1000\n",
      "118/118 [==============================] - 62s 524ms/step - loss: 2.1552 - mean_absolute_error: 2.1359 - val_loss: 2.1646 - val_mean_absolute_error: 2.1453\n",
      "\n",
      "Epoch 00113: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 114/1000\n",
      "118/118 [==============================] - 61s 520ms/step - loss: 2.1495 - mean_absolute_error: 2.1302 - val_loss: 2.3775 - val_mean_absolute_error: 2.3582\n",
      "\n",
      "Epoch 00114: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 115/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1550 - mean_absolute_error: 2.136\n",
      "Epoch 00114: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 115/1000\n",
      "118/118 [==============================] - 61s 514ms/step - loss: 2.1536 - mean_absolute_error: 2.1347 - val_loss: 2.5403 - val_mean_absolute_error: 2.5214\n",
      "\n",
      "Epoch 00115: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 116/1000\n",
      "118/118 [==============================] - 61s 520ms/step - loss: 2.1668 - mean_absolute_error: 2.1477 - val_loss: 2.9235 - val_mean_absolute_error: 2.9042\n",
      "\n",
      "Epoch 00116: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 117/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.1542 - mean_absolute_error: 2.1351 - val_loss: 3.2305 - val_mean_absolute_error: 3.2113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00117: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 118/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1490 - mean_absolute_error: 2.1298 - val_loss: 2.2188 - val_mean_absolute_error: 2.1999\n",
      "\n",
      "Epoch 00118: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 119/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1486 - mean_absolute_error: 2.1297 - val_loss: 2.7943 - val_mean_absolute_error: 2.7753\n",
      "\n",
      "Epoch 00119: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 120/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1671 - mean_absolute_error: 2.1479 - val_loss: 2.6773 - val_mean_absolute_error: 2.6582\n",
      "\n",
      "Epoch 00120: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 121/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1653 - mean_absolute_error: 2.1460 - val_loss: 2.1815 - val_mean_absolute_error: 2.1619\n",
      "\n",
      "Epoch 00121: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 122/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.1576 - mean_absolute_error: 2.1379 - val_loss: 2.2276 - val_mean_absolute_error: 2.2078\n",
      "\n",
      "Epoch 00122: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 123/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1490 - mean_absolute_error: 2.1294 - val_loss: 2.8864 - val_mean_absolute_error: 2.8668\n",
      "\n",
      "Epoch 00123: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 124/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1391 - mean_absolute_error: 2.1196 - val_loss: 2.5992 - val_mean_absolute_error: 2.5795\n",
      "\n",
      "Epoch 00124: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 125/1000\n",
      "\n",
      "Epoch 00123: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 124/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1528 - mean_absolute_error: 2.1330 - val_loss: 2.2539 - val_mean_absolute_error: 2.2342\n",
      "\n",
      "Epoch 00125: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 126/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1569 - mean_absolute_error: 2.1374 - val_loss: 2.2126 - val_mean_absolute_error: 2.1931\n",
      "\n",
      "Epoch 00126: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 127/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1436 - mean_absolute_error: 2.1238 - val_loss: 2.6045 - val_mean_absolute_error: 2.5844\n",
      "\n",
      "Epoch 00127: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 128/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1717 - mean_absolute_error: 2.1517 - val_loss: 2.2312 - val_mean_absolute_error: 2.2116\n",
      "\n",
      "Epoch 00128: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 129/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1501 - mean_absolute_error: 2.1304 - val_loss: 2.2338 - val_mean_absolute_error: 2.2142\n",
      "\n",
      "Epoch 00129: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 130/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1512 - mean_absolute_error: 2.1315 - val_loss: 2.1626 - val_mean_absolute_error: 2.1431\n",
      "\n",
      "Epoch 00130: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 131/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1402 - mean_absolute_error: 2.1207\n",
      "Epoch 131/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1442 - mean_absolute_error: 2.1247 - val_loss: 2.2055 - val_mean_absolute_error: 2.1862\n",
      "\n",
      "Epoch 00131: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 132/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.1585 - mean_absolute_error: 2.1391 - val_loss: 2.5895 - val_mean_absolute_error: 2.5700\n",
      "\n",
      "Epoch 00132: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 133/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1421 - mean_absolute_error: 2.1224\n",
      "Epoch 133/1000\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 2.1405 - mean_absolute_error: 2.1208 - val_loss: 3.2474 - val_mean_absolute_error: 3.2273\n",
      "\n",
      "Epoch 00133: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 134/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1399 - mean_absolute_error: 2.1202 - val_loss: 2.1931 - val_mean_absolute_error: 2.1731\n",
      "\n",
      "Epoch 00134: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 135/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1622 - mean_absolute_error: 2.1426 - val_loss: 2.3259 - val_mean_absolute_error: 2.3063\n",
      "\n",
      "Epoch 00135: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 136/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1578 - mean_absolute_error: 2.1382 - val_loss: 2.3457 - val_mean_absolute_error: 2.3264\n",
      "\n",
      "Epoch 00136: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 137/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1441 - mean_absolute_error: 2.1247 - val_loss: 2.3047 - val_mean_absolute_error: 2.2850\n",
      "\n",
      "Epoch 00137: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 138/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1429 - mean_absolute_error: 2.1233 - val_loss: 2.2879 - val_mean_absolute_error: 2.2684\n",
      "\n",
      "Epoch 00138: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 139/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.1386 - mean_absolute_error: 2.1189 - val_loss: 2.3262 - val_mean_absolute_error: 2.3067\n",
      "\n",
      "Epoch 00139: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 140/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1428 - mean_absolute_error: 2.1232 - val_loss: 2.2164 - val_mean_absolute_error: 2.1969\n",
      "\n",
      "Epoch 00140: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 141/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1439 - mean_absolute_error: 2.1246 - val_loss: 2.3796 - val_mean_absolute_error: 2.3608\n",
      "\n",
      "Epoch 00141: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 142/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1469 - mean_absolute_error: 2.1277 - val_loss: 2.2139 - val_mean_absolute_error: 2.1946\n",
      "\n",
      "Epoch 00142: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 143/1000\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 2.1448 - mean_absolute_error: 2.1255 - val_loss: 3.0273 - val_mean_absolute_error: 3.0080\n",
      "\n",
      "Epoch 00143: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 144/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1366 - mean_absolute_error: 2.1172 - val_loss: 2.6015 - val_mean_absolute_error: 2.5824\n",
      "\n",
      "Epoch 00144: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 145/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.1500 - mean_absolute_error: 2.1307 - val_loss: 2.2190 - val_mean_absolute_error: 2.1999\n",
      "\n",
      "Epoch 00145: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 146/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.1437 - mean_absolute_error: 2.1244 - val_loss: 3.1868 - val_mean_absolute_error: 3.1677\n",
      "\n",
      "Epoch 00146: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 147/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1466 - mean_absolute_error: 2.1274 - val_loss: 2.5594 - val_mean_absolute_error: 2.5403\n",
      "\n",
      "Epoch 00147: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 148/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1312 - mean_absolute_error: 2.1122 - val_loss: 2.3456 - val_mean_absolute_error: 2.3265\n",
      "\n",
      "Epoch 00148: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 149/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1460 - mean_absolute_error: 2.1270 - val_loss: 2.4185 - val_mean_absolute_error: 2.3995\n",
      "\n",
      "Epoch 00149: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 150/1000\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 2.1284 - mean_absolute_error: 2.1094 - val_loss: 2.2022 - val_mean_absolute_error: 2.1831\n",
      "\n",
      "Epoch 00150: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/118 [============================>.] - ETA: 0s - loss: 2.1410 - mean_absolute_error: 2.1214\n",
      "Epoch 00150: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 151/1000\n",
      "118/118 [==============================] - 61s 515ms/step - loss: 2.1460 - mean_absolute_error: 2.1264 - val_loss: 3.1544 - val_mean_absolute_error: 3.1347\n",
      "\n",
      "Epoch 00151: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 152/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1355 - mean_absolute_error: 2.1160 - val_loss: 2.2422 - val_mean_absolute_error: 2.2227\n",
      "\n",
      "Epoch 00152: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 153/1000\n",
      "118/118 [==============================] - 61s 514ms/step - loss: 2.1360 - mean_absolute_error: 2.1165 - val_loss: 2.3338 - val_mean_absolute_error: 2.3144\n",
      "\n",
      "Epoch 00153: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 154/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1397 - mean_absolute_error: 2.1205 - val_loss: 2.8064 - val_mean_absolute_error: 2.7871\n",
      "\n",
      "Epoch 00154: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 155/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.1278 - mean_absolute_error: 2.1085 - val_loss: 2.1790 - val_mean_absolute_error: 2.1597\n",
      "\n",
      "Epoch 00155: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 156/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1349 - mean_absolute_error: 2.1156 - val_loss: 2.1829 - val_mean_absolute_error: 2.1638\n",
      "\n",
      "Epoch 00156: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 157/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1424 - mean_absolute_error: 2.1230 - val_loss: 2.1954 - val_mean_absolute_error: 2.1760\n",
      "\n",
      "Epoch 00157: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 158/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1337 - mean_absolute_error: 2.1146 - val_loss: 2.1665 - val_mean_absolute_error: 2.1477\n",
      "\n",
      "Epoch 00158: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 159/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1444 - mean_absolute_error: 2.1255 - val_loss: 2.5820 - val_mean_absolute_error: 2.5633\n",
      "\n",
      "Epoch 00159: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 160/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1403 - mean_absolute_error: 2.1215 - val_loss: 2.2059 - val_mean_absolute_error: 2.1870\n",
      "\n",
      "Epoch 00160: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 161/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1408 - mean_absolute_error: 2.1218\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1445 - mean_absolute_error: 2.1254 - val_loss: 2.4728 - val_mean_absolute_error: 2.4536\n",
      "\n",
      "Epoch 00161: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 162/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1500 - mean_absolute_error: 2.1310 - val_loss: 2.2504 - val_mean_absolute_error: 2.2313\n",
      "\n",
      "Epoch 00162: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 163/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1303 - mean_absolute_error: 2.1113 - val_loss: 2.2461 - val_mean_absolute_error: 2.2272\n",
      "\n",
      "Epoch 00163: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 164/1000\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 2.1335 - mean_absolute_error: 2.1149 - val_loss: 2.8660 - val_mean_absolute_error: 2.8474\n",
      "\n",
      "Epoch 00164: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 165/1000\n",
      "118/118 [==============================] - 59s 498ms/step - loss: 2.1453 - mean_absolute_error: 2.1264 - val_loss: 2.2803 - val_mean_absolute_error: 2.2609\n",
      "\n",
      "Epoch 00165: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 166/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1390 - mean_absolute_error: 2.1197 - val_loss: 2.2621 - val_mean_absolute_error: 2.2429\n",
      "\n",
      "Epoch 00166: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 167/1000\n",
      "118/118 [==============================] - 59s 500ms/step - loss: 2.1361 - mean_absolute_error: 2.1169 - val_loss: 2.2244 - val_mean_absolute_error: 2.2052\n",
      "\n",
      "Epoch 00167: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 168/1000\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.1322 - mean_absolute_error: 2.1130 - val_loss: 2.4331 - val_mean_absolute_error: 2.4139\n",
      "\n",
      "Epoch 00168: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 169/1000\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 2.1266 - mean_absolute_error: 2.1074 - val_loss: 2.3026 - val_mean_absolute_error: 2.2834\n",
      "\n",
      "Epoch 00169: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 170/1000\n",
      "118/118 [==============================] - 59s 501ms/step - loss: 2.1345 - mean_absolute_error: 2.1154 - val_loss: 2.3692 - val_mean_absolute_error: 2.3500\n",
      "\n",
      "Epoch 00170: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 171/1000\n",
      "117/118 [============================>.] - ETA: 0s - loss: 2.1323 - mean_absolute_error: 2.1131\n",
      "Epoch 00170: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 171/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1320 - mean_absolute_error: 2.1127 - val_loss: 2.2909 - val_mean_absolute_error: 2.2714\n",
      "\n",
      "Epoch 00171: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 172/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1216 - mean_absolute_error: 2.1021 - val_loss: 2.8623 - val_mean_absolute_error: 2.8429\n",
      "\n",
      "Epoch 00172: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 173/1000\n",
      "118/118 [==============================] - 60s 512ms/step - loss: 2.1385 - mean_absolute_error: 2.1190 - val_loss: 2.8971 - val_mean_absolute_error: 2.8778\n",
      "\n",
      "Epoch 00173: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 174/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1409 - mean_absolute_error: 2.1216 - val_loss: 2.7321 - val_mean_absolute_error: 2.7127\n",
      "\n",
      "Epoch 00174: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 175/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1381 - mean_absolute_error: 2.1190 - val_loss: 2.5908 - val_mean_absolute_error: 2.5714\n",
      "\n",
      "Epoch 00175: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 176/1000\n",
      "118/118 [==============================] - 60s 510ms/step - loss: 2.1339 - mean_absolute_error: 2.1145 - val_loss: 3.0391 - val_mean_absolute_error: 3.0194\n",
      "\n",
      "Epoch 00176: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 177/1000\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 2.1308 - mean_absolute_error: 2.1115 - val_loss: 2.2886 - val_mean_absolute_error: 2.2694\n",
      "\n",
      "Epoch 00177: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 178/1000\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 2.1435 - mean_absolute_error: 2.1242 - val_loss: 2.2559 - val_mean_absolute_error: 2.2365\n",
      "\n",
      "Epoch 00178: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 179/1000\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 2.1348 - mean_absolute_error: 2.1153 - val_loss: 2.6500 - val_mean_absolute_error: 2.6307\n",
      "\n",
      "Epoch 00179: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 180/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1347 - mean_absolute_error: 2.1154 - val_loss: 2.4190 - val_mean_absolute_error: 2.3996\n",
      "\n",
      "Epoch 00180: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 181/1000\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 2.1285 - mean_absolute_error: 2.1093 - val_loss: 2.3654 - val_mean_absolute_error: 2.3466\n",
      "\n",
      "Epoch 00181: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 182/1000\n",
      "118/118 [==============================] - 61s 516ms/step - loss: 2.1318 - mean_absolute_error: 2.1127 - val_loss: 2.2161 - val_mean_absolute_error: 2.1969\n",
      "\n",
      "Epoch 00182: val_mean_absolute_error did not improve from 2.14300\n",
      "Epoch 183/1000\n",
      " 96/118 [=======================>......] - ETA: 10s - loss: 2.1117 - mean_absolute_error: 2.0927"
     ]
    }
   ],
   "source": [
    "#train_data, val_data, y_train, y_val = train_test_split(training, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "TIMESTEPS=150000\n",
    "\n",
    "dropout=0.3\n",
    "\n",
    "kernel_size=5\n",
    "filters=3\n",
    "strides=5\n",
    "pool_size=2\n",
    "regularizer=l2(0.1)\n",
    "\n",
    "my_model = Sequential()\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation=\"relu\",\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,NUMBER_OF_FEATURES))\n",
    ")\n",
    "             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,NUMBER_OF_FEATURES))\n",
    ")             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,NUMBER_OF_FEATURES))\n",
    ")             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "my_model.add(GRU(units = 4,dropout=dropout,recurrent_dropout=dropout))\n",
    "\n",
    "my_model.add(Dense(10, activation='relu'))\n",
    "\n",
    "\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "my_model.compile(loss = 'mae',optimizer = 'adam', metrics = ['mean_absolute_error'])\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "filepath=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks = [\n",
    "    #EarlyStopping(monitor='val_loss', patience=30, verbose=0),\n",
    "    ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, \n",
    "                    save_best_only=True, mode='min')\n",
    "]\n",
    "\n",
    "my_training_batch_generator = MY_Generator(train_data_batch, NUMBER_OF_BATCHES, 'train')\n",
    "my_validation_batch_generator = MY_Generator(val_data_batch, NUMBER_OF_VALIDATION_STEPS, 'val')\n",
    "\n",
    "\n",
    "history = my_model.fit_generator(generator=my_training_batch_generator,\n",
    "                                      #steps_per_epoch=NUMBER_OF_BATCHES,\n",
    "                                      epochs=1000,\n",
    "                                      validation_data=my_validation_batch_generator,\n",
    "                                      #validation_steps=NUMBER_OF_VALIDATION_STEPS,\n",
    "                                      callbacks=callbacks,\n",
    "                                      shuffle=True,\n",
    "                                      #verbose=1,\n",
    "                                      #validation_data=my_validation_batch_generator,\n",
    "                                      #validation_steps=(num_validation_samples // batch_size),\n",
    "                                      use_multiprocessing=True,\n",
    "                                      workers=8,\n",
    "                                      #max_queue_size=32\n",
    "                  )\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "#import math\n",
    "#print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))\n",
    "\n",
    "\n",
    "#my_model.load_weights('weights.515-2.17.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLITS='test'\n",
    "test_splits = [f for f in listdir(TEST_SPLITS) if isfile(join(TEST_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits\n",
    "\n",
    "ids = []\n",
    "preds = []\n",
    "\n",
    "i = 0\n",
    "for test_file in test_splits:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    path = f'test/{test_file}'\n",
    "    df = pd.read_csv(path, float_precision='round_trip', header=0)\n",
    "    df.columns = ['acoustic_data']\n",
    "    df[['acoustic_data']] = StandardScaler().fit_transform(df[['acoustic_data']].astype('float'))\n",
    "    ids.append(test_file.split(\".\")[0])\n",
    "    preds.append(my_model.predict(df['acoustic_data'].values.reshape(1,TIMESTEPS,1))[0][0])\n",
    "    i+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(ids)\n",
    "submission.columns = ['seg_id']\n",
    "submission['time_to_failure'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res[0].get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test/%s' % (np.random.choice(test_splits))\n",
    "#\n",
    "\n",
    "df = pd.read_csv(path, float_precision='round_trip', header=[0])\n",
    "\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
