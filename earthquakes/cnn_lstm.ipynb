{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,Conv1D,TimeDistributed,MaxPooling1D,Flatten,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tail -n +2 train.csv | split -l 150000\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "TRAIN_SPLITS='train'\n",
    "splits = [f for f in listdir(TRAIN_SPLITS) if isfile(join(TRAIN_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "columns = ['acoustic_data','time_to_failure']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(splits, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS=150000\n",
    "BATCH_SIZE=16\n",
    "NUMBER_OF_BATCHES = int(np.ceil(len(train_data)/BATCH_SIZE))\n",
    "NUMBER_OF_VALIDATION_STEPS = int(np.ceil(len(val_data) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_batch = np.array_split(train_data, NUMBER_OF_BATCHES)\n",
    "val_data_batch = np.array_split(val_data, NUMBER_OF_VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE=0.75\n",
    "\n",
    "def add_noise(dff, pct=NOISE):\n",
    "    mu = dff['acoustic_data'].mean()\n",
    "    sigma = dff['acoustic_data'].std()\n",
    "\n",
    "    indices = np.random.choice(dff.index.values, int(len(dff)*pct))\n",
    "    dff.loc[indices, 'acoustic_data'] = np.random.normal(mu, sigma, len(indices)) \n",
    "    return dff\n",
    "\n",
    "\n",
    "def get_batch(list_of_files, valid=False):\n",
    "#     batch = np.empty((len(list_of_files),TIMESTEPS,1),dtype=float)\n",
    "#     target = np.empty((len(list_of_files),1),dtype=float)\n",
    "    #print(list_of_files)\n",
    "    batch = []\n",
    "    target = []\n",
    "\n",
    "    for idx, file in enumerate(list_of_files):\n",
    "        #print(idx,file)\n",
    "        path = f'train/{file}'\n",
    "        df = pd.read_csv(path, float_precision='round_trip', header=None)\n",
    "        df.columns = columns\n",
    "        df[['acoustic_data']] = StandardScaler().fit_transform(df[['acoustic_data']].astype('float'))\n",
    "        #print(df.head())\n",
    "        #print(len(batch))\n",
    "        batch.append(df['acoustic_data'].values)\n",
    "        target.append(df['time_to_failure'].values[-1])\n",
    "        #print(df_noise.head())\n",
    "        #if not valid:\n",
    "            #df_noise = add_noise(df)\n",
    "            #batch.append(df_noise['acoustic_data'].values)#.reshape(-1,TIMESTEPS,1))\n",
    "            #target.append(df_noise['time_to_failure'].values[-1])#.reshape(-1,1))\n",
    "        #print(np.array(batch).reshape(-1,TIMESTEPS,1).shape)\n",
    "        #batch = np.array(batch).reshape(-1,TIMESTEPS,1)\n",
    "        #target = np.array(target).reshape(-1,1)\n",
    "    return (batch, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class MY_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, list_of_files, steps,name):\n",
    "        self.list_of_files = list_of_files\n",
    "        self.steps = steps\n",
    "        self.name = name\n",
    "\n",
    "    #This function computes the number of batches that this generator is supposed to produce. \n",
    "    #So, we divide the number of total samples by the batch_size and return that value.    \n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    #Here, given the batch numberidx you need to put together a list that consists of data \n",
    "    #batch and the ground-truth (GT). In this example, we read a batch images of size \n",
    "    #self.batch and return an array of form[image_batch, GT]\n",
    "    def __getitem__(self, idx):\n",
    "#         if self.name == 'val':\n",
    "        #print('idx', idx)\n",
    "        #print(\"DEBUG\", self.list_of_files[idx])\n",
    "        #if idx == len(self.list_of_files):\n",
    "            #print(idx, self.list_of_files)\n",
    "        if self.name == 'val':\n",
    "            valid = True\n",
    "        else:\n",
    "            valid=False\n",
    "        train,Y = get_batch(self.list_of_files[idx], valid)\n",
    "            \n",
    "        #print(np.array(train).reshape(-1,TIMESTEPS,1).shape)\n",
    "        #print(\"idx\",idx)\n",
    "        #print(\"LOLILOL\")\n",
    "        #print(train.shape, Y.shape)\n",
    "        train = np.array(train).reshape(-1,TIMESTEPS,1)\n",
    "        #print(train.shape)\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "        return (train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 30000, 50)         300       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15000, 50)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15000, 50)         200       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3000, 50)          12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1500, 50)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1500, 50)          200       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 300, 50)           12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 150, 50)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 150, 50)           200       \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 24)                5400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 31,425\n",
      "Trainable params: 31,125\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/stephane/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "236/236 [==============================] - 62s 262ms/step - loss: 6.6146 - mean_absolute_error: 3.1314 - val_loss: 3.3898 - val_mean_absolute_error: 2.5480\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 2.54802, saving model to weights.01-3.39.hdf5\n",
      "Epoch 2/1000\n",
      "236/236 [==============================] - 55s 233ms/step - loss: 2.9542 - mean_absolute_error: 2.4399 - val_loss: 2.7335 - val_mean_absolute_error: 2.3981\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 2.54802 to 2.39810, saving model to weights.02-2.73.hdf5\n",
      "Epoch 3/1000\n",
      "236/236 [==============================] - 55s 234ms/step - loss: 2.6948 - mean_absolute_error: 2.4122 - val_loss: 2.8028 - val_mean_absolute_error: 2.5713\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 2.39810\n",
      "Epoch 4/1000\n",
      "236/236 [==============================] - 56s 237ms/step - loss: 2.6262 - mean_absolute_error: 2.3989 - val_loss: 2.6288 - val_mean_absolute_error: 2.4009\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 2.39810\n",
      "Epoch 5/1000\n",
      "236/236 [==============================] - 77s 326ms/step - loss: 2.6133 - mean_absolute_error: 2.3897 - val_loss: 2.6221 - val_mean_absolute_error: 2.3905\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 2.39810 to 2.39047, saving model to weights.05-2.62.hdf5\n",
      "Epoch 6/1000\n",
      "236/236 [==============================] - 88s 373ms/step - loss: 2.5845 - mean_absolute_error: 2.3637 - val_loss: 2.6928 - val_mean_absolute_error: 2.4870\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 2.39047\n",
      "Epoch 7/1000\n",
      "236/236 [==============================] - 86s 366ms/step - loss: 2.5513 - mean_absolute_error: 2.3654 - val_loss: 2.5363 - val_mean_absolute_error: 2.3549\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 2.39047 to 2.35486, saving model to weights.07-2.54.hdf5\n",
      "Epoch 8/1000\n",
      "236/236 [==============================] - 86s 365ms/step - loss: 2.5285 - mean_absolute_error: 2.3464 - val_loss: 2.4810 - val_mean_absolute_error: 2.2950\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 2.35486 to 2.29504, saving model to weights.08-2.48.hdf5\n",
      "Epoch 9/1000\n",
      "236/236 [==============================] - 87s 367ms/step - loss: 2.5744 - mean_absolute_error: 2.3611 - val_loss: 2.4898 - val_mean_absolute_error: 2.3125\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 10/1000\n",
      "236/236 [==============================] - 86s 364ms/step - loss: 2.4935 - mean_absolute_error: 2.3178 - val_loss: 2.7816 - val_mean_absolute_error: 2.6020\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 11/1000\n",
      "236/236 [==============================] - 87s 367ms/step - loss: 2.5171 - mean_absolute_error: 2.3453 - val_loss: 2.5133 - val_mean_absolute_error: 2.3535\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 12/1000\n",
      "236/236 [==============================] - 72s 306ms/step - loss: 2.4961 - mean_absolute_error: 2.3298 - val_loss: 2.4569 - val_mean_absolute_error: 2.3001\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 13/1000\n",
      "236/236 [==============================] - 73s 308ms/step - loss: 2.5109 - mean_absolute_error: 2.3433 - val_loss: 2.5424 - val_mean_absolute_error: 2.3622\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 14/1000\n",
      "236/236 [==============================] - 74s 313ms/step - loss: 2.4983 - mean_absolute_error: 2.3147 - val_loss: 2.4888 - val_mean_absolute_error: 2.3006\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 15/1000\n",
      "236/236 [==============================] - 73s 309ms/step - loss: 2.5213 - mean_absolute_error: 2.3129 - val_loss: 2.6620 - val_mean_absolute_error: 2.4874\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 16/1000\n",
      "236/236 [==============================] - 74s 313ms/step - loss: 2.4948 - mean_absolute_error: 2.3152 - val_loss: 2.5261 - val_mean_absolute_error: 2.3505\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 17/1000\n",
      "236/236 [==============================] - 72s 304ms/step - loss: 2.5067 - mean_absolute_error: 2.3203 - val_loss: 3.0084 - val_mean_absolute_error: 2.8262\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 2.29504\n",
      "Epoch 18/1000\n",
      "236/236 [==============================] - 66s 282ms/step - loss: 2.4861 - mean_absolute_error: 2.2979 - val_loss: 2.4755 - val_mean_absolute_error: 2.2878\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error improved from 2.29504 to 2.28783, saving model to weights.18-2.48.hdf5\n",
      "Epoch 19/1000\n",
      "236/236 [==============================] - 70s 295ms/step - loss: 2.4753 - mean_absolute_error: 2.2963 - val_loss: 2.7288 - val_mean_absolute_error: 2.5637\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 20/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.4605 - mean_absolute_error: 2.2912 - val_loss: 2.5026 - val_mean_absolute_error: 2.3311\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 21/1000\n",
      "236/236 [==============================] - 66s 280ms/step - loss: 2.4484 - mean_absolute_error: 2.2825 - val_loss: 2.4726 - val_mean_absolute_error: 2.3024\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 22/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.4717 - mean_absolute_error: 2.2850 - val_loss: 2.8719 - val_mean_absolute_error: 2.6853\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 23/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 68s 290ms/step - loss: 2.4938 - mean_absolute_error: 2.3053 - val_loss: 2.5971 - val_mean_absolute_error: 2.3876\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 24/1000\n",
      "236/236 [==============================] - 66s 281ms/step - loss: 2.4598 - mean_absolute_error: 2.2731 - val_loss: 2.4541 - val_mean_absolute_error: 2.2890\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 2.28783\n",
      "Epoch 25/1000\n",
      "236/236 [==============================] - 66s 280ms/step - loss: 2.4312 - mean_absolute_error: 2.2708 - val_loss: 2.4048 - val_mean_absolute_error: 2.2539\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error improved from 2.28783 to 2.25387, saving model to weights.25-2.40.hdf5\n",
      "Epoch 26/1000\n",
      "236/236 [==============================] - 67s 284ms/step - loss: 2.4132 - mean_absolute_error: 2.2707 - val_loss: 2.4238 - val_mean_absolute_error: 2.2900\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 27/1000\n",
      "236/236 [==============================] - 67s 282ms/step - loss: 2.4015 - mean_absolute_error: 2.2627 - val_loss: 2.6828 - val_mean_absolute_error: 2.5390\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 28/1000\n",
      "236/236 [==============================] - 66s 281ms/step - loss: 2.4165 - mean_absolute_error: 2.2738 - val_loss: 2.6757 - val_mean_absolute_error: 2.5454\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 29/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.4175 - mean_absolute_error: 2.2828 - val_loss: 2.4018 - val_mean_absolute_error: 2.2573\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 30/1000\n",
      "236/236 [==============================] - 68s 287ms/step - loss: 2.3881 - mean_absolute_error: 2.2411 - val_loss: 4.2290 - val_mean_absolute_error: 4.0152\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 31/1000\n",
      "236/236 [==============================] - 67s 283ms/step - loss: 2.4423 - mean_absolute_error: 2.2615 - val_loss: 2.7848 - val_mean_absolute_error: 2.6286\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 32/1000\n",
      "236/236 [==============================] - 69s 293ms/step - loss: 2.3912 - mean_absolute_error: 2.2436 - val_loss: 2.5353 - val_mean_absolute_error: 2.4031\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 33/1000\n",
      "236/236 [==============================] - 61s 259ms/step - loss: 2.3700 - mean_absolute_error: 2.2507 - val_loss: 2.6066 - val_mean_absolute_error: 2.4920\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 34/1000\n",
      "236/236 [==============================] - 68s 287ms/step - loss: 2.3348 - mean_absolute_error: 2.2163 - val_loss: 2.7845 - val_mean_absolute_error: 2.6650\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 35/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.3558 - mean_absolute_error: 2.2361 - val_loss: 2.8471 - val_mean_absolute_error: 2.7263\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 36/1000\n",
      "236/236 [==============================] - 72s 304ms/step - loss: 2.3269 - mean_absolute_error: 2.2165 - val_loss: 2.4755 - val_mean_absolute_error: 2.3694\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 37/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.3280 - mean_absolute_error: 2.2182 - val_loss: 2.3654 - val_mean_absolute_error: 2.2545\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 38/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.3207 - mean_absolute_error: 2.2093 - val_loss: 3.1458 - val_mean_absolute_error: 3.0369\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 39/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.3200 - mean_absolute_error: 2.2098 - val_loss: 3.2142 - val_mean_absolute_error: 3.1029\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 40/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.3067 - mean_absolute_error: 2.2039 - val_loss: 2.3907 - val_mean_absolute_error: 2.2892\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 41/1000\n",
      "236/236 [==============================] - 71s 301ms/step - loss: 2.3064 - mean_absolute_error: 2.2018 - val_loss: 2.6623 - val_mean_absolute_error: 2.5541\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 42/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.3060 - mean_absolute_error: 2.2059 - val_loss: 2.8987 - val_mean_absolute_error: 2.8086\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 43/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.2974 - mean_absolute_error: 2.2047 - val_loss: 2.6391 - val_mean_absolute_error: 2.5476\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 44/1000\n",
      "236/236 [==============================] - 69s 293ms/step - loss: 2.2898 - mean_absolute_error: 2.1973 - val_loss: 2.5835 - val_mean_absolute_error: 2.4912\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 45/1000\n",
      "236/236 [==============================] - 66s 278ms/step - loss: 2.2973 - mean_absolute_error: 2.2019 - val_loss: 2.4296 - val_mean_absolute_error: 2.3374\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 46/1000\n",
      "236/236 [==============================] - 70s 296ms/step - loss: 2.2891 - mean_absolute_error: 2.1924 - val_loss: 2.4655 - val_mean_absolute_error: 2.3624\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 2.25387\n",
      "Epoch 47/1000\n",
      "236/236 [==============================] - 68s 286ms/step - loss: 2.2985 - mean_absolute_error: 2.1959 - val_loss: 2.3355 - val_mean_absolute_error: 2.2323\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error improved from 2.25387 to 2.23232, saving model to weights.47-2.34.hdf5\n",
      "Epoch 48/1000\n",
      "236/236 [==============================] - 70s 295ms/step - loss: 2.2878 - mean_absolute_error: 2.1854 - val_loss: 3.2484 - val_mean_absolute_error: 3.1480\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 2.23232\n",
      "Epoch 49/1000\n",
      "236/236 [==============================] - 66s 280ms/step - loss: 2.3091 - mean_absolute_error: 2.2011 - val_loss: 2.3224 - val_mean_absolute_error: 2.2138\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error improved from 2.23232 to 2.21382, saving model to weights.49-2.32.hdf5\n",
      "Epoch 50/1000\n",
      "236/236 [==============================] - 64s 271ms/step - loss: 2.2930 - mean_absolute_error: 2.1926 - val_loss: 3.3768 - val_mean_absolute_error: 3.2843\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 51/1000\n",
      "236/236 [==============================] - 72s 303ms/step - loss: 2.2744 - mean_absolute_error: 2.1803 - val_loss: 2.5659 - val_mean_absolute_error: 2.4704\n",
      "\n",
      "Epoch 00051: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 52/1000\n",
      "236/236 [==============================] - 71s 300ms/step - loss: 2.2945 - mean_absolute_error: 2.1999 - val_loss: 3.0789 - val_mean_absolute_error: 2.9852\n",
      "\n",
      "Epoch 00052: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 53/1000\n",
      "236/236 [==============================] - 69s 293ms/step - loss: 2.2679 - mean_absolute_error: 2.1701 - val_loss: 2.5531 - val_mean_absolute_error: 2.4477\n",
      "\n",
      "Epoch 00053: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 54/1000\n",
      "236/236 [==============================] - 70s 296ms/step - loss: 2.2656 - mean_absolute_error: 2.1608 - val_loss: 2.8841 - val_mean_absolute_error: 2.7816\n",
      "\n",
      "Epoch 00054: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 55/1000\n",
      "236/236 [==============================] - 70s 296ms/step - loss: 2.2514 - mean_absolute_error: 2.1513 - val_loss: 2.4844 - val_mean_absolute_error: 2.3844\n",
      "\n",
      "Epoch 00055: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 56/1000\n",
      "236/236 [==============================] - 70s 296ms/step - loss: 2.2684 - mean_absolute_error: 2.1642 - val_loss: 3.5948 - val_mean_absolute_error: 3.4889\n",
      "\n",
      "Epoch 00056: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 70s 295ms/step - loss: 2.2649 - mean_absolute_error: 2.1618 - val_loss: 2.4561 - val_mean_absolute_error: 2.3591\n",
      "\n",
      "Epoch 00057: val_mean_absolute_error did not improve from 2.21382\n",
      "Epoch 58/1000\n",
      "236/236 [==============================] - 62s 264ms/step - loss: 2.2658 - mean_absolute_error: 2.1718 - val_loss: 2.2626 - val_mean_absolute_error: 2.1674\n",
      "\n",
      "Epoch 00058: val_mean_absolute_error improved from 2.21382 to 2.16741, saving model to weights.58-2.26.hdf5\n",
      "Epoch 59/1000\n",
      "236/236 [==============================] - 68s 289ms/step - loss: 2.2666 - mean_absolute_error: 2.1706 - val_loss: 2.7539 - val_mean_absolute_error: 2.6558\n",
      "\n",
      "Epoch 00059: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 60/1000\n",
      "236/236 [==============================] - 69s 292ms/step - loss: 2.2705 - mean_absolute_error: 2.1711 - val_loss: 2.2862 - val_mean_absolute_error: 2.1892\n",
      "\n",
      "Epoch 00060: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 61/1000\n",
      "236/236 [==============================] - 70s 297ms/step - loss: 2.2425 - mean_absolute_error: 2.1459 - val_loss: 2.3537 - val_mean_absolute_error: 2.2563\n",
      "\n",
      "Epoch 00061: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 62/1000\n",
      "236/236 [==============================] - 69s 293ms/step - loss: 2.2494 - mean_absolute_error: 2.1495 - val_loss: 2.4356 - val_mean_absolute_error: 2.3363\n",
      "\n",
      "Epoch 00062: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 63/1000\n",
      "236/236 [==============================] - 69s 294ms/step - loss: 2.2347 - mean_absolute_error: 2.1330 - val_loss: 2.3537 - val_mean_absolute_error: 2.2536\n",
      "\n",
      "Epoch 00063: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 64/1000\n",
      "236/236 [==============================] - 62s 263ms/step - loss: 2.2257 - mean_absolute_error: 2.1206 - val_loss: 2.3176 - val_mean_absolute_error: 2.2065\n",
      "\n",
      "Epoch 00064: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 65/1000\n",
      "236/236 [==============================] - 57s 241ms/step - loss: 2.2506 - mean_absolute_error: 2.1428 - val_loss: 2.3463 - val_mean_absolute_error: 2.2413\n",
      "\n",
      "Epoch 00065: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 66/1000\n",
      "236/236 [==============================] - 56s 235ms/step - loss: 2.2287 - mean_absolute_error: 2.1272 - val_loss: 2.3499 - val_mean_absolute_error: 2.2505\n",
      "\n",
      "Epoch 00066: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 67/1000\n",
      "236/236 [==============================] - 57s 241ms/step - loss: 2.2406 - mean_absolute_error: 2.1356 - val_loss: 2.3979 - val_mean_absolute_error: 2.2894\n",
      "\n",
      "Epoch 00067: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 68/1000\n",
      "236/236 [==============================] - 57s 241ms/step - loss: 2.2430 - mean_absolute_error: 2.1372 - val_loss: 2.3760 - val_mean_absolute_error: 2.2744\n",
      "\n",
      "Epoch 00068: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 69/1000\n",
      "236/236 [==============================] - 57s 240ms/step - loss: 2.2363 - mean_absolute_error: 2.1324 - val_loss: 2.5853 - val_mean_absolute_error: 2.4813\n",
      "\n",
      "Epoch 00069: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 70/1000\n",
      "236/236 [==============================] - 56s 238ms/step - loss: 2.2118 - mean_absolute_error: 2.1108 - val_loss: 2.7375 - val_mean_absolute_error: 2.6340\n",
      "\n",
      "Epoch 00070: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 71/1000\n",
      "236/236 [==============================] - 56s 239ms/step - loss: 2.2239 - mean_absolute_error: 2.1217 - val_loss: 2.4756 - val_mean_absolute_error: 2.3723\n",
      "\n",
      "Epoch 00071: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 72/1000\n",
      "236/236 [==============================] - 56s 237ms/step - loss: 2.2071 - mean_absolute_error: 2.1070 - val_loss: 3.2109 - val_mean_absolute_error: 3.1123\n",
      "\n",
      "Epoch 00072: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 73/1000\n",
      "236/236 [==============================] - 55s 235ms/step - loss: 2.2302 - mean_absolute_error: 2.1278 - val_loss: 2.3736 - val_mean_absolute_error: 2.2697\n",
      "\n",
      "Epoch 00073: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 74/1000\n",
      "236/236 [==============================] - 56s 236ms/step - loss: 2.2294 - mean_absolute_error: 2.1246 - val_loss: 2.7669 - val_mean_absolute_error: 2.6631\n",
      "\n",
      "Epoch 00074: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 75/1000\n",
      "236/236 [==============================] - 56s 235ms/step - loss: 2.2076 - mean_absolute_error: 2.1042 - val_loss: 3.8052 - val_mean_absolute_error: 3.7035\n",
      "\n",
      "Epoch 00075: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 76/1000\n",
      "236/236 [==============================] - 56s 236ms/step - loss: 2.2080 - mean_absolute_error: 2.1045 - val_loss: 2.4841 - val_mean_absolute_error: 2.3757\n",
      "\n",
      "Epoch 00076: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 77/1000\n",
      "236/236 [==============================] - 57s 242ms/step - loss: 2.2106 - mean_absolute_error: 2.1045 - val_loss: 3.2576 - val_mean_absolute_error: 3.1523\n",
      "\n",
      "Epoch 00077: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 78/1000\n",
      "236/236 [==============================] - 56s 236ms/step - loss: 2.2183 - mean_absolute_error: 2.1147 - val_loss: 2.7025 - val_mean_absolute_error: 2.5990\n",
      "\n",
      "Epoch 00078: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 79/1000\n",
      "236/236 [==============================] - 56s 236ms/step - loss: 2.1889 - mean_absolute_error: 2.0827 - val_loss: 2.7162 - val_mean_absolute_error: 2.6120\n",
      "\n",
      "Epoch 00079: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 80/1000\n",
      "236/236 [==============================] - 57s 240ms/step - loss: 2.2176 - mean_absolute_error: 2.1172 - val_loss: 2.4011 - val_mean_absolute_error: 2.3014\n",
      "\n",
      "Epoch 00080: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 81/1000\n",
      "236/236 [==============================] - 56s 237ms/step - loss: 2.2015 - mean_absolute_error: 2.1014 - val_loss: 2.4702 - val_mean_absolute_error: 2.3699\n",
      "\n",
      "Epoch 00081: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 82/1000\n",
      "236/236 [==============================] - 56s 238ms/step - loss: 2.2084 - mean_absolute_error: 2.1057 - val_loss: 2.4578 - val_mean_absolute_error: 2.3541\n",
      "\n",
      "Epoch 00082: val_mean_absolute_error did not improve from 2.16741\n",
      "Epoch 83/1000\n",
      "217/236 [==========================>...] - ETA: 4s - loss: 2.2011 - mean_absolute_error: 2.0948"
     ]
    }
   ],
   "source": [
    "#train_data, val_data, y_train, y_val = train_test_split(training, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "TIMESTEPS=150000\n",
    "\n",
    "dropout=0.33\n",
    "\n",
    "kernel_size=5\n",
    "filters=50\n",
    "strides=5\n",
    "pool_size=2\n",
    "regularizer=l2(0.1)\n",
    "\n",
    "my_model = Sequential()\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation=\"relu\",\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,1))\n",
    ")\n",
    "             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,1))\n",
    ")             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "my_model.add(\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "               kernel_regularizer=regularizer,\n",
    "               strides=strides, input_shape=(TIMESTEPS,1))\n",
    ")             \n",
    "my_model.add(MaxPooling1D(pool_size=pool_size))\n",
    "my_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "my_model.add(GRU(units = 24,dropout=dropout,recurrent_dropout=dropout))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "my_model.compile(loss = 'mae',optimizer = 'adam', metrics = ['mean_absolute_error'])\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "filepath=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks = [\n",
    "    #EarlyStopping(monitor='val_loss', patience=30, verbose=0),\n",
    "    ModelCheckpoint(filepath, monitor='val_mean_absolute_error', verbose=1, \n",
    "                    save_best_only=True, mode='min')\n",
    "]\n",
    "\n",
    "my_training_batch_generator = MY_Generator(train_data_batch, NUMBER_OF_BATCHES, 'train')\n",
    "my_validation_batch_generator = MY_Generator(val_data_batch, NUMBER_OF_VALIDATION_STEPS, 'val')\n",
    "\n",
    "\n",
    "history = my_model.fit_generator(generator=my_training_batch_generator,\n",
    "                                      #steps_per_epoch=NUMBER_OF_BATCHES,\n",
    "                                      epochs=1000,\n",
    "                                      validation_data=my_validation_batch_generator,\n",
    "                                      #validation_steps=NUMBER_OF_VALIDATION_STEPS,\n",
    "                                      callbacks=callbacks,\n",
    "                                      shuffle=True,\n",
    "                                      #verbose=1,\n",
    "                                      #validation_data=my_validation_batch_generator,\n",
    "                                      #validation_steps=(num_validation_samples // batch_size),\n",
    "                                      use_multiprocessing=True,\n",
    "                                      workers=8,\n",
    "                                      #max_queue_size=32\n",
    "                  )\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "#import math\n",
    "#print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))\n",
    "\n",
    "\n",
    "#my_model.load_weights('weights.515-2.17.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLITS='test'\n",
    "test_splits = [f for f in listdir(TEST_SPLITS) if isfile(join(TEST_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits\n",
    "\n",
    "ids = []\n",
    "preds = []\n",
    "\n",
    "i = 0\n",
    "for test_file in test_splits:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    path = f'test/{test_file}'\n",
    "    df = pd.read_csv(path, float_precision='round_trip', header=0)\n",
    "    df.columns = ['acoustic_data']\n",
    "    df[['acoustic_data']] = StandardScaler().fit_transform(df[['acoustic_data']].astype('float'))\n",
    "    ids.append(test_file.split(\".\")[0])\n",
    "    preds.append(my_model.predict(df['acoustic_data'].values.reshape(1,TIMESTEPS,1))[0][0])\n",
    "    i+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(ids)\n",
    "submission.columns = ['seg_id']\n",
    "submission['time_to_failure'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res[0].get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test/%s' % (np.random.choice(test_splits))\n",
    "#\n",
    "\n",
    "df = pd.read_csv(path, float_precision='round_trip', header=[0])\n",
    "\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
