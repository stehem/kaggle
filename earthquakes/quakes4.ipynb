{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tail -n +2 train.csv | split -l 150000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['acoustic_data','time_to_failure']\n",
    "path = 'train.csv.zip'\n",
    "\n",
    "\n",
    "BATCH_SIZE=64\n",
    "TIMESTEPS=150000\n",
    "\n",
    "hdf_key = 'hdf_key'\n",
    "df_cols_to_index = columns # list of columns (labels) that should be indexed\n",
    "store = pd.HDFStore(\"train.hdf5\")\n",
    "\n",
    "for chunk in pd.read_csv(path, float_precision='round_trip', header=0, compression='zip',\n",
    "                         chunksize=BATCH_SIZE*TIMESTEPS, iterator=True):\n",
    "    # don't index data columns in each iteration - we'll do it later ...\n",
    "    store.append(hdf_key, chunk, data_columns=df_cols_to_index, index=False)\n",
    "    # index data columns in HDFStore\n",
    "\n",
    "store.create_table_index(hdf_key, columns=df_cols_to_index, optlevel=9, kind='full')\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train.csv.zip'\n",
    "#\n",
    "columns = ['acoustic_data','time_to_failure']\n",
    "\n",
    "BATCH_SIZE=64\n",
    "TIMESTEPS=150000\n",
    "\n",
    "def get_chunk(idx):\n",
    "    chunks = pd.read_csv(path, float_precision='round_trip', header=0, compression='zip',\n",
    "                         chunksize=BATCH_SIZE*TIMESTEPS, iterator=True)\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        if idx == i:\n",
    "            return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(path, float_precision='round_trip', header=0, compression='zip',\n",
    "                         chunksize=BATCH_SIZE*TIMESTEPS, iterator=True)\n",
    "start_time = time.time()\n",
    "\n",
    "for i,chunk in enumerate(chunks):\n",
    "    start_time = time.time()\n",
    "    print(len(chunk))\n",
    "    end_time = time.time()\n",
    "    print(\"Elapsed time was %g seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    df = pd.read_csv(\"train/xaa\", float_precision='round_trip', header=0, iterator=True)\n",
    "    #df[[\"acoustic_data\"]] = StandardScaler().fit_transform(df[[\"acoustic_data\"]])\n",
    "    #print(df.head())\n",
    "    print(df.get_chunk(15000).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_batch(bigchunk):\n",
    "    #print(\"idx\",idx)\n",
    "    bigchunk_data = np.array_split(bigchunk['acoustic_data'].values, BATCH_SIZE)\n",
    "    bigchunk_target = np.array_split(bigchunk['time_to_failure'].values, BATCH_SIZE)\n",
    "    batch = np.empty((BATCH_SIZE,TIMESTEPS,1),dtype=float)\n",
    "    target = np.empty((BATCH_SIZE,1),dtype=float)\n",
    "    for i in range(0,BATCH_SIZE):\n",
    "        #print(df.head())\n",
    "        scaled = StandardScaler().fit_transform(bigchunk_data[i].reshape(-1,1))\n",
    "        #print(i, len(df))\n",
    "        #print(df[\"acoustic_data\"].values.reshape(-1,1).shape)\n",
    "        batch[i] = scaled\n",
    "        target[i] = bigchunk_target[i][-1]\n",
    "        #print(batch.shape)\n",
    "\n",
    "    return batch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    chunks = pd.read_csv(path, float_precision='round_trip', header=0, compression='zip',\n",
    "                         chunksize=BATCH_SIZE*TIMESTEPS, iterator=True)\n",
    "    while 1:\n",
    "        f = open(path)\n",
    "        for line in f:\n",
    "            # create numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x, y = process_line(line)\n",
    "            img = load_images(x)\n",
    "            yield (img, y)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class MY_Generator(Sequence):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    #This function computes the number of batches that this generator is supposed to produce. \n",
    "    #So, we divide the number of total samples by the batch_size and return that value.    \n",
    "    def __len__(self):\n",
    "        return int(629145481/(64*150000))\n",
    "\n",
    "    #Here, given the batch numberidx you need to put together a list that consists of data \n",
    "    #batch and the ground-truth (GT). In this example, we read a batch images of size \n",
    "    #self.batch and return an array of form[image_batch, GT]\n",
    "    def __getitem__(self, idx):\n",
    "        bigchunk = get_chunk(idx)\n",
    "        train,Y = get_generated_batch(bigchunk)\n",
    "        #print(\"idx\",idx)\n",
    "        #print(\"LOLILOL\")\n",
    "        #print(train.shape, Y.shape)\n",
    "\n",
    "        return (train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 150, 5)\n",
      "(None, 8)\n",
      "(None, 1)\n",
      "epoch 0\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "batch 49\n",
      "batch 50\n",
      "batch 51\n",
      "batch 52\n",
      "batch 53\n",
      "batch 54\n",
      "batch 55\n",
      "batch 56\n",
      "batch 57\n",
      "batch 58\n",
      "batch 59\n",
      "batch 60\n",
      "batch 61\n",
      "batch 62\n",
      "batch 63\n",
      "batch 64\n",
      "epoch loss:  5.2536335\n",
      "epoch 1\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "batch 49\n",
      "batch 50\n",
      "batch 51\n",
      "batch 52\n",
      "batch 53\n",
      "batch 54\n",
      "batch 55\n",
      "batch 56\n",
      "batch 57\n",
      "batch 58\n",
      "batch 59\n",
      "batch 60\n",
      "batch 61\n",
      "batch 62\n",
      "batch 63\n",
      "batch 64\n",
      "epoch loss:  3.6953104\n",
      "epoch 2\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "batch 49\n",
      "batch 50\n",
      "batch 51\n",
      "batch 52\n",
      "batch 53\n",
      "batch 54\n",
      "batch 55\n",
      "batch 56\n",
      "batch 57\n",
      "batch 58\n",
      "batch 59\n",
      "batch 60\n",
      "batch 61\n",
      "batch 62\n",
      "batch 63\n",
      "batch 64\n",
      "epoch loss:  3.2353933\n",
      "epoch 3\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n"
     ]
    }
   ],
   "source": [
    "#train_data, val_data, y_train, y_val = train_test_split(training, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,Conv1D,TimeDistributed,MaxPooling1D,Flatten#,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "BATCH_SIZE=64\n",
    "TIMESTEPS=150000\n",
    "\n",
    "dropout=0.2\n",
    "\n",
    "\n",
    "my_model = Sequential()\n",
    "my_model.add(\n",
    "        Conv1D(filters=5, kernel_size=1000,strides=1000, input_shape=(TIMESTEPS,1))\n",
    ")\n",
    "             \n",
    "#my_model.add(MaxPooling1D(pool_size=2))\n",
    "#my_model.add(Flatten())\n",
    "\n",
    "\n",
    "my_model.add(LSTM(use_bias = True,unit_forget_bias=True,units = 8))\n",
    "\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "for layer in my_model.layers:\n",
    "    print(layer.output_shape)\n",
    "\n",
    "my_model.compile(loss = 'mae',optimizer = 'adam', metrics = ['mean_absolute_error'])\n",
    "#my_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "    #Reseter()\n",
    "]\n",
    "\n",
    "#my_training_batch_generator = MY_Generator()\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "for i in range(0,100):\n",
    "    print(\"epoch\",i)\n",
    "    big_chunks = pd.read_csv('train.csv.zip', float_precision='round_trip', header=0, compression='zip',\n",
    "                         chunksize=BATCH_SIZE*TIMESTEPS, iterator=True)\n",
    "    epoch_losses = []\n",
    "    for j,big_chunk in enumerate(big_chunks):\n",
    "        if j > BATCH_SIZE:\n",
    "            continue\n",
    "        print(\"batch\",j)\n",
    "        train, targets = get_generated_batch(big_chunk)\n",
    "        losses = my_model.train_on_batch(train, targets)\n",
    "        epoch_losses.append(losses[0])\n",
    "    print(\"epoch loss: \", np.mean(epoch_losses))\n",
    "\n",
    "\n",
    "\n",
    "#history = my_model.fit_generator(generator=my_training_batch_generator,\n",
    "                                          #steps_per_epoch=500,\n",
    "                                          #epochs=10,\n",
    "                                          #shuffle=False,\n",
    "                                          #verbose=1,\n",
    "                                          #validation_data=my_validation_batch_generator,\n",
    "                                          #validation_steps=(num_validation_samples // batch_size),\n",
    "                                          #use_multiprocessing=False,\n",
    "                                          #workers=8,\n",
    "                                          #max_queue_size=32\n",
    "                      #)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#import math\n",
    "#print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLITS='test'\n",
    "test_splits = [f for f in listdir(TEST_SPLITS) if isfile(join(TEST_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_chunks = np.array_split(test_splits,mp.cpu_count())\n",
    "\n",
    "import build_segment\n",
    "importlib.reload(build_segment)\n",
    "\n",
    "from build_segment import build_segment_f\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_segment_f,args=[chunk,TIMESTEPS, True]) \\\n",
    "           for chunk in test_split_chunks]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "preds = []\n",
    "i=0\n",
    "for r in res:\n",
    "    for df in r.get():\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        #training[i] = df.loc[:,df.columns != 'time_to_failure']\n",
    "        ids.append(df['seg_id'].unique()[0].split(\".\")[0])\n",
    "        test_df = df.drop('seg_id', axis=1)\n",
    "        preds.append(my_model.predict(test_df.values.reshape(1,-1,NUMBER_OF_FEATURES))[0][0])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(ids)\n",
    "submission.columns = ['seg_id']\n",
    "submission['time_to_failure'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"time_to_failure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res[0].get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test/%s' % (np.random.choice(test_splits))\n",
    "#\n",
    "\n",
    "df = pd.read_csv(path, float_precision='round_trip', header=[0])\n",
    "\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
