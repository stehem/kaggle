{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import gc\n",
    "import pickle as pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tail -n +2 train.csv | split -l 150000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "TRAIN_SPLITS='train'\n",
    "splits = [f for f in listdir(TRAIN_SPLITS) if isfile(join(TRAIN_SPLITS, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS=10000\n",
    "\n",
    "\n",
    "import build_segment\n",
    "importlib.reload(build_segment)\n",
    "\n",
    "from build_segment import build_segment_f\n",
    "\n",
    "split_chunks = np.array_split(splits,mp.cpu_count())\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_segment_f,args=[chunk,TIMESTEPS]) for chunk in split_chunks]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = np.empty((len(splits),TIMESTEPS,1),dtype=float)\n",
    "targets = np.empty((len(splits),1),dtype=float)\n",
    "i=0\n",
    "for r in res:\n",
    "    for df in r.get():\n",
    "        training[i] = df.loc[:,df.columns != 'time_to_failure']\n",
    "        #training[i] = df[['acoustic_data','rolling_100']]\n",
    "        targets[i] = np.array([df['time_to_failure'].unique()])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res[0].get()[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].get()[0][['acoustic_data', 'rolling_10', 'rolling_50', 'rolling_100', 'min_50',\n",
    "       'max_50', 'std_50']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(training, targets, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Flatten,GRU,CuDNNGRU,CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "dropout=0.2\n",
    "BATCH_SIZE=64\n",
    "\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(CuDNNLSTM(#use_bias = True,unit_forget_bias=True,\\\n",
    "                  units = 4,\\\n",
    "                  #stateful=True,\n",
    "                    #batch_input_shape=(BATCH_SIZE,TIMESTEPS,2),\\\n",
    "                  #dropout=dropout,recurrent_dropout=dropout,\n",
    "                    #return_sequences=True\n",
    "                  ))\n",
    "\n",
    "#my_model.add(BatchNormalization())\n",
    "#my_model.add(CuDNNLSTM(#use_bias = True,unit_forget_bias=True,\\\n",
    "                 # units = 8,\\\n",
    "                  #stateful=True,\n",
    "                    #batch_input_shape=(BATCH_SIZE,TIMESTEPS,2),\\\n",
    "                  #dropout=dropout,recurrent_dropout=dropout,\n",
    "                 # ))\n",
    "my_model.add(Dense(1))\n",
    "\n",
    "my_model.compile(loss = 'mae',optimizer = 'adam', metrics = ['mean_absolute_error'])\n",
    "#my_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "]\n",
    "\n",
    "history = my_model.fit(train_data, y_train, batch_size=BATCH_SIZE, epochs=100, #shuffle=False,\n",
    "                      validation_data=(val_data,y_val), callbacks=callbacks\n",
    "                    )\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#import math\n",
    "#print(\"best rmse val:\", math.sqrt(my_model.history.history['val_mean_squared_error'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(my_training_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xzfgi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import build_segment\n",
    "importlib.reload(build_segment)\n",
    "\n",
    "from build_segment import build_segment_g\n",
    "\n",
    "split_chunks = np.array_split(splits,mp.cpu_count())\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    res = [pool.apply_async(build_segment_g,args=[chunk]) for chunk in split_chunks]\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = np.empty((100,150000))\n",
    "i=0\n",
    "for r in res:\n",
    "    for df in r.get():\n",
    "        #print(df.shape)\n",
    "        if i < 100:\n",
    "            training[i] = df\n",
    "        #training[i] = df[['acoustic_data','rolling_100']]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "train_data, val_data, y_train, y_val = train_test_split(training, training, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = Input(shape = (150000, ))\n",
    "\n",
    "# Encoder Layers\n",
    "#encoded0 = Dense(50000, activation = 'relu')(input_dim)\n",
    "encoded1 = Dense(10000, activation = 'relu')(input_dim)\n",
    "encoded2 = Dense(5000, activation = 'relu')(encoded1)\n",
    "encoded3 = Dense(2500, activation = 'relu')(encoded2)\n",
    "encoded4 = Dense(1250, activation = 'relu')(encoded3)\n",
    "encoded5 = Dense(750, activation = 'relu')(encoded4)\n",
    "encoded6 = Dense(500, activation = 'relu')(encoded5)\n",
    "\n",
    "# Decoder Layers\n",
    "decoded0 = Dense(500, activation = 'relu')(encoded6)\n",
    "decoded1 = Dense(750, activation = 'relu')(decoded0)\n",
    "decoded2 = Dense(1250, activation = 'relu')(decoded1)\n",
    "decoded3 = Dense(2500, activation = 'relu')(decoded2)\n",
    "decoded4 = Dense(5000, activation = 'relu')(decoded3)\n",
    "decoded5 = Dense(10000, activation = 'relu')(decoded4)\n",
    "#decoded6 = Dense(50000, activation = 'relu')(decoded5)\n",
    "decoded7 = Dense(150000)(decoded5)\n",
    "\n",
    "# Combine Encoder and Deocder layers\n",
    "autoencoder = Model(inputs = input_dim, outputs = decoded7)\n",
    "\n",
    "# Compile the Model\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "autoencoder.fit(train_data, train_data, nb_epoch = 10, batch_size = 1, shuffle = False, validation_data = (val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
